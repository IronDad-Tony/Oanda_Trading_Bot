# test_integration_script.py
"""
æ¸¬è©¦è‡ªé©æ‡‰å…ƒå­¸ç¿’ç³»çµ±èˆ‡å¢å¼·ç‰ˆé‡å­ç­–ç•¥å±¤çš„é›†æˆ
é©—è­‰ç³»çµ±èƒ½å¦æ­£ç¢ºè™•ç†ä¸åŒæ•¸é‡çš„ç­–ç•¥å’Œç¶­åº¦è®ŠåŒ–
"""

import sys
import os
from pathlib import Path

# æ·»åŠ é …ç›®æ ¹ç›®éŒ„åˆ°è·¯å¾‘
current_dir = Path(__file__).resolve().parent
project_root = current_dir
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

import torch
import torch.nn as nn
import logging

# è¨­ç½®æ—¥èªŒ
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def test_integration():
    """æ¸¬è©¦è‡ªé©æ‡‰å…ƒå­¸ç¿’ç³»çµ±èˆ‡é‡å­ç­–ç•¥å±¤çš„é›†æˆ"""
    logger.info("é–‹å§‹æ¸¬è©¦è‡ªé©æ‡‰å…ƒå­¸ç¿’ç³»çµ±èˆ‡é‡å­ç­–ç•¥å±¤é›†æˆ...")
    
    try:
        # å°å…¥æ¨¡çµ„
        from src.agent.meta_learning_system import MetaLearningSystem
        from src.agent.enhanced_quantum_strategy_layer import EnhancedStrategySuperposition
        
        # æ¸¬è©¦é…ç½®
        batch_size = 8
        initial_state_dim = 64
        action_dim = 10
        
        # 1. å‰µå»ºåˆå§‹å…ƒå­¸ç¿’ç³»çµ±
        logger.info("æ­¥é©Ÿ 1: å‰µå»ºåˆå§‹å…ƒå­¸ç¿’ç³»çµ±...")
        meta_system = MetaLearningSystem(
            initial_state_dim=initial_state_dim,
            action_dim=action_dim,
            meta_learning_dim=256
        )
        
        # 2. å‰µå»ºå¢å¼·ç‰ˆé‡å­ç­–ç•¥å±¤ï¼ˆåˆå§‹é…ç½®ï¼‰
        logger.info("æ­¥é©Ÿ 2: å‰µå»ºå¢å¼·ç‰ˆé‡å­ç­–ç•¥å±¤ï¼ˆ64ç¶­ï¼Œ15ç­–ç•¥ï¼‰...")
        strategy_layer_v1 = EnhancedStrategySuperposition(
            state_dim=initial_state_dim,
            action_dim=action_dim,
            enable_dynamic_generation=True
        )
        
        # 3. æ¸¬è©¦åˆå§‹é©æ‡‰
        logger.info("æ­¥é©Ÿ 3: æ¸¬è©¦åˆå§‹é©æ‡‰...")
        initial_adaptation = meta_system.adapt_to_strategy_layer(strategy_layer_v1)
        logger.info(f"åˆå§‹é©æ‡‰çµæœ: {initial_adaptation}")
        
        # 4. æ¸¬è©¦èˆ‡ç­–ç•¥å±¤çš„äº¤äº’
        logger.info("æ­¥é©Ÿ 4: æ¸¬è©¦èˆ‡ç­–ç•¥å±¤çš„äº¤äº’...")
        test_state = torch.randn(batch_size, initial_state_dim)
        test_volatility = torch.rand(batch_size) * 0.5
        
        # é‡å­ç­–ç•¥å±¤è¼¸å‡º
        with torch.no_grad():
            strategy_output, strategy_info = strategy_layer_v1(test_state, test_volatility)
            
        # å…ƒå­¸ç¿’ç³»çµ±è¼¸å‡º
        with torch.no_grad():
            meta_output, meta_info = meta_system(test_state)
            
        logger.info(f"ç­–ç•¥å±¤è¼¸å‡ºå½¢ç‹€: {strategy_output.shape}")
        logger.info(f"å…ƒå­¸ç¿’è¼¸å‡ºå½¢ç‹€: {meta_output.shape}")
        logger.info(f"ç­–ç•¥æ¬Šé‡å½¢ç‹€: {strategy_info['strategy_weights'].shape}")
        logger.info(f"æ´»èºç­–ç•¥æ•¸é‡: {strategy_info['num_active_strategies'].mean():.1f}")
        
        # 5. æ¨¡æ“¬ç­–ç•¥æ•¸é‡è®ŠåŒ–ï¼ˆ15 -> 25ç­–ç•¥ï¼‰
        logger.info("æ­¥é©Ÿ 5: æ¨¡æ“¬ç­–ç•¥æ•¸é‡è®ŠåŒ–ï¼ˆ15 -> 25ç­–ç•¥ï¼‰...")
        
        # å‰µå»ºæ“´å±•çš„ç­–ç•¥å±¤ï¼ˆæ¨¡æ“¬ï¼‰
        class ExtendedStrategyLayer(nn.Module):
            def __init__(self, state_dim, action_dim, num_strategies=25):
                super().__init__()
                self.state_dim = state_dim
                self.action_dim = action_dim
                self.num_strategies = num_strategies
                
                # å‰µå»º25å€‹æ¨¡æ“¬ç­–ç•¥
                self.base_strategies = nn.ModuleList([
                    nn.Sequential(
                        nn.Linear(state_dim, 64),
                        nn.ReLU(),
                        nn.Linear(64, action_dim),
                        nn.Tanh()
                    ) for _ in range(num_strategies)
                ])
                
            def forward(self, state, volatility):
                # ç°¡å–®çš„ç­–ç•¥çµ„åˆ
                outputs = []
                for strategy in self.base_strategies:
                    outputs.append(strategy(state))
                
                # å¹³å‡çµ„åˆ
                combined = torch.stack(outputs, dim=1).mean(dim=1)
                
                # æ¨¡æ“¬ç­–ç•¥æ¬Šé‡
                weights = torch.softmax(torch.randn(state.size(0), self.num_strategies), dim=-1)
                
                return combined, {
                    'strategy_weights': weights,
                    'num_active_strategies': torch.sum(weights > 0.04, dim=-1).float()
                }
        
        # æ·»åŠ ç­–ç•¥åç¨±æ–¹æ³•
        strategy_layer_v2 = ExtendedStrategyLayer(initial_state_dim, action_dim, 25)
        for i, strategy in enumerate(strategy_layer_v2.base_strategies):
            strategy.get_strategy_name = lambda idx=i: f"ExtendedStrategy_{idx}"
        
        # 6. æ¸¬è©¦ç­–ç•¥æ•¸é‡è®ŠåŒ–é©æ‡‰
        logger.info("æ­¥é©Ÿ 6: æ¸¬è©¦ç­–ç•¥æ•¸é‡è®ŠåŒ–é©æ‡‰...")
        strategy_adaptation = meta_system.adapt_to_strategy_layer(strategy_layer_v2)
        logger.info(f"ç­–ç•¥æ•¸é‡è®ŠåŒ–é©æ‡‰çµæœ: {strategy_adaptation}")
        
        # é©—è­‰æ–°é…ç½®
        current_config = meta_system.current_config
        logger.info(f"ç•¶å‰é…ç½® - ç‹€æ…‹ç¶­åº¦: {current_config.state_dim}, "
                   f"ç­–ç•¥æ•¸é‡: {current_config.num_strategies}")
        
        # 7. æ¨¡æ“¬ç‹€æ…‹ç¶­åº¦è®ŠåŒ–ï¼ˆ64 -> 96ç¶­ï¼‰
        logger.info("æ­¥é©Ÿ 7: æ¨¡æ“¬ç‹€æ…‹ç¶­åº¦è®ŠåŒ–ï¼ˆ64 -> 96ç¶­ï¼‰...")
        new_state_dim = 96
        strategy_layer_v3 = ExtendedStrategyLayer(new_state_dim, action_dim, 25)
        
        # æ·»åŠ ç­–ç•¥åç¨±æ–¹æ³•
        for i, strategy in enumerate(strategy_layer_v3.base_strategies):
            strategy.get_strategy_name = lambda idx=i: f"ExtendedStrategy96_{idx}"
        
        # æ¸¬è©¦ç¶­åº¦è®ŠåŒ–é©æ‡‰
        dimension_adaptation = meta_system.adapt_to_strategy_layer(strategy_layer_v3)
        logger.info(f"ç¶­åº¦è®ŠåŒ–é©æ‡‰çµæœ: {dimension_adaptation}")
        
        # 8. æ¸¬è©¦æ–°é…ç½®ä¸‹çš„å‰å‘å‚³æ’­
        logger.info("æ­¥é©Ÿ 8: æ¸¬è©¦æ–°é…ç½®ä¸‹çš„å‰å‘å‚³æ’­...")
        new_test_state = torch.randn(batch_size, new_state_dim)
        new_test_volatility = torch.rand(batch_size) * 0.5
        
        with torch.no_grad():
            # æ–°ç­–ç•¥å±¤è¼¸å‡º
            new_strategy_output, new_strategy_info = strategy_layer_v3(new_test_state, new_test_volatility)
            
            # æ–°å…ƒå­¸ç¿’è¼¸å‡º
            new_meta_output, new_meta_info = meta_system(new_test_state)
            
        logger.info(f"æ–°é…ç½®ç­–ç•¥å±¤è¼¸å‡ºå½¢ç‹€: {new_strategy_output.shape}")
        logger.info(f"æ–°é…ç½®å…ƒå­¸ç¿’è¼¸å‡ºå½¢ç‹€: {new_meta_output.shape}")
        
        # 9. æª¢æŸ¥é©æ‡‰æ­·å²
        logger.info("æ­¥é©Ÿ 9: æª¢æŸ¥é©æ‡‰æ­·å²...")
        system_status = meta_system.get_system_status()
        logger.info(f"ç¸½é…ç½®è®ŠåŒ–æ¬¡æ•¸: {system_status['total_config_changes']}")
        logger.info(f"ç¸½ç¶­åº¦è®ŠåŒ–æ¬¡æ•¸: {system_status['total_dimension_changes']}")
        logger.info(f"é©æ‡‰æˆåŠŸç‡: {system_status['adaptation_success_rate']:.2%}")
        logger.info(f"ç·¨ç¢¼å™¨ç•¶å‰ç¶­åº¦: {system_status['encoder_current_dim']}")
        
        # 10. æ¸¬è©¦æ¢¯åº¦å‚³æ’­
        logger.info("æ­¥é©Ÿ 10: æ¸¬è©¦æ¢¯åº¦å‚³æ’­...")
        meta_system.train()
        strategy_layer_v3.train()
        
        # å‰µå»ºéœ€è¦æ¢¯åº¦çš„è¼¸å…¥
        grad_test_state = torch.randn(4, new_state_dim, requires_grad=True)
        grad_test_volatility = torch.rand(4) * 0.5
        
        # å‰å‘å‚³æ’­
        strategy_out, _ = strategy_layer_v3(grad_test_state, grad_test_volatility)
        meta_out, _ = meta_system(grad_test_state)
        
        # è¨ˆç®—æå¤±
        strategy_loss = strategy_out.abs().mean()
        meta_loss = meta_out.abs().mean()
        combined_loss = strategy_loss + meta_loss
        
        # åå‘å‚³æ’­
        combined_loss.backward()
        
        # æª¢æŸ¥æ¢¯åº¦
        meta_grad_norm = 0
        strategy_grad_norm = 0
        
        for param in meta_system.parameters():
            if param.grad is not None:
                meta_grad_norm += param.grad.data.norm(2) ** 2
                
        for param in strategy_layer_v3.parameters():
            if param.grad is not None:
                strategy_grad_norm += param.grad.data.norm(2) ** 2
        
        meta_grad_norm = meta_grad_norm ** 0.5
        strategy_grad_norm = strategy_grad_norm ** 0.5
        
        logger.info(f"å…ƒå­¸ç¿’ç³»çµ±æ¢¯åº¦ç¯„æ•¸: {meta_grad_norm:.6f}")
        logger.info(f"ç­–ç•¥å±¤æ¢¯åº¦ç¯„æ•¸: {strategy_grad_norm:.6f}")
        
        # 11. ä¿å­˜å’Œè¼‰å…¥é©æ‡‰æ­·å²
        logger.info("æ­¥é©Ÿ 11: ä¿å­˜å’Œè¼‰å…¥é©æ‡‰æ­·å²...")
        history_file = "integration_test_history.json"
        meta_system.save_configuration_history(history_file)
        
        # 12. è¼¸å‡ºæœ€çµ‚ç¸½çµ
        logger.info("æ­¥é©Ÿ 12: è¼¸å‡ºæœ€çµ‚ç¸½çµ...")
        final_status = meta_system.get_system_status()
        
        logger.info("=== é›†æˆæ¸¬è©¦ç¸½çµ ===")
        logger.info(f"âœ… åˆå§‹é…ç½®: 64ç¶­ -> ç•¶å‰é…ç½®: {final_status['encoder_current_dim']}ç¶­")
        logger.info(f"âœ… ç­–ç•¥æ•¸é‡è®ŠåŒ–: 15ç­–ç•¥ -> 25ç­–ç•¥")
        logger.info(f"âœ… ç¸½é©æ‡‰æ¬¡æ•¸: {final_status['total_adaptations']}")
        logger.info(f"âœ… é©æ‡‰æˆåŠŸç‡: {final_status['adaptation_success_rate']:.2%}")
        logger.info(f"âœ… é…ç½®æª¢æ¸¬æ–¹æ³•: {len(final_status['supported_detection_methods'])}ç¨®")
        logger.info(f"âœ… ç¶­åº¦è®ŠåŒ–æ­·å²: {final_status['encoder_dimension_changes']}æ¬¡")
        
        # æ¸…ç†æ¸¬è©¦æ–‡ä»¶
        if os.path.exists(history_file):
            os.remove(history_file)
            
        logger.info("è‡ªé©æ‡‰å…ƒå­¸ç¿’ç³»çµ±èˆ‡é‡å­ç­–ç•¥å±¤é›†æˆæ¸¬è©¦å®Œæˆï¼")
        return True
        
    except Exception as e:
        logger.error(f"é›†æˆæ¸¬è©¦å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    success = test_integration()
    if success:
        print("\nğŸ‰ é›†æˆæ¸¬è©¦æˆåŠŸï¼è‡ªé©æ‡‰å…ƒå­¸ç¿’ç³»çµ±èƒ½å¤ æ­£ç¢ºè™•ç†ç­–ç•¥å’Œç¶­åº¦è®ŠåŒ–ï¼")
    else:
        print("\nâŒ é›†æˆæ¸¬è©¦å¤±æ•—ï¼")
