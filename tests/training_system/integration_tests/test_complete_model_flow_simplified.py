"""
å…¨é¢æ¸¬è©¦æ¨¡å‹æµç¨‹ï¼ŒåŒ…å«æ‰€æœ‰æ¨¡çµ„çš„æ¢¯åº¦æµé©—è­‰
"""

import sys
import os
import torch
import torch.nn as nn
import numpy as np
import logging
from typing import Dict, Any
import traceback

# Add src to path
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..', 'src')))

# Import required modules
try:
    from src.models.enhanced_transformer import EnhancedTransformer
    from src.agent.enhanced_quantum_strategy_layer import EnhancedStrategySuperposition
    from src.agent.meta_learning_system import MetaLearningSystem
    from src.environment.progressive_reward_system import ProgressiveLearningSystem
except ImportError as e:
    print(f"Import error: {e}")
    raise

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class CompleteModelFlowTester:
    """å®Œæ•´æ¨¡å‹æµç¨‹æ¸¬è©¦å™¨"""
    
    def __init__(self):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.batch_size = 4
        self.seq_len = 100
        self.num_features = 24
        self.num_symbols = 3
        self.action_dim = 3
        
        # åˆå§‹åŒ–æ‰€æœ‰æ¨¡çµ„
        self.setup_models()
        
    def setup_models(self):
        """è¨­ç½®æ‰€æœ‰æ¨¡å‹çµ„ä»¶"""
        logger.info("è¨­ç½®æ¨¡å‹çµ„ä»¶...")
        
        # 1. Enhanced Transformer
        self.transformer_config = {
            'input_size': self.num_features,
            'hidden_size': 256,
            'num_layers': 4,
            'num_heads': 8,
            'num_symbols': self.num_symbols,
            'dropout': 0.1,
            'use_msfe': True,
            'use_wavelet': True,
            'use_fourier': True,
            'use_market_state_detection': True
        }
        
        self.transformer = EnhancedTransformer(**self.transformer_config).to(self.device)
        
        # 2. Enhanced Strategy Superposition (Quantum Strategy Layer)
        self.quantum_layer = EnhancedStrategySuperposition(
            input_dim=256,
            num_strategies=5,
            dropout_rate=0.1,
            strategy_input_dim=256
        ).to(self.device)
        
        # 3. Meta Learning System
        self.meta_learning = MetaLearningSystem(
            strategy_pool_size=10,
            adaptation_lr=1e-3,
            memory_size=1000,
            device=self.device
        )
        
        # 4. Progressive Reward System
        self.reward_system = ProgressiveLearningSystem(
            initial_stage='simple',
            stage_criteria={'simple': 100, 'intermediate': 500}
        )
        
        logger.info("æ‰€æœ‰æ¨¡å‹çµ„ä»¶è¨­ç½®å®Œæˆ")
    
    def create_sample_data(self) -> Dict[str, torch.Tensor]:
        """å‰µå»ºæ¨£æœ¬æ•¸æ“š"""
        # å¸‚å ´æ•¸æ“š
        market_data = torch.randn(
            self.batch_size, 
            self.seq_len, 
            self.num_symbols,
            self.num_features,
            device=self.device
        )
        
        # Padding mask for symbols
        src_key_padding_mask = torch.zeros(
            self.batch_size,
            self.num_symbols,
            dtype=torch.bool,
            device=self.device
        )
        
        # å¸‚å ´ç‹€æ…‹
        market_state = torch.randint(0, 3, (self.batch_size,), device=self.device)
        
        # çå‹µ
        rewards = torch.randn(self.batch_size, device=self.device)
        
        return {
            'market_data': market_data,
            'src_key_padding_mask': src_key_padding_mask,
            'market_state': market_state,
            'rewards': rewards
        }
    
    def test_transformer_forward_pass(self, data: Dict[str, torch.Tensor]) -> torch.Tensor:
        """æ¸¬è©¦Transformerå‰å‘å‚³æ’­"""
        logger.info("æ¸¬è©¦Transformerå‰å‘å‚³æ’­...")
        
        try:
            # é‡å¡‘è¼¸å…¥æ•¸æ“š
            batch_size, seq_len, num_symbols, num_features = data['market_data'].shape
            input_tensor = data['market_data'].view(batch_size, seq_len, num_symbols * num_features)
            
            # å‰å‘å‚³æ’­
            output = self.transformer(
                input_tensor,
                src_key_padding_mask=data['src_key_padding_mask']
            )
            
            logger.info(f"Transformerè¼¸å‡ºå½¢ç‹€: {output.shape}")
            assert output.shape == (batch_size, seq_len, 256), f"æœŸæœ›å½¢ç‹€ {(batch_size, seq_len, 256)}, å¯¦éš› {output.shape}"
            assert output.requires_grad, "Transformerè¼¸å‡ºæ‡‰è©²éœ€è¦æ¢¯åº¦"
            
            logger.info("âœ“ Transformerå‰å‘å‚³æ’­æ¸¬è©¦é€šé")
            return output
            
        except Exception as e:
            logger.error(f"âœ— Transformerå‰å‘å‚³æ’­æ¸¬è©¦å¤±æ•—: {e}")
            raise
    
    def test_quantum_strategy_layer(self, transformer_output: torch.Tensor, data: Dict[str, torch.Tensor]) -> torch.Tensor:
        """æ¸¬è©¦é‡å­ç­–ç•¥å±¤"""
        logger.info("æ¸¬è©¦é‡å­ç­–ç•¥å±¤...")
        
        try:
            # ä½¿ç”¨æœ€å¾Œä¸€å€‹æ™‚é–“æ­¥çš„è¼¸å‡º
            last_hidden = transformer_output[:, -1, :]  # [batch, hidden_size]
            
            # å‰å‘å‚³æ’­
            strategy_output = self.quantum_layer(last_hidden)
            
            logger.info(f"é‡å­ç­–ç•¥å±¤è¼¸å‡ºå½¢ç‹€: {strategy_output.shape}")
            assert strategy_output.requires_grad, "é‡å­ç­–ç•¥å±¤è¼¸å‡ºæ‡‰è©²éœ€è¦æ¢¯åº¦"
            
            logger.info("âœ“ é‡å­ç­–ç•¥å±¤æ¸¬è©¦é€šé")
            return strategy_output
            
        except Exception as e:
            logger.error(f"âœ— é‡å­ç­–ç•¥å±¤æ¸¬è©¦å¤±æ•—: {e}")
            raise
    
    def test_meta_learning_adaptation(self, data: Dict[str, torch.Tensor]) -> Dict[str, Any]:
        """æ¸¬è©¦å…ƒå­¸ç¿’é©æ‡‰"""
        logger.info("æ¸¬è©¦å…ƒå­¸ç¿’é©æ‡‰...")
        
        try:
            # æ¨¡æ“¬ç­–ç•¥è¡¨ç¾æ•¸æ“š
            strategy_performance = {
                'returns': np.random.randn(5),
                'sharpe_ratios': np.random.uniform(0.5, 2.0, 5),
                'max_drawdowns': np.random.uniform(0.05, 0.15, 5),
                'win_rates': np.random.uniform(0.4, 0.7, 5)
            }
            
            # è©•ä¼°ç­–ç•¥è¡¨ç¾
            evaluation_results = self.meta_learning.evaluate_strategy_performance(
                strategy_performance,
                data['market_state'][0].item()
            )
            
            logger.info(f"ç­–ç•¥è©•ä¼°çµæœ: {evaluation_results}")
            assert 'overall_score' in evaluation_results, "ç¼ºå°‘æ•´é«”è©•åˆ†"
            
            logger.info("âœ“ å…ƒå­¸ç¿’é©æ‡‰æ¸¬è©¦é€šé")
            return evaluation_results
            
        except Exception as e:
            logger.error(f"âœ— å…ƒå­¸ç¿’é©æ‡‰æ¸¬è©¦å¤±æ•—: {e}")
            raise
    
    def test_gradient_flow(self, data: Dict[str, torch.Tensor]) -> Dict[str, bool]:
        """æ¸¬è©¦æ¢¯åº¦æµå’Œæ¬Šé‡æ›´æ–°"""
        logger.info("æ¸¬è©¦æ¢¯åº¦æµå’Œæ¬Šé‡æ›´æ–°...")
        
        try:
            results = {}
            
            # è¨˜éŒ„åˆå§‹æ¬Šé‡
            initial_weights = {}
            for name, param in self.transformer.named_parameters():
                if param.requires_grad:
                    initial_weights[f"transformer_{name}"] = param.clone().detach()
            
            for name, param in self.quantum_layer.named_parameters():
                if param.requires_grad:
                    initial_weights[f"quantum_{name}"] = param.clone().detach()
            
            # å®Œæ•´å‰å‘å‚³æ’­
            market_data_reshaped = data['market_data'].view(
                self.batch_size, self.seq_len, self.num_symbols * self.num_features
            )
            
            transformer_output = self.transformer(
                market_data_reshaped,
                src_key_padding_mask=data['src_key_padding_mask']
            )
            
            last_hidden = transformer_output[:, -1, :]
            strategy_output = self.quantum_layer(last_hidden)
            
            # è¨ˆç®—æå¤±
            target = torch.randn_like(strategy_output)
            loss = nn.MSELoss()(strategy_output, target)
            
            logger.info(f"è¨ˆç®—æå¤±: {loss.item()}")
            
            # åå‘å‚³æ’­
            loss.backward()
            
            # æª¢æŸ¥æ¢¯åº¦
            transformer_has_gradients = False
            for name, param in self.transformer.named_parameters():
                if param.requires_grad and param.grad is not None:
                    grad_norm = param.grad.norm().item()
                    if grad_norm > 1e-8:
                        transformer_has_gradients = True
                        break
            
            quantum_has_gradients = False
            for name, param in self.quantum_layer.named_parameters():
                if param.requires_grad and param.grad is not None:
                    grad_norm = param.grad.norm().item()
                    if grad_norm > 1e-8:
                        quantum_has_gradients = True
                        break
            
            results['transformer_gradients'] = transformer_has_gradients
            results['quantum_gradients'] = quantum_has_gradients
            
            # æ¨¡æ“¬å„ªåŒ–å™¨æ­¥é©Ÿ
            optimizer = torch.optim.Adam(
                list(self.transformer.parameters()) + list(self.quantum_layer.parameters()),
                lr=1e-4
            )
            optimizer.step()
            
            # æª¢æŸ¥æ¬Šé‡æ›´æ–°
            weights_updated = False
            for name, param in self.transformer.named_parameters():
                if param.requires_grad and f"transformer_{name}" in initial_weights:
                    initial = initial_weights[f"transformer_{name}"]
                    current = param.detach()
                    diff = torch.norm(current - initial).item()
                    if diff > 1e-8:
                        weights_updated = True
                        break
            
            results['weights_updated'] = weights_updated
            
            logger.info(f"æ¢¯åº¦æª¢æŸ¥çµæœ: transformer_gradients={transformer_has_gradients}, quantum_gradients={quantum_has_gradients}, weights_updated={weights_updated}")
            logger.info("âœ“ æ¢¯åº¦æµå’Œæ¬Šé‡æ›´æ–°æ¸¬è©¦é€šé")
            
            return results
            
        except Exception as e:
            logger.error(f"âœ— æ¢¯åº¦æµå’Œæ¬Šé‡æ›´æ–°æ¸¬è©¦å¤±æ•—: {e}")
            traceback.print_exc()
            raise
    
    def test_progressive_reward_system(self, data: Dict[str, torch.Tensor]) -> Dict[str, Any]:
        """æ¸¬è©¦æ¼¸é€²å¼çå‹µç³»çµ±"""
        logger.info("æ¸¬è©¦æ¼¸é€²å¼çå‹µç³»çµ±...")
        
        try:
            # æ¨¡æ“¬äº¤æ˜“çµæœ
            trading_results = {
                'profit_loss': data['rewards'].cpu().numpy(),
                'sharpe_ratio': np.random.uniform(0.5, 2.0, self.batch_size),
                'max_drawdown': np.random.uniform(0.05, 0.15, self.batch_size),
                'volatility': np.random.uniform(0.1, 0.3, self.batch_size),
                'trade_count': np.random.randint(10, 100, self.batch_size)
            }
            
            # è¨ˆç®—ç•¶å‰éšæ®µçå‹µ
            current_reward_func = self.reward_system.get_current_reward_function()
            rewards = []
            
            for i in range(self.batch_size):
                batch_results = {k: v[i] if isinstance(v, np.ndarray) else v for k, v in trading_results.items()}
                reward = current_reward_func.calculate_reward(batch_results, data['market_state'][i].item())
                rewards.append(reward)
            
            rewards = np.array(rewards)
            
            logger.info(f"ç•¶å‰éšæ®µ: {self.reward_system.current_stage}")
            logger.info(f"è¨ˆç®—çå‹µ: {rewards}")
            
            results = {
                'current_stage': self.reward_system.current_stage,
                'rewards': rewards.tolist(),
                'mean_reward': np.mean(rewards)
            }
            
            logger.info("âœ“ æ¼¸é€²å¼çå‹µç³»çµ±æ¸¬è©¦é€šé")
            return results
            
        except Exception as e:
            logger.error(f"âœ— æ¼¸é€²å¼çå‹µç³»çµ±æ¸¬è©¦å¤±æ•—: {e}")
            raise
    
    def run_complete_test(self) -> Dict[str, Any]:
        """é‹è¡Œå®Œæ•´çš„æ¸¬è©¦æµç¨‹"""
        logger.info("é–‹å§‹å®Œæ•´æ¨¡å‹æµç¨‹æ¸¬è©¦...")
        
        test_results = {}
        
        try:
            # 1. å‰µå»ºæ¸¬è©¦æ•¸æ“š
            data = self.create_sample_data()
            logger.info("âœ“ æ¸¬è©¦æ•¸æ“šå‰µå»ºå®Œæˆ")
            
            # 2. æ¸¬è©¦Transformer
            transformer_output = self.test_transformer_forward_pass(data)
            test_results['transformer'] = True
            
            # 3. æ¸¬è©¦é‡å­ç­–ç•¥å±¤
            strategy_output = self.test_quantum_strategy_layer(transformer_output, data)
            test_results['quantum_strategy'] = True
            
            # 4. æ¸¬è©¦å…ƒå­¸ç¿’
            meta_results = self.test_meta_learning_adaptation(data)
            test_results['meta_learning'] = True
            test_results['meta_results'] = meta_results
            
            # 5. æ¸¬è©¦æ¢¯åº¦æµ
            gradient_results = self.test_gradient_flow(data)
            test_results['gradient_flow'] = True
            test_results['gradient_results'] = gradient_results
            
            # 6. æ¸¬è©¦æ¼¸é€²å¼çå‹µç³»çµ±
            reward_results = self.test_progressive_reward_system(data)
            test_results['progressive_reward'] = True
            test_results['reward_results'] = reward_results
            
            # æ•´é«”æˆåŠŸ
            test_results['overall_success'] = True
            test_results['all_tests_passed'] = all([
                test_results['transformer'],
                test_results['quantum_strategy'],
                test_results['meta_learning'],
                test_results['gradient_flow'],
                test_results['progressive_reward']
            ])
            
            logger.info("ğŸ‰ å®Œæ•´æ¨¡å‹æµç¨‹æ¸¬è©¦å…¨éƒ¨é€šéï¼")
            
        except Exception as e:
            test_results['overall_success'] = False
            test_results['error'] = str(e)
            test_results['traceback'] = traceback.format_exc()
            logger.error(f"å®Œæ•´æ¨¡å‹æµç¨‹æ¸¬è©¦å¤±æ•—: {e}")
            
        return test_results


def test_complete_model_flow():
    """å®Œæ•´æ¨¡å‹æµç¨‹æ¸¬è©¦å‡½æ•¸"""
    tester = CompleteModelFlowTester()
    results = tester.run_complete_test()
    
    # æ‰“å°è©³ç´°çµæœ
    print("\n" + "="*80)
    print("å®Œæ•´æ¨¡å‹æµç¨‹æ¸¬è©¦çµæœ")
    print("="*80)
    
    for key, value in results.items():
        if key not in ['gradient_results', 'meta_results', 'reward_results']:
            print(f"{key}: {value}")
    
    print("\nè©³ç´°çµæœ:")
    if 'gradient_results' in results:
        print(f"æ¢¯åº¦æµçµæœ: {results['gradient_results']}")
    if 'meta_results' in results:
        print(f"å…ƒå­¸ç¿’çµæœ: {results['meta_results']}")
    
    print("="*80)
    
    # æ–·è¨€æ¸¬è©¦çµæœ
    assert results.get('overall_success', False), f"æ¸¬è©¦å¤±æ•—: {results.get('error', 'Unknown error')}"
    assert results.get('all_tests_passed', False), "ä¸æ˜¯æ‰€æœ‰æ¸¬è©¦éƒ½é€šé"
    
    # ç‰¹åˆ¥æª¢æŸ¥æ¢¯åº¦æµ
    gradient_results = results.get('gradient_results', {})
    assert gradient_results.get('transformer_gradients', False), "Transformeræ²’æœ‰è¨ˆç®—æ¢¯åº¦"
    assert gradient_results.get('quantum_gradients', False), "é‡å­ç­–ç•¥å±¤æ²’æœ‰è¨ˆç®—æ¢¯åº¦"
    assert gradient_results.get('weights_updated', False), "æ¬Šé‡æ²’æœ‰æ›´æ–°"
    
    print("âœ… æ‰€æœ‰æ¸¬è©¦æª¢æŸ¥é€šéï¼")


if __name__ == "__main__":
    # é‹è¡Œæ¸¬è©¦
    test_complete_model_flow()
