# tests/unit_tests/test_progressive_reward_system.py\n\"\"\"\n單元測試：Progressive Reward System\n\n此文件包含針對 src.environment.progressive_reward_system.py 中各組件的單元測試。\n"\"\"\nimport unittest\nimport logging\nfrom typing import Dict, Any\n\nfrom src.environment.progressive_reward_system import (\n    BaseRewardStrategy, \n    SimpleReward, \n    IntermediateReward, \n    ComplexReward, \n    ProgressiveLearningSystem\n)\n\n# 配置日誌 (與主程式碼分開，或使用通用測試配置)\nlogger = logging.getLogger(__name__)\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(module)s - %(message)s')\n\nclass TestRewardStrategies(unittest.TestCase):\n    \"\"\"測試各種獎勵策略的計算邏輯。\"\"\"\n\n    def test_simple_reward_calculation(self):\n        logger.info(\"Testing SimpleReward calculation.\")\n        config = {\'profit_weight\': 0.7, \'risk_penalty_weight\': 0.3, \'risk_metric\': \'drawdown\'}\n        strategy = SimpleReward(config=config)\n        \n        trade_info1 = {\'realized_pnl\': 100, \'drawdown\': 10}\n        expected_reward1 = 100 * 0.7 - 10 * 0.3 # 70 - 3 = 67\n        self.assertAlmostEqual(strategy.calculate_reward(trade_info1), expected_reward1)\n\n        trade_info2 = {\'realized_pnl\': -50, \'drawdown\': 5}\n        expected_reward2 = -50 * 0.7 - 5 * 0.3 # -35 - 1.5 = -36.5\n        self.assertAlmostEqual(strategy.calculate_reward(trade_info2), expected_reward2)\n\n        trade_info3 = {\'realized_pnl\': 0, \'drawdown\': 0}\n        expected_reward3 = 0 * 0.7 - 0 * 0.3 # 0\n        self.assertAlmostEqual(strategy.calculate_reward(trade_info3), expected_reward3)\n        \n        # 測試不同的 risk_metric\n        config_max_drawdown = {\'profit_weight\': 0.8, \'risk_penalty_weight\': 0.2, \'risk_metric\': \'max_drawdown\'}\n        strategy_max_drawdown = SimpleReward(config=config_max_drawdown)\n        trade_info4 = {\'realized_pnl\': 100, \'max_drawdown\': 15, \'drawdown\': 5} # 應使用 max_drawdown\n        expected_reward4 = 100 * 0.8 - 15 * 0.2 # 80 - 3 = 77\n        self.assertAlmostEqual(strategy_max_drawdown.calculate_reward(trade_info4), expected_reward4)\n\n        # 測試缺少鍵的情況\n        trade_info_missing = {\'realized_pnl\': 100} # drawdown 缺失\n        expected_reward_missing = 100 * 0.7 - 0 * 0.3 # 70 (drawdown 預設為 0)\n        self.assertAlmostEqual(strategy.calculate_reward(trade_info_missing), expected_reward_missing)\n\n    def test_intermediate_reward_calculation(self):\n        logger.info(\"Testing IntermediateReward calculation.\")\n        config = {\n            \'sharpe_weight\': 0.4, \'pnl_weight\': 0.3, \n            \'drawdown_penalty_weight\': 0.2, \'cost_penalty_weight\': 0.1\n        }\n        strategy = IntermediateReward(config=config)\n        \n        trade_info1 = {\'realized_pnl\': 150, \'sharpe_ratio\': 1.5, \'drawdown\': 20, \'trade_cost\': 5}\n        expected_reward1 = (1.5 * 0.4) + (150 * 0.3) - (20 * 0.2) - (5 * 0.1) # 0.6 + 45 - 4 - 0.5 = 41.1\n        self.assertAlmostEqual(strategy.calculate_reward(trade_info1), expected_reward1)\n\n        trade_info2 = {\'realized_pnl\': -100, \'sharpe_ratio\': -0.5, \'drawdown\': 30, \'trade_cost\': 2}\n        expected_reward2 = (-0.5 * 0.4) + (-100 * 0.3) - (30 * 0.2) - (2 * 0.1) # -0.2 - 30 - 6 - 0.2 = -36.4\n        self.assertAlmostEqual(strategy.calculate_reward(trade_info2), expected_reward2)\n\n        # 測試缺少鍵的情況\n        trade_info_missing = {\'realized_pnl\': 100, \'sharpe_ratio\': 1.0} # drawdown 和 cost 缺失\n        expected_reward_missing = (1.0 * 0.4) + (100 * 0.3) - (0 * 0.2) - (0 * 0.1) # 0.4 + 30 = 30.4\n        self.assertAlmostEqual(strategy.calculate_reward(trade_info_missing), expected_reward_missing)\n\n    def test_complex_reward_calculation(self):\n        logger.info(\"Testing ComplexReward calculation.\")\n        config = { # 使用自定義權重進行測試\n            \'sortino_weight\': 0.25,\n            \'profit_factor_weight\': 0.15,\n            \'win_rate_weight\': 0.1,\n            \'market_adaptability_weight\': 0.2,\n            \'consistency_weight\': 0.1,\n            \'max_drawdown_penalty_weight\': 0.2\n        }\n        strategy = ComplexReward(config=config)\n\n        trade_info1 = {\n            \'sortino_ratio\': 2.0, \'profit_factor\': 1.8, \'win_rate\': 0.6,\n            \'max_drawdown\': 25, \'market_adaptability_score\': 0.7, \'behavioral_consistency_score\': 0.6\n        }\n        expected_reward1 = (\n            (2.0 * 0.25) +        # 0.5\n            ((1.8 - 1) * 0.15) +  # 0.8 * 0.15 = 0.12\n            ((0.6 - 0.5) * 0.1) + # 0.1 * 0.1 = 0.01\n            (0.7 * 0.2) +         # 0.14\n            (0.6 * 0.1) -         # 0.06\n            (25 * 0.2)            # 5.0\n        ) # 0.5 + 0.12 + 0.01 + 0.14 + 0.06 - 5.0 = 0.83 - 5.0 = -4.17\n        self.assertAlmostEqual(strategy.calculate_reward(trade_info1), expected_reward1)\n\n        # 測試 profit_factor < 1 和 win_rate < 0.5 的情況\n        trade_info2 = {\n            \'sortino_ratio\': 0.5, \'profit_factor\': 0.8, \'win_rate\': 0.4,\n            \'max_drawdown\': 10, \'market_adaptability_score\': 0.3, \'behavioral_consistency_score\': 0.2\n        }\n        expected_reward2 = (\n            (0.5 * 0.25) +        # 0.125\n            ((0.8 - 1) * 0.15) +  # -0.2 * 0.15 = -0.03\n            ((0.4 - 0.5) * 0.1) + # -0.1 * 0.1 = -0.01\n            (0.3 * 0.2) +         # 0.06\n            (0.2 * 0.1) -         # 0.02\n            (10 * 0.2)            # 2.0\n        ) # 0.125 - 0.03 - 0.01 + 0.06 + 0.02 - 2.0 = 0.165 - 2.0 = -1.835\n        self.assertAlmostEqual(strategy.calculate_reward(trade_info2), expected_reward2)\n\n        # 測試缺少鍵的情況 (使用預設權重，因為 config 中未提供所有鍵的預設值)\n        default_strategy = ComplexReward() # 測試預設權重\n        trade_info_missing = {\'sortino_ratio\': 1.0, \'max_drawdown\': 5}\n        # 預設權重: sortino=0.3, pf=0.2, wr=0.1, adapt=0.2, consist=0.1, mdd_penalty=0.1\n        # profit_factor 預設 1.0, win_rate 預設 0.5, adapt_score 預設 0, consist_score 預設 0\n        expected_reward_missing = (\n            (1.0 * 0.3) +           # 0.3\n            ((1.0 - 1) * 0.2) +   # 0\n            ((0.5 - 0.5) * 0.1) + # 0\n            (0.0 * 0.2) +           # 0\n            (0.0 * 0.1) -           # 0\n            (5 * 0.1)             # 0.5\n        ) # 0.3 - 0.5 = -0.2\n        self.assertAlmostEqual(default_strategy.calculate_reward(trade_info_missing), expected_reward_missing)\n\nclass TestProgressiveLearningSystem(unittest.TestCase):\n    \"\"\"測試 ProgressiveLearningSystem 的階段管理和獎勵計算。\"\"\"\n\n    @classmethod\n    def setUpClass(cls):\n        # 定義晉級標準函數 (靜態或類方法，以便在 pls_configs 中引用)\n        cls.stage1_criteria = lambda stats: stats.get(\'avg_sharpe_ratio\', 0) > 0.5 and stats.get(\'episodes_completed\', 0) >= 2\n        cls.stage2_criteria = lambda stats: stats.get(\'avg_sortino_ratio\', 0) > 0.8 and stats.get(\'episodes_completed\', 0) >= 3\n\n        cls.simple_reward_cfg = {\'profit_weight\': 0.7, \'risk_penalty_weight\': 0.3}\n        cls.intermediate_reward_cfg = {\'sharpe_weight\': 0.4, \'pnl_weight\': 0.3}\n        cls.complex_reward_cfg = {\'sortino_weight\': 0.25}\n\n        cls.pls_configs = {\n            1: {\n                \'reward_strategy_class\': SimpleReward,\n                \'reward_config\': cls.simple_reward_cfg,\n                \'criteria_to_advance\': cls.stage1_criteria,\n                \'max_episodes_or_steps\': 5 # 較小的 episodes 用於測試\n            },\n            2: {\n                \'reward_strategy_class\': IntermediateReward,\n                \'reward_config\': cls.intermediate_reward_cfg,\n                \'criteria_to_advance\': cls.stage2_criteria,\n                \'max_episodes_or_steps\': 5\n            },\n            3: {\n                \'reward_strategy_class\': ComplexReward,\n                \'reward_config\': cls.complex_reward_cfg,\n                \'criteria_to_advance\': None,\n                \'max_episodes_or_steps\': None\n            }\n        }\n\n    def test_initialization(self):\n        logger.info(\"Testing PLS initialization.\")\n        pls = ProgressiveLearningSystem(stage_configs=self.pls_configs, initial_stage=1)\n        self.assertEqual(pls.current_stage_number, 1)\n        self.assertIsInstance(pls.get_current_reward_function(), SimpleReward)\n        self.assertEqual(pls.get_current_reward_function().config, self.simple_reward_cfg)\n\n        pls_stage2 = ProgressiveLearningSystem(stage_configs=self.pls_configs, initial_stage=2)\n        self.assertEqual(pls_stage2.current_stage_number, 2)\n        self.assertIsInstance(pls_stage2.get_current_reward_function(), IntermediateReward)\n\n        with self.assertRaises(ValueError): # 測試無效的初始階段\n            ProgressiveLearningSystem(stage_configs=self.pls_configs, initial_stage=99)\n        \n        with self.assertRaises(ValueError): # 測試空的 stage_configs\n            ProgressiveLearningSystem(stage_configs={}, initial_stage=1)\n\n        invalid_class_config = {1: {\'reward_strategy_class\': dict, \'reward_config\': {}}}\n        with self.assertRaises(ValueError): # 測試無效的 reward_strategy_class\n            ProgressiveLearningSystem(stage_configs=invalid_class_config, initial_stage=1)\n\n    def test_reward_calculation_through_pls(self):\n        logger.info(\"Testing reward calculation via PLS.\")\n        pls = ProgressiveLearningSystem(stage_configs=self.pls_configs, initial_stage=1)\n        trade_info_simple = {\'realized_pnl\': 100, \'drawdown\': 10}\n        expected_simple_reward = 100 * 0.7 - 10 * 0.3 # 67\n        self.assertAlmostEqual(pls.calculate_reward(trade_info_simple), expected_simple_reward)\n\n        # 手動晉級到階段3並測試\n        pls.advance_stage_manually() # 到階段2\n        pls.advance_stage_manually() # 到階段3\n        self.assertEqual(pls.current_stage_number, 3)\n        self.assertIsInstance(pls.get_current_reward_function(), ComplexReward)\n        trade_info_complex = {\'sortino_ratio\': 2.0, \'profit_factor\': 1.0, \'win_rate\': 0.5, \'max_drawdown\': 0}\n        expected_complex_reward = 2.0 * 0.25 # 0.5 (其他項為0或被忽略，基於 complex_reward_cfg)\n        self.assertAlmostEqual(pls.calculate_reward(trade_info_complex), expected_complex_reward)\n\n    def test_stage_advancement_criteria(self):\n        logger.info(\"Testing PLS stage advancement by criteria.\")\n        pls = ProgressiveLearningSystem(stage_configs=self.pls_configs, initial_stage=1)\n\n        # 模擬訓練，未達到晉級標準\n        stats_fail = {\'avg_sharpe_ratio\': 0.2, \'episodes_completed\': 1}\n        pls.record_episode_end(stats_fail)\n        self.assertEqual(pls.current_stage_number, 1, \"Should not advance with failing criteria\")\n\n        # 模擬訓練，達到晉級標準 (episodes_completed = 2)\n        stats_pass_stage1 = {\'avg_sharpe_ratio\': 0.6, \'episodes_completed\': 2}\n        pls.record_episode_end(stats_pass_stage1) # episodes_in_current_stage is now 2\n        self.assertEqual(pls.current_stage_number, 2, \"Should advance to stage 2\")\n        self.assertIsInstance(pls.get_current_reward_function(), IntermediateReward)\n        self.assertEqual(pls.episode_in_current_stage, 0, \"Episode count should reset after advancing\")\n\n        # 測試階段2的晉級\n        stats_fail_stage2 = {\'avg_sortino_ratio\': 0.5, \'episodes_completed\': 1}\n        pls.record_episode_end(stats_fail_stage2)\n        self.assertEqual(pls.current_stage_number, 2)\n        pls.record_episode_end(stats_fail_stage2) # episodes_in_current_stage is now 2\n        self.assertEqual(pls.current_stage_number, 2)\n\n        stats_pass_stage2 = {\'avg_sortino_ratio\': 0.9, \'episodes_completed\': 3}\n        pls.record_episode_end(stats_pass_stage2) # episodes_in_current_stage is now 3\n        self.assertEqual(pls.current_stage_number, 3, \"Should advance to stage 3\")\n        self.assertIsInstance(pls.get_current_reward_function(), ComplexReward)\n\n        # 在最後階段嘗試晉級 (不應發生)\n        pls.record_episode_end(stats_pass_stage2) # criteria is None for stage 3\n        self.assertEqual(pls.current_stage_number, 3, \"Should remain in final stage\")\n\n    def test_stage_advancement_max_episodes(self):\n        logger.info(\"Testing PLS stage advancement by max_episodes (criteria not met).\")\n        # 修改配置，使晉級標準難以達到，以測試 max_episodes\n        strict_criteria = lambda stats: stats.get(\'avg_sharpe_ratio\', 0) > 999 \n        test_configs = {\n            1: {\n                \'reward_strategy_class\': SimpleReward,\n                \'reward_config\': self.simple_reward_cfg,\n                \'criteria_to_advance\': strict_criteria, # 非常嚴格的標準\n                \'max_episodes_or_steps\': 3 # 3 回合後應停留 (預設行為)\n            },\n            2: {\n                \'reward_strategy_class\': IntermediateReward,\n                \'reward_config\': self.intermediate_reward_cfg,\n                \'criteria_to_advance\': None\n            }\n        }\n        pls = ProgressiveLearningSystem(stage_configs=test_configs, initial_stage=1)\n        stats = {\'avg_sharpe_ratio\': 0.1, \'episodes_completed\': 0}\n        for i in range(1, 4): # 1, 2, 3\n            stats[\'episodes_completed\'] = i\n            pls.record_episode_end(stats)\n            if i < 3:\n                self.assertEqual(pls.current_stage_number, 1, f\"Should be in stage 1 after {i} episodes\")\n        \n        # 在第3個 episode 結束後，max_episodes_or_steps (3) 達到，但 criteria 未滿足\n        # 預設行為是停留在當前階段\n        self.assertEqual(pls.current_stage_number, 1, \"Should remain in stage 1 after max_episodes if criteria not met\")\n        self.assertEqual(pls.episode_in_current_stage, 3)\n\n        # 如果此時標準突然滿足 (例如，外部因素改變了 stats)\n        passing_stats = {\'avg_sharpe_ratio\': 1000, \'episodes_completed\': 4}\n        pls.record_episode_end(passing_stats) # episode_in_current_stage becomes 4\n        self.assertEqual(pls.current_stage_number, 2, \"Should advance if criteria met even after max_episodes hit\")\n\n    def test_manual_advancement(self):\n        logger.info(\"Testing PLS manual stage advancement.\")\n        pls = ProgressiveLearningSystem(stage_configs=self.pls_configs, initial_stage=1)\n        self.assertTrue(pls.advance_stage_manually(), \"Manual advance from 1 to 2 should succeed\")\n        self.assertEqual(pls.current_stage_number, 2)\n        self.assertIsInstance(pls.get_current_reward_function(), IntermediateReward)\n        self.assertEqual(pls.episode_in_current_stage, 0) # 計數器重置\n\n        self.assertTrue(pls.advance_stage_manually(), \"Manual advance from 2 to 3 should succeed\")\n        self.assertEqual(pls.current_stage_number, 3)\n        self.assertIsInstance(pls.get_current_reward_function(), ComplexReward)\n\n        self.assertFalse(pls.advance_stage_manually(), \"Manual advance from final stage should fail\")\n        self.assertEqual(pls.current_stage_number, 3)\n\n    def test_record_step_advancement(self):\n        logger.info(\"Testing PLS stage advancement by record_step.\")\n        # 假設 record_step 也使用 episodes_completed 和相同的 max_episodes_or_steps 邏輯\n        # 實際應用中，max_episodes_or_steps 可能需要區分是 episodes 還是 steps\n        # 此處的 ProgressiveLearningSystem 實作中，max_episodes_or_steps 和 criteria_fn\n        # 在 record_step 和 record_episode_end 中都會被檢查。\n        # duration_metric 預設為 \'episodes\', 所以 record_step 不會直接觸發基於 max_episodes 的停留\n        # 但如果 criteria_fn 滿足，record_step 仍可觸發晉級。\n\n        pls = ProgressiveLearningSystem(stage_configs=self.pls_configs, initial_stage=1)\n        \n        # 模擬 steps，但晉級標準基於 episodes_completed\n        stats_pass_on_step = {\'avg_sharpe_ratio\': 0.6, \'episodes_completed\': 2} # 假設這是某個 step 時的狀態\n        pls.record_step(stats_pass_on_step)\n        self.assertEqual(pls.current_stage_number, 2, \"Should advance via record_step if criteria met\")\n        self.assertEqual(pls.steps_in_current_stage, 0) # steps 計數器也應重置\n\nif __name__ == \'__main__\':\n    unittest.main()\n
