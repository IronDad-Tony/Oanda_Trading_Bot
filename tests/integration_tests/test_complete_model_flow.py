"""
å…¨é¢æ¸¬è©¦æ¨¡å‹æµç¨‹ï¼ŒåŒ…å«æ‰€æœ‰æ¨¡çµ„çš„æ¢¯åº¦æµé©—è­‰
æ¸¬è©¦ç¯„åœï¼š
1. Transformeræ¨¡å‹
2. é‡å­ç­–ç•¥å±¤ (EnhancedStrategySuperposition)
3. é‡å­ç­–ç•¥æ± 
4. è‡ªå‰µç­–ç•¥æ¨¡çµ„
5. SACå¼·åŒ–å­¸ç¿’è¨“ç·´ (QuantumEnhancedSAC)
6. å…ƒå­¸ç¿’è¨“ç·´æ¨¡çµ„
7. æ¢¯åº¦å‰å‘å‚³æ’­å’Œæ¬Šé‡æ›´æ–°
"""

import sys
import os
import torch
import torch.nn as nn
import numpy as np
import pytest
import logging
from typing import Dict, Tuple, List, Any, Optional
from unittest.mock import Mock, MagicMock, patch
import traceback

# Add src to path
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..', 'src')))

# Import all required modules
from src.models.enhanced_transformer import EnhancedTransformer
from src.agent.enhanced_quantum_strategy_layer import EnhancedStrategySuperposition, DynamicStrategyGenerator
from src.agent.sac_agent_wrapper import QuantumEnhancedSAC
from src.agent.meta_learning_system import MetaLearningSystem
from src.environment.progressive_reward_system import ProgressiveLearningSystem, SimpleReward, IntermediateReward, ComplexReward # Added reward strategy imports
from src.agent.strategy_innovation_engine import StrategyInnovationEngine

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class CompleteModelFlowTester:
    """å®Œæ•´æ¨¡å‹æµç¨‹æ¸¬è©¦å™¨"""
    
    def __init__(self):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.batch_size = 4
        self.seq_len = 100
        self.num_features = 24
        self.num_symbols = 3
        self.action_dim = 3  # [buy, sell, hold]
        
        # åˆå§‹åŒ–æ‰€æœ‰æ¨¡çµ„
        self.setup_models()
        
    def setup_models(self):
        """è¨­ç½®æ‰€æœ‰æ¨¡å‹çµ„ä»¶"""
        logger.info("è¨­ç½®æ¨¡å‹çµ„ä»¶...")
        
        # 1. Enhanced Transformer
        self.transformer_config = {
            'input_dim': self.num_features,  # Features per symbol
            'd_model': 256,
            'transformer_nhead': 8,
            'num_encoder_layers': 4,
            'dim_feedforward': 512,
            'dropout': 0.1,
            'max_seq_len': self.seq_len,
            'num_symbols': self.num_symbols,
            'output_dim': 256,  # Output dim for downstream layers
            'use_msfe': True,
            'use_fourier_features': True,
            'use_wavelet_features': True
        }
        self.transformer = EnhancedTransformer(**self.transformer_config).to(self.device)
        
        # 2. Enhanced Strategy Superposition (Quantum Strategy Layer)
        self.quantum_layer = EnhancedStrategySuperposition(
            input_dim=256,  # transformer hidden size
            num_strategies=5,
            dropout_rate=0.1,
            strategy_input_dim=256
        ).to(self.device)
        
        # 3. Mock SAC Agent (simplified for testing)
        self.sac_agent = self.create_mock_sac_agent()
        
        # 4. Meta Learning System
        self.meta_learning = MetaLearningSystem(
            initial_state_dim=256,  # é€™è£¡ç”¨ transformer è¼¸å‡ºç¶­åº¦
            action_dim=self.action_dim
        )
        
        # 5. Progressive Reward System
        # Define stage configurations for ProgressiveLearningSystem
        stage_configs = {
            1: {
                'reward_strategy_class': SimpleReward, # Assuming SimpleReward is defined
                'reward_config': {'profit_weight': 0.7, 'risk_penalty_weight': 0.3},
                'criteria_to_advance': lambda stats: stats.get('avg_sharpe_ratio', 0) > 0.5,
                'max_episodes_or_steps': 100
            },
            2: {
                'reward_strategy_class': IntermediateReward, # Assuming IntermediateReward is defined
                'reward_config': {'sharpe_weight': 0.4, 'pnl_weight': 0.3, 'drawdown_penalty_weight': 0.2, 'cost_penalty_weight': 0.1},
                'criteria_to_advance': lambda stats: stats.get('avg_sortino_ratio', 0) > 0.8,
                'max_episodes_or_steps': 500
            }
            # Add more stages if needed
        }
        self.reward_system = ProgressiveLearningSystem(
            stage_configs=stage_configs,
            initial_stage=1
        )
        
        # 6. Strategy Innovation Engine
        self.innovation_engine = self.create_mock_innovation_engine()
        
        logger.info("æ‰€æœ‰æ¨¡å‹çµ„ä»¶è¨­ç½®å®Œæˆ")
    
    def create_mock_sac_agent(self):
        """å‰µå»ºæ¨¡æ“¬çš„SACæ™ºèƒ½é«”ç”¨æ–¼æ¸¬è©¦"""
        class MockSACAgent:
            def __init__(self):
                self.memory = []
                self.batch_size = 4 # MODIFIED: Align with tester's batch_size to ensure train() is called
                
            def store_transition(self, state, action, reward, next_state, done):
                self.memory.append((state, action, reward, next_state, done))
                if len(self.memory) > 1000:
                    self.memory.pop(0)
            
            def train(self):
                if len(self.memory) < self.batch_size:
                    return {}
                return {
                    'actor_loss': np.random.uniform(0.1, 1.0),
                    'critic_loss': np.random.uniform(0.1, 1.0),
                    'alpha_loss': np.random.uniform(0.01, 0.1)
                }
        
        return MockSACAgent()
    
    def create_mock_innovation_engine(self):
        """å‰µå»ºæ¨¡æ“¬çš„ç­–ç•¥å‰µæ–°å¼•æ“"""
        class MockInnovationEngine:
            def innovate_strategy(self, market_conditions, performance_threshold=0.1):
                return {
                    'new_strategy': f"strategy_{np.random.randint(1000, 9999)}",
                    'fitness_score': np.random.uniform(0.1, 1.0),
                    'parameters': {
                        'param1': np.random.uniform(0.1, 1.0),
                        'param2': np.random.uniform(0.1, 1.0)
                    }
                }
        
        return MockInnovationEngine()
    
    def create_sample_data(self) -> Dict[str, torch.Tensor]:
        """å‰µå»ºæ¨£æœ¬æ•¸æ“š"""
        # å¸‚å ´æ•¸æ“š: [batch_size, num_symbols, seq_len, num_features]
        market_data = torch.randn(
            self.batch_size, 
            self.num_symbols,
            self.seq_len, 
            self.num_features,
            device=self.device
        )
        
        # Padding mask for symbols
        src_key_padding_mask = torch.zeros(
            self.batch_size,
            self.num_symbols,
            dtype=torch.bool,
            device=self.device
        )
        
        # å¸‚å ´ç‹€æ…‹
        market_state = torch.randint(0, 3, (self.batch_size,), device=self.device)
        
        # æ­·å²è¡Œå‹•
        actions = torch.randint(0, self.action_dim, (self.batch_size, 10), device=self.device)
        
        # çå‹µ
        rewards = torch.randn(self.batch_size, device=self.device)
        
        return {
            'market_data': market_data,
            'src_key_padding_mask': src_key_padding_mask,
            'market_state': market_state,
            'actions': actions,
            'rewards': rewards
        }
    
    def test_transformer_forward_pass(self, data: Dict[str, torch.Tensor]) -> torch.Tensor:
        """æ¸¬è©¦Transformerå‰å‘å‚³æ’­"""
        logger.info("æ¸¬è©¦Transformerå‰å‘å‚³æ’­...")
        
        try:
            # æº–å‚™è¼¸å…¥å­—å…¸
            # src: [batch_size, num_active_symbols, seq_len, input_dim]
            # market_data from create_sample_data is already in the correct shape [B, N, S, F]
            input_dict = {
                "src": data['market_data'], 
                "symbol_ids": torch.arange(self.num_symbols, device=self.device).unsqueeze(0).repeat(self.batch_size, 1),
                "src_key_padding_mask": data['src_key_padding_mask'] 
            }
            
            # å‰å‘å‚³æ’­
            output = self.transformer(input_dict)
            
            logger.info(f"Transformerè¼¸å‡ºå½¢ç‹€: {output.shape}")
            # The output shape of EnhancedTransformer is [batch_size, num_active_symbols, output_dim]
            # output_dim is set to 256 in transformer_config
            expected_shape = (self.batch_size, self.num_symbols, 256)
            assert output.shape == expected_shape, f"æœŸæœ›å½¢ç‹€ {expected_shape}, å¯¦éš› {output.shape}"
            
            # æª¢æŸ¥æ¢¯åº¦
            assert output.requires_grad, "Transformerè¼¸å‡ºæ‡‰è©²éœ€è¦æ¢¯åº¦"
            
            logger.info("âœ“ Transformerå‰å‘å‚³æ’­æ¸¬è©¦é€šé")
            return output
            
        except Exception as e:
            logger.error(f"âœ— Transformerå‰å‘å‚³æ’­æ¸¬è©¦å¤±æ•—: {e}")
            raise
    
    def test_quantum_strategy_layer(self, transformer_output: torch.Tensor, data: Dict[str, torch.Tensor]) -> torch.Tensor:
        """æ¸¬è©¦é‡å­ç­–ç•¥å±¤"""
        logger.info("æ¸¬è©¦é‡å­ç­–ç•¥å±¤...")
        
        try:
            # ä½¿ç”¨æœ€å¾Œä¸€å€‹æ™‚é–“æ­¥çš„è¼¸å‡º
            last_hidden = transformer_output[:, -1, :]  # [batch, hidden_size]
            
            # å‰å‘å‚³æ’­
            # transformer_output shape: (batch_size, num_symbols, transformer_output_dim)
            # asset_features_batch expects: (batch_size, num_assets, sequence_length, feature_dim)
            # Let num_assets = num_symbols, sequence_length = 1, feature_dim = transformer_output_dim
            asset_features = transformer_output.unsqueeze(2)  # Shape: (B, N_symbols, 1, F_transformer_out)

            # market_state_features expects: (batch_size, quantum_layer_input_dim)
            # quantum_layer_input_dim is transformer_output_dim (256)
            # Use the features of the last symbol as market_state_features, consistent with previous 'last_hidden' logic
            market_features = transformer_output[:, -1, :]  # Shape: (B, F_transformer_out)
            
            # å‰å‘å‚³æ’­
            # Pass arguments by keyword for clarity
            strategy_output_raw = self.quantum_layer(
                asset_features_batch=asset_features,
                market_state_features=market_features
            )
            
            # Output of quantum_layer is (batch_size, num_symbols, 1)
            # Squeeze the last dimension to match expected shape (batch_size, action_dim)
            # where num_symbols is effectively action_dim in this context for the test assertion.
            strategy_output = strategy_output_raw.squeeze(-1) # Shape: (batch_size, num_symbols)
            
            logger.info(f"é‡å­ç­–ç•¥å±¤è¼¸å‡ºå½¢ç‹€: {strategy_output.shape}")
            # Assuming num_symbols (3) corresponds to action_dim (3) for this test's assertion
            assert strategy_output.shape == (self.batch_size, self.num_symbols), f"æœŸæœ›å½¢ç‹€ {(self.batch_size, self.num_symbols)}, å¯¦éš› {strategy_output.shape}"
            
            # æª¢æŸ¥æ¢¯åº¦
            assert strategy_output.requires_grad, "é‡å­ç­–ç•¥å±¤è¼¸å‡ºæ‡‰è©²éœ€è¦æ¢¯åº¦"
            
            logger.info("âœ“ é‡å­ç­–ç•¥å±¤æ¸¬è©¦é€šé")
            return strategy_output
            
        except Exception as e:
            logger.error(f"âœ— é‡å­ç­–ç•¥å±¤æ¸¬è©¦å¤±æ•—: {e}")
            raise
    
    def test_sac_agent_training(self, strategy_output: torch.Tensor, data: Dict[str, torch.Tensor]) -> Dict[str, float]:
        """æ¸¬è©¦SACæ™ºèƒ½é«”è¨“ç·´"""
        logger.info("æ¸¬è©¦SACæ™ºèƒ½é«”è¨“ç·´...")
        
        try:
            # æº–å‚™SACè¨“ç·´æ•¸æ“š
            states = strategy_output.detach()  # ä½¿ç”¨ç­–ç•¥è¼¸å‡ºä½œç‚ºç‹€æ…‹
            actions = torch.softmax(strategy_output, dim=-1)  # è»Ÿå‹•ä½œ
            next_states = states + torch.randn_like(states) * 0.1
            rewards = data['rewards']
            dones = torch.zeros(self.batch_size, dtype=torch.bool, device=self.device)
            
            # å­˜å„²è½‰æ›
            for i in range(self.batch_size):
                self.sac_agent.store_transition(
                    states[i].cpu().numpy(),
                    actions[i].cpu().detach().numpy(), # MODIFIED: Added .detach()
                    rewards[i].item(),
                    next_states[i].cpu().numpy(),
                    dones[i].item()
                )
            
            # è¨“ç·´SAC
            if len(self.sac_agent.memory) >= self.sac_agent.batch_size:
                losses = self.sac_agent.train()
                logger.info(f"SACè¨“ç·´æå¤±: {losses}")
                
                # æª¢æŸ¥æå¤±å€¼çš„åˆç†æ€§
                for loss_name, loss_value in losses.items():
                    assert not np.isnan(loss_value), f"SAC {loss_name} æå¤±ç‚ºNaN"
                    assert not np.isinf(loss_value), f"SAC {loss_name} æå¤±ç‚ºç„¡é™å¤§"
                
                logger.info("âœ“ SACæ™ºèƒ½é«”è¨“ç·´æ¸¬è©¦é€šé")
                return losses
            else:
                logger.info("SACè¨˜æ†¶é«”ä¸è¶³ï¼Œè·³éè¨“ç·´")
                return {}
            
        except Exception as e:
            logger.error(f"âœ— SACæ™ºèƒ½é«”è¨“ç·´æ¸¬è©¦å¤±æ•—: {e}")
            raise
    
    # MODIFIED: Changed signature to accept transformer_output
    def test_meta_learning_adaptation(self, transformer_output: torch.Tensor) -> Dict[str, Any]:
        """æ¸¬è©¦å…ƒå­¸ç¿’é©æ‡‰"""
        logger.info("æ¸¬è©¦å…ƒå­¸ç¿’é©æ‡‰...")
        
        try:
            # æ¨¡æ“¬ç­–ç•¥è¡¨ç¾æ•¸æ“š
            strategy_performance = {
                'strategy_A': {'reward': 10.5, 'uncertainty': 0.2},
                'strategy_B': {'reward': -2.3, 'uncertainty': 0.5}
            }
            
            # æ¨¡æ“¬å¸‚å ´ä¸Šä¸‹æ–‡ from transformer_output
            # transformer_output shape: (batch_size, num_symbols, 256)
            # MetaLearningSystem initialized with initial_state_dim=256
            # Use features from the first symbol for the whole batch, matching expected dim.
            market_context = transformer_output[:, 0, :] # Shape: (batch_size, 256)
            
            # åŸ·è¡Œå…ƒå­¸ç¿’é©æ‡‰
            adaptation_results = self.meta_learning.adapt_strategies(strategy_performance, market_context)
            
            logger.info(f"å…ƒå­¸ç¿’é©æ‡‰çµæœ: {adaptation_results}")
            
            # æª¢æŸ¥é©æ‡‰çµæœ
            assert isinstance(adaptation_results, dict), "é©æ‡‰çµæœæ‡‰ç‚ºå­—å…¸æ ¼å¼"
            assert 'ç­–ç•¥A' in adaptation_results, "ç¼ºå°‘ç­–ç•¥Açš„é©æ‡‰çµæœ"
            assert 'ç­–ç•¥B' in adaptation_results, "ç¼ºå°‘ç­–ç•¥Bçš„é©æ‡‰çµæœ"
            
            logger.info("âœ“ å…ƒå­¸ç¿’é©æ‡‰æ¸¬è©¦é€šé")
            return adaptation_results
            
        except Exception as e:
            logger.error(f"âœ— å…ƒå­¸ç¿’é©æ‡‰æ¸¬è©¦å¤±æ•—: {e}")
            raise
    
    def test_strategy_innovation(self) -> Dict[str, Any]:
        """æ¸¬è©¦ç­–ç•¥å‰µæ–°"""
        logger.info("æ¸¬è©¦ç­–ç•¥å‰µæ–°...")
        
        try:
            # å‰µæ–°æ–°ç­–ç•¥
            market_conditions = {
                'volatility': np.random.uniform(0.1, 0.3),
                'trend': np.random.choice(['bullish', 'bearish', 'sideways']),
                'volume': np.random.uniform(0.5, 2.0)
            }
            
            innovation_result = self.innovation_engine.innovate_strategy(
                market_conditions,
                performance_threshold=0.1
            )
            
            logger.info(f"ç­–ç•¥å‰µæ–°çµæœ: {innovation_result}")
            
            # æª¢æŸ¥å‰µæ–°çµæœ
            assert 'new_strategy' in innovation_result, "ç¼ºå°‘æ–°ç­–ç•¥"
            assert 'fitness_score' in innovation_result, "ç¼ºå°‘é©æ‡‰åº¦è©•åˆ†"
            
            logger.info("âœ“ ç­–ç•¥å‰µæ–°æ¸¬è©¦é€šé")
            return innovation_result
            
        except Exception as e:
            logger.error(f"âœ— ç­–ç•¥å‰µæ–°æ¸¬è©¦å¤±æ•—: {e}")
            raise
    
    def test_gradient_flow(self, data: Dict[str, torch.Tensor]) -> Dict[str, bool]:
        """æ¸¬è©¦æ¢¯åº¦æµå’Œæ¬Šé‡æ›´æ–°"""
        logger.info("æ¸¬è©¦æ¢¯åº¦æµå’Œæ¬Šé‡æ›´æ–°...")
        
        try:
            results = {}
            
            # è¨˜éŒ„åˆå§‹æ¬Šé‡
            initial_weights = {}
            for name, param in self.transformer.named_parameters():
                if param.requires_grad:
                    initial_weights[f"transformer_{name}"] = param.clone().detach()
            
            for name, param in self.quantum_layer.named_parameters():
                if param.requires_grad:
                    initial_weights[f"quantum_{name}"] = param.clone().detach()
            
            # å®Œæ•´å‰å‘å‚³æ’­
            market_data_reshaped = data['market_data'].view(
                self.batch_size, self.seq_len, self.num_symbols * self.num_features
            )
            
            transformer_output = self.transformer(
                market_data_reshaped,
                src_key_padding_mask=data['src_key_padding_mask']
            )
            
            last_hidden = transformer_output[:, -1, :]
            strategy_output = self.quantum_layer(
                last_hidden,
                market_state=data['market_state']
            )
            
            # è¨ˆç®—æå¤±
            target = torch.randn_like(strategy_output)
            loss = nn.MSELoss()(strategy_output, target)
            
            logger.info(f"è¨ˆç®—æå¤±: {loss.item()}")
            
            # åå‘å‚³æ’­
            loss.backward()
            
            # æª¢æŸ¥æ¢¯åº¦
            gradient_check_results = {}
            
            # æª¢æŸ¥Transformeræ¢¯åº¦
            transformer_has_gradients = False
            for name, param in self.transformer.named_parameters():
                if param.requires_grad and param.grad is not None:
                    grad_norm = param.grad.norm().item()
                    if grad_norm > 1e-8:
                        transformer_has_gradients = True
                        gradient_check_results[f"transformer_{name}"] = grad_norm
            
            # æª¢æŸ¥é‡å­ç­–ç•¥å±¤æ¢¯åº¦
            quantum_has_gradients = False
            for name, param in self.quantum_layer.named_parameters():
                if param.requires_grad and param.grad is not None:
                    grad_norm = param.grad.norm().item()
                    if grad_norm > 1e-8:
                        quantum_has_gradients = True
                        gradient_check_results[f"quantum_{name}"] = grad_norm
            
            results['transformer_gradients'] = transformer_has_gradients
            results['quantum_gradients'] = quantum_has_gradients
            
            logger.info(f"æ¢¯åº¦æª¢æŸ¥çµæœ: {gradient_check_results}")
            
            # æ¨¡æ“¬å„ªåŒ–å™¨æ­¥é©Ÿ
            optimizer = torch.optim.Adam(
                list(self.transformer.parameters()) + list(self.quantum_layer.parameters()),
                lr=1e-4
            )
            optimizer.step()
            
            # æª¢æŸ¥æ¬Šé‡æ›´æ–°
            weights_updated = {}
            
            for name, param in self.transformer.named_parameters():
                if param.requires_grad and f"transformer_{name}" in initial_weights:
                    initial = initial_weights[f"transformer_{name}"]
                    current = param.detach()
                    diff = torch.norm(current - initial).item()
                    weights_updated[f"transformer_{name}"] = diff > 1e-8
            
            for name, param in self.quantum_layer.named_parameters():
                if param.requires_grad and f"quantum_{name}" in initial_weights:
                    initial = initial_weights[f"quantum_{name}"]
                    current = param.detach()
                    diff = torch.norm(current - initial).item()
                    weights_updated[f"quantum_{name}"] = diff > 1e-8
            
            results['weights_updated'] = any(weights_updated.values())
            results['weight_update_details'] = weights_updated
            
            logger.info(f"æ¬Šé‡æ›´æ–°çµæœ: {weights_updated}")
            logger.info("âœ“ æ¢¯åº¦æµå’Œæ¬Šé‡æ›´æ–°æ¸¬è©¦é€šé")
            
            return results
            
        except Exception as e:
            logger.error(f"âœ— æ¢¯åº¦æµå’Œæ¬Šé‡æ›´æ–°æ¸¬è©¦å¤±æ•—: {e}")
            traceback.print_exc()
            raise
    
    def test_progressive_reward_system(self, data: Dict[str, torch.Tensor]) -> Dict[str, Any]:
        """æ¸¬è©¦æ¼¸é€²å¼çå‹µç³»çµ±"""
        logger.info("æ¸¬è©¦æ¼¸é€²å¼çå‹µç³»çµ±...")
        
        try:
            # æ¨¡æ“¬äº¤æ˜“çµæœ
            trading_results = {
                'profit_loss': data['rewards'].cpu().numpy(),
                'sharpe_ratio': np.random.uniform(0.5, 2.0, self.batch_size),
                'max_drawdown': np.random.uniform(0.05, 0.15, self.batch_size),
                'volatility': np.random.uniform(0.1, 0.3, self.batch_size),
                'trade_count': np.random.randint(10, 100, self.batch_size)
            }
            
            # è¨ˆç®—ç•¶å‰éšæ®µçå‹µ
            current_reward_func = self.reward_system.get_current_reward_function()
            rewards = []
            
            for i in range(self.batch_size):
                batch_results = {k: v[i] if isinstance(v, np.ndarray) else v for k, v in trading_results.items()}
                reward = current_reward_func.calculate_reward(batch_results, data['market_state'][i].item())
                rewards.append(reward)
            
            rewards = np.array(rewards)
            
            logger.info(f"ç•¶å‰éšæ®µ: {self.reward_system.current_stage}")
            logger.info(f"è¨ˆç®—çå‹µ: {rewards}")
            
            # æª¢æŸ¥éšæ®µè½‰æ›
            stage_before = self.reward_system.current_stage
            self.reward_system.update_progress(np.mean(rewards))
            stage_after = self.reward_system.current_stage
            
            results = {
                'current_stage': stage_after,
                'stage_changed': stage_before != stage_after,
                'rewards': rewards.tolist(),
                'mean_reward': np.mean(rewards)
            }
            
            logger.info("âœ“ æ¼¸é€²å¼çå‹µç³»çµ±æ¸¬è©¦é€šé")
            return results
            
        except Exception as e:
            logger.error(f"âœ— æ¼¸é€²å¼çå‹µç³»çµ±æ¸¬è©¦å¤±æ•—: {e}")
            raise
    
    def run_complete_test(self) -> Dict[str, Any]:
        """é‹è¡Œå®Œæ•´çš„æ¸¬è©¦æµç¨‹"""
        logger.info("é–‹å§‹å®Œæ•´æ¨¡å‹æµç¨‹æ¸¬è©¦...")
        
        results = {}
        
        try:
            # 1. å‰µå»ºæ¸¬è©¦æ•¸æ“š
            data = self.create_sample_data()
            logger.info("âœ“ æ¸¬è©¦æ•¸æ“šå‰µå»ºå®Œæˆ")
            
            # 2. æ¸¬è©¦Transformer
            transformer_output = self.test_transformer_forward_pass(data)
            results['transformer'] = True
            
            # 3. æ¸¬è©¦é‡å­ç­–ç•¥å±¤
            strategy_output = self.test_quantum_strategy_layer(transformer_output, data)
            results['quantum_strategy'] = True
            
            # 4. æ¸¬è©¦SACæ™ºèƒ½é«”
            sac_losses = self.test_sac_agent_training(strategy_output, data)
            results['sac_agent'] = True
            results['sac_losses'] = sac_losses
            
            # 5. æ¸¬è©¦å…ƒå­¸ç¿’
            meta_results = self.test_meta_learning_adaptation(transformer_output) 
            results['meta_learning'] = True
            results['meta_results'] = meta_results
            
            # 6. æ¸¬è©¦ç­–ç•¥å‰µæ–°
            innovation_results = self.test_strategy_innovation()
            results['strategy_innovation'] = True
            results['innovation_results'] = innovation_results
            
            # 7. æ¸¬è©¦æ¢¯åº¦æµ
            gradient_results = self.test_gradient_flow(data)
            results['gradient_flow'] = True
            results['gradient_results'] = gradient_results
            
            # 8. æ¸¬è©¦æ¼¸é€²å¼çå‹µç³»çµ±
            reward_results = self.test_progressive_reward_system(data)
            results['progressive_reward'] = True
            results['reward_results'] = reward_results
            
            # æ•´é«”æˆåŠŸ
            results['overall_success'] = True
            results['all_tests_passed'] = all([
                results['transformer'],
                results['quantum_strategy'],
                results['sac_agent'],
                results['meta_learning'],
                results['strategy_innovation'],
                results['gradient_flow'],
                results['progressive_reward']
            ])
            
            logger.info("ğŸ‰ å®Œæ•´æ¨¡å‹æµç¨‹æ¸¬è©¦å…¨éƒ¨é€šéï¼")
            
        except Exception as e:
            results['overall_success'] = False
            results['error'] = str(e)
            results['traceback'] = traceback.format_exc()
            logger.error(f"å®Œæ•´æ¨¡å‹æµç¨‹æ¸¬è©¦å¤±æ•—: {e}")
            
        return results


def test_complete_model_flow():
    """å®Œæ•´æ¨¡å‹æµç¨‹æ¸¬è©¦å‡½æ•¸"""
    tester = CompleteModelFlowTester()
    results = tester.run_complete_test()
    
    # æ‰“å°è©³ç´°çµæœ
    print("\n" + "="*80)
    print("å®Œæ•´æ¨¡å‹æµç¨‹æ¸¬è©¦çµæœ")
    print("="*80)
    
    for key, value in results.items():
        if key not in ['gradient_results', 'meta_results', 'innovation_results', 'reward_results', 'sac_losses']:
            print(f"{key}: {value}")
    
    print("\nè©³ç´°çµæœ:")
    if 'gradient_results' in results:
        print(f"æ¢¯åº¦æµçµæœ: {results['gradient_results']}")
    if 'meta_results' in results:
        print(f"å…ƒå­¸ç¿’çµæœ: {results['meta_results']}")
    if 'sac_losses' in results:
        print(f"SACæå¤±: {results['sac_losses']}")
    
    print("="*80)
    
    # æ–·è¨€æ¸¬è©¦çµæœ
    assert results.get('overall_success', False), f"æ¸¬è©¦å¤±æ•—: {results.get('error', 'Unknown error')}"
    assert results.get('all_tests_passed', False), "ä¸æ˜¯æ‰€æœ‰æ¸¬è©¦éƒ½é€šé"
    
    # ç‰¹åˆ¥æª¢æŸ¥æ¢¯åº¦æµ
    gradient_results = results.get('gradient_results', {})
    assert gradient_results.get('transformer_gradients', False), "Transformeræ²’æœ‰è¨ˆç®—æ¢¯åº¦"
    assert gradient_results.get('quantum_gradients', False), "é‡å­ç­–ç•¥å±¤æ²’æœ‰è¨ˆç®—æ¢¯åº¦"
    assert gradient_results.get('weights_updated', False), "æ¬Šé‡æ²’æœ‰æ›´æ–°"
    
    print("âœ… æ‰€æœ‰æ¸¬è©¦æª¢æŸ¥é€šéï¼")


if __name__ == "__main__":
    # é‹è¡Œæ¸¬è©¦
    test_complete_model_flow()
