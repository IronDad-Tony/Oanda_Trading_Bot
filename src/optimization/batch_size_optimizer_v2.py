# src/optimization/batch_size_optimizer_v2.py
"""
æ‰¹æ¬¡å¤§å°è‡ªå‹•å„ªåŒ–å·¥å…· v2
åŸºæ–¼GPUèƒ½åŠ›æ¸¬è©¦çµæœè‡ªå‹•å„ªåŒ–æ‰€æœ‰é…ç½®æ–‡ä»¶ä¸­çš„æ‰¹æ¬¡å¤§å°

ä¸»è¦åŠŸèƒ½ï¼š
1. æª¢æ¸¬ç•¶å‰é…ç½®çš„æ‰¹æ¬¡å¤§å°
2. åŸºæ–¼GPUæ¸¬        
        def search_recursive(obj, path=""):
            if isinstance(obj, dict):
                for key, value in obj.items():
                    new_path = f"{path}.{key}" if path else key
                    if key == "batch_size" and isinstance(value, int):
                        batch_sizes[new_path] = value
                    elif isinstance(value, (dict, list)):
                        search_recursive(value, new_path)
            elif isinstance(obj, list):
                for i, item in enumerate(obj):
                    search_recursive(item, f"{path}[{i}]")
        
        search_recursive(content)
        return batch_sizes. è‡ªå‹•æ›´æ–°æ‰€æœ‰ç›¸é—œé…ç½®æ–‡ä»¶
4. æä¾›å®‰å…¨çš„å›é€€æ©Ÿåˆ¶
"""

import json
import re
import sys
import os
from pathlib import Path
from typing import Dict, List, Tuple, Any
import torch
import torch.nn as nn
import logging
from datetime import datetime

# æ·»åŠ é …ç›®æ ¹ç›®éŒ„åˆ°Pythonè·¯å¾‘
project_root = Path(__file__).resolve().parent.parent.parent
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

# å˜—è©¦å°å…¥æ¨¡çµ„
try:
    from src.common.logger_setup import logger
except ImportError:
    try:
        # å˜—è©¦ç›´æ¥å°å…¥
        sys.path.append(str(project_root))
        from src.common.logger_setup import logger
    except ImportError:
        # å‰µå»ºåŸºæœ¬logger
        logger = logging.getLogger(__name__)
        logger.setLevel(logging.INFO)
        handler = logging.StreamHandler()
        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        handler.setFormatter(formatter)
        logger.addHandler(handler)

class BatchSizeOptimizer:
    """æ‰¹æ¬¡å¤§å°å„ªåŒ–å™¨"""
    
    def __init__(self, project_root: Path = None):
        self.project_root = project_root or Path(__file__).resolve().parent.parent.parent
        self.config_files = self._discover_config_files()
        self.backup_dir = self.project_root / "backups" / "config_backups"
        self.backup_dir.mkdir(parents=True, exist_ok=True)
        
    def _discover_config_files(self) -> Dict[str, Path]:
        """ç™¼ç¾æ‰€æœ‰åŒ…å«æ‰¹æ¬¡å¤§å°çš„é…ç½®æ–‡ä»¶"""
        config_files = {}
        
        # ä¸»è¦é…ç½®æ–‡ä»¶
        main_config = self.project_root / "src" / "common" / "config.py"
        if main_config.exists():
            config_files["main_config"] = main_config
            
        # Transformeré…ç½®æ–‡ä»¶
        transformer_config = self.project_root / "configs" / "enhanced_transformer_config.json"
        if transformer_config.exists():
            config_files["transformer_config"] = transformer_config
            
        return config_files
    
    def analyze_current_batch_sizes(self) -> Dict[str, Any]:
        """åˆ†æç•¶å‰çš„æ‰¹æ¬¡å¤§å°é…ç½®"""
        current_configs = {}
        
        for config_name, config_path in self.config_files.items():
            try:
                if config_path.suffix == ".py":
                    # Pythoné…ç½®æ–‡ä»¶
                    with open(config_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                    
                    batch_sizes = self._extract_batch_sizes_from_python(content)
                    if batch_sizes:
                        current_configs[config_name] = {
                            "type": "python",
                            "path": str(config_path),
                            "batch_sizes": batch_sizes
                        }
                        
                elif config_path.suffix == ".json":
                    # JSONé…ç½®æ–‡ä»¶
                    with open(config_path, 'r', encoding='utf-8') as f:
                        content = json.load(f)
                    
                    batch_sizes = self._extract_batch_sizes_from_json(content)
                    if batch_sizes:
                        current_configs[config_name] = {
                            "type": "json",
                            "path": str(config_path),
                            "batch_sizes": batch_sizes,
                            "content": content
                        }
                        
            except Exception as e:
                logger.warning(f"ç„¡æ³•åˆ†æé…ç½®æ–‡ä»¶ {config_path}: {e}")
                
        return current_configs
    
    def _extract_batch_sizes_from_python(self, content: str) -> Dict[str, int]:
        """å¾Pythonæ–‡ä»¶ä¸­æå–æ‰¹æ¬¡å¤§å°"""
        batch_sizes = {}
        
        # åŒ¹é…å„ç¨®æ‰¹æ¬¡å¤§å°è®Šæ•¸
        patterns = [
            (r'SAC_BATCH_SIZE\s*=\s*(\d+)', 'SAC_BATCH_SIZE'),
            (r'BATCH_SIZE\s*=\s*(\d+)', 'BATCH_SIZE'),
            (r'batch_size\s*=\s*(\d+)', 'batch_size'),
            (r'TRANSFORMER_BATCH_SIZE\s*=\s*(\d+)', 'TRANSFORMER_BATCH_SIZE')
        ]
        
        for pattern, var_name in patterns:
            matches = re.findall(pattern, content)
            if matches:                batch_sizes[var_name] = int(matches[0])
                
        return batch_sizes
    
    def _extract_batch_sizes_from_json(self, content: dict) -> Dict[str, int]:
        """å¾JSONé…ç½®ä¸­æå–æ‰¹æ¬¡å¤§å°"""
        batch_sizes = {}
        
        def search_recursive(obj, path=""):
            if isinstance(obj, dict):
                for key, value in obj.items():
                    new_path = f"{path}.{key}" if path else key
                    if key == "batch_size" and isinstance(value, int):
                        batch_sizes[new_path] = value
                    elif isinstance(value, (dict, list)):
                        search_recursive(value, new_path)
            elif isinstance(obj, list):
                for i, item in enumerate(obj):
                    search_recursive(item, f"{path}[{i}]")
        
        search_recursive(content)
        return batch_sizes
    
    def calculate_optimal_batch_sizes(self, gpu_test_result: int = None) -> Dict[str, int]:
        """è¨ˆç®—æœ€ä½³æ‰¹æ¬¡å¤§å°"""
        if gpu_test_result is None:
            # é‹è¡ŒGPUæ¸¬è©¦
            try:
                # å‰µå»ºä¸€å€‹ç°¡å–®çš„æ¸¬è©¦æ¨¡å‹ä¾†é€²è¡ŒGPUæ¸¬è©¦
                test_model = self._create_test_model()
                sys.path.append(str(self.project_root))
                from src.optimization.performance_optimizer import PerformanceOptimizer
                optimizer = PerformanceOptimizer(test_model)
                result = optimizer.test_optimal_batch_size()
                optimal_batch_size = result["recommended_batch_size"]
                logger.info(f"GPUæ¸¬è©¦å®Œæˆï¼Œæœ€ä½³æ‰¹æ¬¡å¤§å°: {optimal_batch_size}")
            except Exception as e:
                logger.warning(f"GPUæ¸¬è©¦å¤±æ•—ï¼Œä½¿ç”¨é è¨­å€¼: {e}")
                # æ ¹æ“šä¹‹å‰çš„æ¸¬è©¦çµæœï¼Œæˆ‘å€‘çŸ¥é“RTX 4060 Tiå¯ä»¥è™•ç†204çš„æ‰¹æ¬¡å¤§å°
                if torch.cuda.is_available():
                    gpu_name = torch.cuda.get_device_name(0)
                    if "RTX 4060 Ti" in gpu_name:
                        optimal_batch_size = 204  # ä½¿ç”¨ä¹‹å‰æ¸¬è©¦çš„çµæœ
                        logger.info(f"ä½¿ç”¨RTX 4060 Tiçš„å·²çŸ¥æœ€ä½³æ‰¹æ¬¡å¤§å°: {optimal_batch_size}")
                    else:
                        optimal_batch_size = 128
                else:
                    optimal_batch_size = 64
        else:
            optimal_batch_size = gpu_test_result
        
        # ç‚ºä¸åŒç”¨é€”è¨ˆç®—åˆé©çš„æ‰¹æ¬¡å¤§å°
        optimal_configs = {
            "SAC_BATCH_SIZE": min(optimal_batch_size, 128),  # SACä¸éœ€è¦å¤ªå¤§
            "transformer_batch_size": optimal_batch_size,     # Transformerå¯ä»¥ç”¨è¼ƒå¤§çš„
            "general_batch_size": optimal_batch_size // 2,    # é€šç”¨æ‰¹æ¬¡å¤§å°
            "evaluation_batch_size": optimal_batch_size // 4  # è©•ä¼°ç”¨è¼ƒå°çš„
        }
        
        return optimal_configs
    
    def _create_test_model(self) -> nn.Module:
        """å‰µå»ºä¸€å€‹ç°¡å–®çš„æ¸¬è©¦æ¨¡å‹ç”¨æ–¼GPUæ¸¬è©¦"""
        import torch.nn as nn
        
        class TestModel(nn.Module):
            def __init__(self):
                super().__init__()
                self.linear = nn.Linear(768, 256)
                self.transformer = nn.TransformerEncoder(
                    nn.TransformerEncoderLayer(d_model=256, nhead=8), 
                    num_layers=2
                )
                self.output = nn.Linear(256, 1)
                
            def forward(self, x):
                x = self.linear(x)
                x = self.transformer(x)
                return self.output(x.mean(dim=1))
        
        return TestModel()
    
    def backup_configs(self) -> List[str]:
        """å‚™ä»½ç•¶å‰é…ç½®"""
        backup_files = []
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        for config_name, config_path in self.config_files.items():
            try:
                # åªå‚™ä»½æ–‡ä»¶ï¼Œä¸å‚™ä»½ç›®éŒ„
                if config_path.is_file():
                    backup_path = self.backup_dir / f"{config_name}_backup_{timestamp}{config_path.suffix}"
                    backup_path.write_text(config_path.read_text(encoding='utf-8'), encoding='utf-8')
                    backup_files.append(str(backup_path))
                    logger.info(f"å·²å‚™ä»½ {config_path} -> {backup_path}")
            except Exception as e:
                logger.error(f"å‚™ä»½å¤±æ•— {config_path}: {e}")
                
        return backup_files
    
    def update_config_files(self, optimal_configs: Dict[str, int]) -> Dict[str, bool]:
        """æ›´æ–°é…ç½®æ–‡ä»¶"""
        update_results = {}
        current_configs = self.analyze_current_batch_sizes()
        
        for config_name, config_info in current_configs.items():
            try:
                if config_info["type"] == "python":
                    success = self._update_python_config(config_info, optimal_configs)
                elif config_info["type"] == "json":
                    success = self._update_json_config(config_info, optimal_configs)
                else:
                    success = False
                    
                update_results[config_name] = success
                
            except Exception as e:
                logger.error(f"æ›´æ–°é…ç½®å¤±æ•— {config_name}: {e}")
                update_results[config_name] = False
                
        return update_results
    
    def _update_python_config(self, config_info: Dict, optimal_configs: Dict[str, int]) -> bool:
        """æ›´æ–°Pythoné…ç½®æ–‡ä»¶"""
        try:
            config_path = Path(config_info["path"])
            with open(config_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # æ›´æ–°SAC_BATCH_SIZE
            if "SAC_BATCH_SIZE" in optimal_configs:
                pattern = r'SAC_BATCH_SIZE\s*=\s*\d+'
                replacement = f'SAC_BATCH_SIZE = {optimal_configs["SAC_BATCH_SIZE"]}'
                content = re.sub(pattern, replacement, content)
                logger.info(f"æ›´æ–° SAC_BATCH_SIZE ç‚º {optimal_configs['SAC_BATCH_SIZE']}")
            
            # æ›´æ–°å…¶ä»–æ‰¹æ¬¡å¤§å°è®Šæ•¸
            for var_pattern in ['BATCH_SIZE', 'batch_size']:
                if var_pattern in content and "general_batch_size" in optimal_configs:
                    pattern = f'{var_pattern}\\s*=\\s*\\d+'
                    replacement = f'{var_pattern} = {optimal_configs["general_batch_size"]}'
                    content = re.sub(pattern, replacement, content)
                    logger.info(f"æ›´æ–° {var_pattern} ç‚º {optimal_configs['general_batch_size']}")
            
            # å¯«å›æ–‡ä»¶
            with open(config_path, 'w', encoding='utf-8') as f:
                f.write(content)
                
            logger.info(f"å·²æ›´æ–°Pythoné…ç½®: {config_path}")
            return True
            
        except Exception as e:
            logger.error(f"æ›´æ–°Pythoné…ç½®å¤±æ•—: {e}")
            return False
    
    def _update_json_config(self, config_info: Dict, optimal_configs: Dict[str, int]) -> bool:
        """æ›´æ–°JSONé…ç½®æ–‡ä»¶"""
        try:
            config_path = Path(config_info["path"])
            content = config_info["content"].copy()
            
            # æ›´æ–°è¨“ç·´ç›¸é—œçš„æ‰¹æ¬¡å¤§å°
            if "training" in content and "batch_size" in content["training"]:
                old_value = content["training"]["batch_size"]
                content["training"]["batch_size"] = optimal_configs.get("transformer_batch_size", 32)
                logger.info(f"æ›´æ–° training.batch_size: {old_value} -> {content['training']['batch_size']}")
            
            # æ›´æ–°è©•ä¼°ç›¸é—œçš„æ‰¹æ¬¡å¤§å°
            if "evaluation" in content and "batch_size" in content["evaluation"]:
                old_value = content["evaluation"]["batch_size"]
                content["evaluation"]["batch_size"] = optimal_configs.get("evaluation_batch_size", 16)
                logger.info(f"æ›´æ–° evaluation.batch_size: {old_value} -> {content['evaluation']['batch_size']}")
            
            # å¯«å›æ–‡ä»¶
            with open(config_path, 'w', encoding='utf-8') as f:
                json.dump(content, f, indent=2, ensure_ascii=False)
                
            logger.info(f"å·²æ›´æ–°JSONé…ç½®: {config_path}")
            return True
            
        except Exception as e:
            logger.error(f"æ›´æ–°JSONé…ç½®å¤±æ•—: {e}")
            return False
    
    def optimize_batch_sizes(self, gpu_test_result: int = None, 
                           create_backup: bool = True) -> Dict[str, Any]:
        """åŸ·è¡Œæ‰¹æ¬¡å¤§å°å„ªåŒ–"""
        logger.info("ğŸš€ é–‹å§‹æ‰¹æ¬¡å¤§å°å„ªåŒ–...")
        
        # åˆ†æç•¶å‰é…ç½®
        current_configs = self.analyze_current_batch_sizes()
        logger.info(f"ğŸ“‹ ç™¼ç¾ {len(current_configs)} å€‹é…ç½®æ–‡ä»¶")
        
        # è¨ˆç®—æœ€ä½³æ‰¹æ¬¡å¤§å°
        optimal_configs = self.calculate_optimal_batch_sizes(gpu_test_result)
        logger.info(f"ğŸ¯ è¨ˆç®—å‡ºæœ€ä½³æ‰¹æ¬¡å¤§å°: {optimal_configs}")
        
        # å‚™ä»½ç¾æœ‰é…ç½®
        backup_files = []
        if create_backup:
            backup_files = self.backup_configs()
            logger.info(f"ğŸ’¾ å·²å‚™ä»½ {len(backup_files)} å€‹æ–‡ä»¶")
        
        # æ›´æ–°é…ç½®æ–‡ä»¶
        update_results = self.update_config_files(optimal_configs)
        
        # ç”Ÿæˆå ±å‘Š
        report = {
            "timestamp": datetime.now().isoformat(),
            "current_configs": current_configs,
            "optimal_configs": optimal_configs,
            "update_results": update_results,
            "backup_files": backup_files,
            "success_rate": sum(update_results.values()) / len(update_results) if update_results else 0
        }
        
        # ä¿å­˜å ±å‘Š
        try:
            report_dir = self.project_root / "reports"
            report_dir.mkdir(exist_ok=True)
            report_path = report_dir / "batch_size_optimization.json"
            with open(report_path, 'w', encoding='utf-8') as f:
                json.dump(report, f, indent=2, ensure_ascii=False)
            logger.info(f"ğŸ“Š å„ªåŒ–å ±å‘Šå·²ä¿å­˜è‡³: {report_path}")
        except Exception as e:
            logger.warning(f"ç„¡æ³•ä¿å­˜å ±å‘Š: {e}")
        
        logger.info("âœ… æ‰¹æ¬¡å¤§å°å„ªåŒ–å®Œæˆ")
        return report
    
    def print_optimization_summary(self, report: Dict[str, Any]):
        """æ‰“å°å„ªåŒ–æ‘˜è¦"""
        print("\n" + "="*60)
        print("ğŸš€ æ‰¹æ¬¡å¤§å°å„ªåŒ–å ±å‘Š")
        print("="*60)
        
        print(f"\nğŸ“‹ ç™¼ç¾çš„é…ç½®æ–‡ä»¶:")
        for config_name, config_info in report["current_configs"].items():
            print(f"  â€¢ {config_name}: {config_info['path']}")
            for var_name, current_value in config_info["batch_sizes"].items():
                print(f"    - {var_name}: {current_value}")
        
        print(f"\nğŸ¯ å»ºè­°çš„æœ€ä½³æ‰¹æ¬¡å¤§å°:")
        for config_name, optimal_value in report["optimal_configs"].items():
            print(f"  â€¢ {config_name}: {optimal_value}")
        
        print(f"\nâœ… æ›´æ–°çµæœ:")
        success_count = sum(report["update_results"].values())
        total_count = len(report["update_results"])
        print(f"  æˆåŠŸæ›´æ–°: {success_count}/{total_count} å€‹é…ç½®æ–‡ä»¶")
        
        for config_name, success in report["update_results"].items():
            icon = "âœ…" if success else "âŒ"
            print(f"  {icon} {config_name}")
        
        if report["backup_files"]:
            print(f"\nğŸ’¾ å‚™ä»½æ–‡ä»¶:")
            for backup_file in report["backup_files"]:
                print(f"  â€¢ {backup_file}")
        
        print(f"\nğŸ“Š å„ªåŒ–æˆåŠŸç‡: {report['success_rate']:.1%}")
        print("\n" + "="*60)


def main():
    """ä¸»å‡½æ•¸ï¼šåŸ·è¡Œæ‰¹æ¬¡å¤§å°å„ªåŒ–"""
    print("ğŸ” æ­£åœ¨åˆå§‹åŒ–æ‰¹æ¬¡å¤§å°å„ªåŒ–å™¨...")
    
    optimizer = BatchSizeOptimizer()
    
    # é¦–å…ˆåˆ†æç•¶å‰é…ç½®
    current_configs = optimizer.analyze_current_batch_sizes()
    if not current_configs:
        print("âŒ æœªæ‰¾åˆ°ä»»ä½•é…ç½®æ–‡ä»¶")
        return False
    
    print(f"ğŸ“‹ æ‰¾åˆ° {len(current_configs)} å€‹é…ç½®æ–‡ä»¶")
    for name, info in current_configs.items():
        print(f"  â€¢ {name}: {info['batch_sizes']}")
      # é‹è¡ŒGPUæ¸¬è©¦
    print("\nğŸ” æ­£åœ¨æ¸¬è©¦GPUæœ€ä½³æ‰¹æ¬¡å¤§å°...")
    # ç›´æ¥ä½¿ç”¨æˆ‘å€‘ä¹‹å‰æ¸¬è©¦å‡ºçš„çµæœï¼Œå› ç‚ºæˆ‘å€‘çŸ¥é“RTX 4060 Ti 16GBå¯ä»¥è™•ç†204æ‰¹æ¬¡å¤§å°
    if torch.cuda.is_available():
        gpu_name = torch.cuda.get_device_name(0)
        if "RTX 4060 Ti" in gpu_name:
            optimal_batch_size = 204  # ä½¿ç”¨ä¹‹å‰æ¸¬è©¦çš„çµæœ
            print(f"âœ… ä½¿ç”¨RTX 4060 Tiçš„å·²çŸ¥æœ€ä½³æ‰¹æ¬¡å¤§å°: {optimal_batch_size}")
        else:
            optimal_batch_size = 128
            print(f"âœ… ä½¿ç”¨é€šç”¨GPUæœ€ä½³æ‰¹æ¬¡å¤§å°: {optimal_batch_size}")
    else:
        optimal_batch_size = 64
        print(f"âš ï¸ æœªæª¢æ¸¬åˆ°GPUï¼Œä½¿ç”¨CPUé è¨­å€¼: {optimal_batch_size}")
    
    # åŸ·è¡Œæ‰¹æ¬¡å¤§å°å„ªåŒ–
    print("\nğŸš€ åŸ·è¡Œæ‰¹æ¬¡å¤§å°å„ªåŒ–...")
    report = optimizer.optimize_batch_sizes(gpu_test_result=optimal_batch_size)
    optimizer.print_optimization_summary(report)
    
    # æç¤ºé‡å•Ÿ
    if report["success_rate"] > 0:
        print("\nâš ï¸ é…ç½®å·²æ›´æ–°ï¼Œå»ºè­°é‡å•Ÿè¨“ç·´ç¨‹åºä»¥æ‡‰ç”¨æ–°è¨­å®š")
        return True
    else:
        print("\nâŒ å„ªåŒ–å¤±æ•—ï¼Œè«‹æª¢æŸ¥éŒ¯èª¤æ—¥èªŒ")
        return False


if __name__ == "__main__":
    main()
