# src/optimization/batch_size_optimizer.py
"""
æ‰¹æ¬¡å¤§å°è‡ªå‹•å„ªåŒ–å·¥å…·
åŸºæ–¼GPUèƒ½åŠ›æ¸¬è©¦çµæœè‡ªå‹•å„ªåŒ–æ‰€æœ‰é…ç½®æ–‡ä»¶ä¸­çš„æ‰¹æ¬¡å¤§å°

ä¸»è¦åŠŸèƒ½ï¼š
1. æª¢æ¸¬ç•¶å‰é…ç½®çš„æ‰¹æ¬¡å¤§å°
2. åŸºæ–¼GPUæ¸¬è©¦çµæœè¨ˆç®—æœ€ä½³æ‰¹æ¬¡å¤§å°
3. è‡ªå‹•æ›´æ–°æ‰€æœ‰ç›¸é—œé…ç½®æ–‡ä»¶
4. æä¾›å®‰å…¨çš„å›é€€æ©Ÿåˆ¶
"""

import json
import re
import sys
import os
from pathlib import Path
from typing import Dict, List, Tuple, Any
import torch
import logging
from datetime import datetime

# æ·»åŠ é …ç›®æ ¹ç›®éŒ„åˆ°Pythonè·¯å¾‘
project_root = Path(__file__).resolve().parent.parent.parent
sys.path.insert(0, str(project_root))

try:
    # å˜—è©¦ç›¸å°å°å…¥
    from src.common.logger_setup import logger
    from src.optimization.performance_optimizer import PerformanceOptimizer
except ImportError:
    try:
        # å˜—è©¦çµ•å°å°å…¥
        import sys
        import os
        sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))
        from src.common.logger_setup import logger
        from src.optimization.performance_optimizer import PerformanceOptimizer
    except ImportError:
        # å‰µå»ºåŸºæœ¬logger
        logger = logging.getLogger(__name__)
        logger.setLevel(logging.INFO)
        handler = logging.StreamHandler()
        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        handler.setFormatter(formatter)
        logger.addHandler(handler)


class BatchSizeOptimizer:
    """æ‰¹æ¬¡å¤§å°å„ªåŒ–å™¨"""
    
    def __init__(self, project_root: Path = None):
        self.project_root = project_root or Path(__file__).resolve().parent.parent.parent
        self.config_files = self._discover_config_files()
        self.backup_dir = self.project_root / "backups" / "config_backups"
        self.backup_dir.mkdir(parents=True, exist_ok=True)
        
    def _discover_config_files(self) -> Dict[str, Path]:
        """ç™¼ç¾æ‰€æœ‰åŒ…å«æ‰¹æ¬¡å¤§å°çš„é…ç½®æ–‡ä»¶"""
        config_files = {}
        
        # ä¸»è¦é…ç½®æ–‡ä»¶
        main_config = self.project_root / "src" / "common" / "config.py"
        if main_config.exists():
            config_files["main_config"] = main_config
            
        # Transformeré…ç½®æ–‡ä»¶
        transformer_config = self.project_root / "configs" / "enhanced_transformer_config.json"
        if transformer_config.exists():
            config_files["transformer_config"] = transformer_config
            
        # å…¶ä»–å¯èƒ½çš„é…ç½®æ–‡ä»¶
        for pattern in ["configs/*.json", "src/**/*config*.py"]:
            parts = pattern.split("*")
            base_dir = self.project_root / parts[0]
            if base_dir.exists():
                for file in base_dir.rglob(f"*{parts[1]}*{parts[2] if len(parts) > 2 else ''}"):
                    if file.name not in [f.name for f in config_files.values()]:
                        config_files[file.stem] = file
                        
        return config_files
    
    def analyze_current_batch_sizes(self) -> Dict[str, Any]:
        """åˆ†æç•¶å‰çš„æ‰¹æ¬¡å¤§å°é…ç½®"""
        current_configs = {}
        
        for config_name, config_path in self.config_files.items():
            try:
                if config_path.suffix == ".py":
                    # Pythoné…ç½®æ–‡ä»¶
                    with open(config_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                    
                    batch_sizes = self._extract_batch_sizes_from_python(content)
                    if batch_sizes:
                        current_configs[config_name] = {
                            "type": "python",
                            "path": config_path,
                            "batch_sizes": batch_sizes
                        }
                        
                elif config_path.suffix == ".json":
                    # JSONé…ç½®æ–‡ä»¶
                    with open(config_path, 'r', encoding='utf-8') as f:
                        content = json.load(f)
                    
                    batch_sizes = self._extract_batch_sizes_from_json(content)
                    if batch_sizes:
                        current_configs[config_name] = {
                            "type": "json",
                            "path": config_path,
                            "batch_sizes": batch_sizes,
                            "content": content
                        }
                        
            except Exception as e:
                logger.warning(f"ç„¡æ³•åˆ†æé…ç½®æ–‡ä»¶ {config_path}: {e}")
                
        return current_configs
    
    def _extract_batch_sizes_from_python(self, content: str) -> Dict[str, int]:
        """å¾Pythonæ–‡ä»¶ä¸­æå–æ‰¹æ¬¡å¤§å°"""
        batch_sizes = {}
        
        # åŒ¹é…å„ç¨®æ‰¹æ¬¡å¤§å°è®Šæ•¸
        patterns = [
            r'SAC_BATCH_SIZE\s*=\s*(\d+)',
            r'BATCH_SIZE\s*=\s*(\d+)',
            r'batch_size\s*=\s*(\d+)',
            r'TRANSFORMER_BATCH_SIZE\s*=\s*(\d+)'
        ]
        
        for pattern in patterns:
            matches = re.findall(pattern, content)
            if matches:
                var_name = pattern.split('\\')[0]
                batch_sizes[var_name] = int(matches[0])
                
        return batch_sizes
    
    def _extract_batch_sizes_from_json(self, content: dict) -> Dict[str, int]:
        """å¾JSONé…ç½®ä¸­æå–æ‰¹æ¬¡å¤§å°"""
        batch_sizes = {}
        
        def search_recursive(obj, path=""):
            if isinstance(obj, dict):
                for key, value in obj.items():
                    new_path = f"{path}.{key}" if path else key
                    if key == "batch_size" and isinstance(value, int):
                        batch_sizes[new_path] = value
                    elif isinstance(value, (dict, list)):
                        search_recursive(value, new_path)
            elif isinstance(obj, list):
                for i, item in enumerate(obj):
                    search_recursive(item, f"{path}[{i}]")
        
        search_recursive(content)
        return batch_sizes
    
    def calculate_optimal_batch_sizes(self, gpu_test_result: int = None) -> Dict[str, int]:
        """è¨ˆç®—æœ€ä½³æ‰¹æ¬¡å¤§å°"""
        if gpu_test_result is None:
            # é‹è¡ŒGPUæ¸¬è©¦
            try:
                optimizer = PerformanceOptimizer()
                result = optimizer.test_optimal_batch_size()
                optimal_batch_size = result["recommended_batch_size"]
            except Exception as e:
                logger.warning(f"GPUæ¸¬è©¦å¤±æ•—ï¼Œä½¿ç”¨é è¨­å€¼: {e}")
                optimal_batch_size = 64
        else:
            optimal_batch_size = gpu_test_result
          # ç‚ºä¸åŒç”¨é€”è¨ˆç®—åˆé©çš„æ‰¹æ¬¡å¤§å°
        optimal_configs = {
            "SAC_BATCH_SIZE": min(optimal_batch_size, 128),  # SACä¸éœ€è¦å¤ªå¤§
            "transformer_batch_size": optimal_batch_size,     # Transformerå¯ä»¥ç”¨è¼ƒå¤§çš„
            "general_batch_size": optimal_batch_size // 2,    # é€šç”¨æ‰¹æ¬¡å¤§å°
            "evaluation_batch_size": optimal_batch_size // 4  # è©•ä¼°ç”¨è¼ƒå°çš„
        }
        
        return optimal_configs
    
    def backup_configs(self) -> List[str]:
        """å‚™ä»½ç•¶å‰é…ç½®"""
        backup_files = []
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        for config_name, config_path in self.config_files.items():
            try:
                # åªå‚™ä»½æ–‡ä»¶ï¼Œä¸å‚™ä»½ç›®éŒ„
                if config_path.is_file():
                    backup_path = self.backup_dir / f"{config_name}_backup_{timestamp}{config_path.suffix}"
                    backup_path.write_text(config_path.read_text(encoding='utf-8'), encoding='utf-8')
                    backup_files.append(str(backup_path))  # è½‰æ›ç‚ºå­—ç¬¦ä¸²
                    logger.info(f"å·²å‚™ä»½ {config_path} -> {backup_path}")
            except Exception as e:
                logger.error(f"å‚™ä»½å¤±æ•— {config_path}: {e}")
                
        return backup_files
    
    def _serialize_configs(self, configs: Dict[str, Any]) -> Dict[str, Any]:
        """åºåˆ—åŒ–é…ç½®å­—å…¸ï¼Œè™•ç†Pathå°è±¡"""
        serialized = {}
        for key, value in configs.items():
            serialized_value = value.copy()
            if "path" in serialized_value:
                serialized_value["path"] = str(serialized_value["path"])
            serialized[key] = serialized_value
        return serialized
    
    def update_config_files(self, optimal_configs: Dict[str, int]) -> Dict[str, bool]:
        """æ›´æ–°é…ç½®æ–‡ä»¶"""
        update_results = {}
        
        for config_name, config_info in self.analyze_current_batch_sizes().items():
            try:
                if config_info["type"] == "python":
                    success = self._update_python_config(config_info, optimal_configs)
                elif config_info["type"] == "json":
                    success = self._update_json_config(config_info, optimal_configs)
                else:
                    success = False
                    
                update_results[config_name] = success
                
            except Exception as e:
                logger.error(f"æ›´æ–°é…ç½®å¤±æ•— {config_name}: {e}")
                update_results[config_name] = False
                
        return update_results
    
    def _update_python_config(self, config_info: Dict, optimal_configs: Dict[str, int]) -> bool:
        """æ›´æ–°Pythoné…ç½®æ–‡ä»¶"""
        try:
            config_path = config_info["path"]
            with open(config_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # æ›´æ–°SAC_BATCH_SIZE
            if "SAC_BATCH_SIZE" in optimal_configs:
                pattern = r'SAC_BATCH_SIZE\s*=\s*\d+'
                replacement = f'SAC_BATCH_SIZE = {optimal_configs["SAC_BATCH_SIZE"]}'
                content = re.sub(pattern, replacement, content)
            
            # æ›´æ–°å…¶ä»–æ‰¹æ¬¡å¤§å°è®Šæ•¸
            for var_pattern in ['BATCH_SIZE', 'batch_size']:
                if var_pattern in content and "general_batch_size" in optimal_configs:
                    pattern = f'{var_pattern}\\s*=\\s*\\d+'
                    replacement = f'{var_pattern} = {optimal_configs["general_batch_size"]}'
                    content = re.sub(pattern, replacement, content)
            
            # å¯«å›æ–‡ä»¶
            with open(config_path, 'w', encoding='utf-8') as f:
                f.write(content)
                
            logger.info(f"å·²æ›´æ–°Pythoné…ç½®: {config_path}")
            return True
            
        except Exception as e:
            logger.error(f"æ›´æ–°Pythoné…ç½®å¤±æ•—: {e}")
            return False
    
    def _update_json_config(self, config_info: Dict, optimal_configs: Dict[str, int]) -> bool:
        """æ›´æ–°JSONé…ç½®æ–‡ä»¶"""
        try:
            config_path = config_info["path"]
            content = config_info["content"].copy()
            
            # æ›´æ–°è¨“ç·´ç›¸é—œçš„æ‰¹æ¬¡å¤§å°
            if "training" in content and "batch_size" in content["training"]:
                content["training"]["batch_size"] = optimal_configs.get("transformer_batch_size", 32)
            
            # æ›´æ–°è©•ä¼°ç›¸é—œçš„æ‰¹æ¬¡å¤§å°
            if "evaluation" in content and "batch_size" in content["evaluation"]:
                content["evaluation"]["batch_size"] = optimal_configs.get("evaluation_batch_size", 16)
            
            # å¯«å›æ–‡ä»¶
            with open(config_path, 'w', encoding='utf-8') as f:
                json.dump(content, f, indent=2, ensure_ascii=False)
                
            logger.info(f"å·²æ›´æ–°JSONé…ç½®: {config_path}")
            return True
            
        except Exception as e:
            logger.error(f"æ›´æ–°JSONé…ç½®å¤±æ•—: {e}")
            return False
    
    def optimize_batch_sizes(self, gpu_test_result: int = None, 
                           create_backup: bool = True) -> Dict[str, Any]:
        """åŸ·è¡Œæ‰¹æ¬¡å¤§å°å„ªåŒ–"""
        logger.info("é–‹å§‹æ‰¹æ¬¡å¤§å°å„ªåŒ–...")
        
        # åˆ†æç•¶å‰é…ç½®
        current_configs = self.analyze_current_batch_sizes()
        logger.info(f"ç™¼ç¾ {len(current_configs)} å€‹é…ç½®æ–‡ä»¶")
        
        # è¨ˆç®—æœ€ä½³æ‰¹æ¬¡å¤§å°
        optimal_configs = self.calculate_optimal_batch_sizes(gpu_test_result)
        logger.info(f"è¨ˆç®—å‡ºæœ€ä½³æ‰¹æ¬¡å¤§å°: {optimal_configs}")
        
        # å‚™ä»½ç¾æœ‰é…ç½®
        backup_files = []
        if create_backup:
            backup_files = self.backup_configs()
          # æ›´æ–°é…ç½®æ–‡ä»¶
        update_results = self.update_config_files(optimal_configs)
        
        # ç”Ÿæˆå ±å‘Š
        report = {
            "timestamp": datetime.now().isoformat(),
            "current_configs": self._serialize_configs(current_configs),
            "optimal_configs": optimal_configs,
            "update_results": update_results,
            "backup_files": backup_files,  # å·²ç¶“æ˜¯å­—ç¬¦ä¸²åˆ—è¡¨
            "success_rate": sum(update_results.values()) / len(update_results) if update_results else 0
        }
        
        # ä¿å­˜å ±å‘Š
        report_path = self.project_root / "reports" / "batch_size_optimization.json"
        report_path.parent.mkdir(exist_ok=True)
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        logger.info(f"å„ªåŒ–å®Œæˆï¼Œå ±å‘Šå·²ä¿å­˜è‡³: {report_path}")
        return report
    
    def print_optimization_summary(self, report: Dict[str, Any]):
        """æ‰“å°å„ªåŒ–æ‘˜è¦"""
        print("\n" + "="*60)
        print("ğŸš€ æ‰¹æ¬¡å¤§å°å„ªåŒ–å ±å‘Š")
        print("="*60)
        
        print(f"\nğŸ“‹ ç™¼ç¾çš„é…ç½®æ–‡ä»¶:")
        for config_name, config_info in report["current_configs"].items():
            print(f"  â€¢ {config_name}: {config_info['path']}")
            for var_name, current_value in config_info["batch_sizes"].items():
                print(f"    - {var_name}: {current_value}")
        
        print(f"\nğŸ¯ å»ºè­°çš„æœ€ä½³æ‰¹æ¬¡å¤§å°:")
        for config_name, optimal_value in report["optimal_configs"].items():
            print(f"  â€¢ {config_name}: {optimal_value}")
        
        print(f"\nâœ… æ›´æ–°çµæœ:")
        success_count = sum(report["update_results"].values())
        total_count = len(report["update_results"])
        print(f"  æˆåŠŸæ›´æ–°: {success_count}/{total_count} å€‹é…ç½®æ–‡ä»¶")
        
        for config_name, success in report["update_results"].items():
            icon = "âœ…" if success else "âŒ"
            print(f"  {icon} {config_name}")
        
        if report["backup_files"]:
            print(f"\nğŸ’¾ å‚™ä»½æ–‡ä»¶:")
            for backup_file in report["backup_files"]:
                print(f"  â€¢ {backup_file}")
        
        print("\n" + "="*60)


def main():
    """ä¸»å‡½æ•¸ï¼šåŸ·è¡Œæ‰¹æ¬¡å¤§å°å„ªåŒ–"""
    # é¦–å…ˆé‹è¡ŒGPUæ¸¬è©¦
    print("ğŸ” æ­£åœ¨æ¸¬è©¦GPUæœ€ä½³æ‰¹æ¬¡å¤§å°...")
    try:
        from src.optimization.performance_optimizer import PerformanceOptimizer
        perf_optimizer = PerformanceOptimizer()
        gpu_result = perf_optimizer.test_optimal_batch_size()
        optimal_batch_size = gpu_result["recommended_batch_size"]
        print(f"âœ… GPUæ¸¬è©¦å®Œæˆï¼Œå»ºè­°æ‰¹æ¬¡å¤§å°: {optimal_batch_size}")
    except Exception as e:
        print(f"âš ï¸ GPUæ¸¬è©¦å¤±æ•—ï¼Œä½¿ç”¨é è¨­å€¼: {e}")
        optimal_batch_size = 64
    
    # åŸ·è¡Œæ‰¹æ¬¡å¤§å°å„ªåŒ–
    optimizer = BatchSizeOptimizer()
    report = optimizer.optimize_batch_sizes(gpu_test_result=optimal_batch_size)
    optimizer.print_optimization_summary(report)
    
    # æç¤ºé‡å•Ÿ
    if report["success_rate"] > 0:
        print("\nâš ï¸ é…ç½®å·²æ›´æ–°ï¼Œå»ºè­°é‡å•Ÿè¨“ç·´ç¨‹åºä»¥æ‡‰ç”¨æ–°è¨­å®š")
        return True
    else:
        print("\nâŒ å„ªåŒ–å¤±æ•—ï¼Œè«‹æª¢æŸ¥éŒ¯èª¤æ—¥èªŒ")
        return False


if __name__ == "__main__":
    main()
