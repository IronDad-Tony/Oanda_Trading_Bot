# src/environment/progressive_learning_system.py
"""
æ¼¸é€²å¼å­¸ç¿’ç³»çµ± - æ¨¡å‹å…¨é¢å¢å¼·è¨ˆç•«éšæ®µäºŒå¯¦æ–½
å¯¦ç¾ä¸‰éšæ®µå­¸ç¿’æ¡†æ¶ï¼ŒåŸºæ–¼å¯¦éš›äº¤æ˜“è¡¨ç¾é€²è¡Œå‹•æ…‹åˆ‡æ›

éšæ®µä¸€ï¼šåŸºç¤äº¤æ˜“åŸç†å­¸ç¿’ (åŸºæœ¬ç›ˆè™§æ¦‚å¿µå’Œé¢¨éšªæ§åˆ¶)
éšæ®µäºŒï¼šé¢¨éšªç®¡ç†å¼·åŒ– (è¤‡é›œé¢¨éšªæŒ‡æ¨™å’Œç¸¾æ•ˆè©•ä¼°)  
éšæ®µä¸‰ï¼šè¤‡é›œç­–ç•¥æŒæ¡ (å®Œæ•´è¤‡é›œçå‹µå‡½æ•¸ï¼Œç™¼å±•é«˜ç´šç­–ç•¥)

ä¸»è¦ç‰¹é»ï¼š
- å®Œå…¨åŸºæ–¼äº¤æ˜“è¡¨ç¾çš„éšæ®µåˆ‡æ›
- ä¸‰ç¨®æ¼¸é€²å¼çå‹µå‡½æ•¸
- æ™ºèƒ½é€²éšæ¢ä»¶åˆ¤æ–·
- è©³ç´°å­¸ç¿’é€²åº¦è¿½è¹¤
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from typing import Dict, List, Tuple, Optional, Any, Union
import logging
from dataclasses import dataclass
from datetime import datetime
from enum import Enum
import math

try:
    from src.common.logger_setup import logger
    from src.common.config import DEVICE
except ImportError:
    logger = logging.getLogger(__name__)
    DEVICE = "cpu"


class LearningStage(Enum):
    """å­¸ç¿’éšæ®µæšèˆ‰"""
    BASIC = 1       # åŸºç¤å­¸ç¿’éšæ®µ
    INTERMEDIATE = 2 # ä¸­ç´šå­¸ç¿’éšæ®µ  
    ADVANCED = 3    # é«˜ç´šå­¸ç¿’éšæ®µ


@dataclass
class LearningMetrics:
    """å­¸ç¿’é€²åº¦æŒ‡æ¨™"""
    stage: LearningStage
    episode: int
    stage_episodes: int
    stage_progress: float  # ç•¶å‰éšæ®µé€²åº¦ (0-1)
    stage_performance: float  # ç•¶å‰éšæ®µè¡¨ç¾
    advancement_progress: float  # é€²éšé€²åº¦ (0-1)
    should_advance: bool = False  # æ˜¯å¦æ‡‰è©²é€²å…¥ä¸‹ä¸€éšæ®µ
    advancement_reason: str = ""  # é€²éšåŸå› 
    timestamp: datetime = None


@dataclass
class RewardComponents:
    """çå‹µçµ„ä»¶è©³æƒ…"""
    basic_pnl: float = 0.0
    risk_penalty: float = 0.0
    trade_frequency: float = 0.0
    sharpe_ratio: float = 0.0
    max_drawdown: float = 0.0
    win_rate: float = 0.0
    sortino_ratio: float = 0.0
    var_risk: float = 0.0
    skewness: float = 0.0
    kurtosis: float = 0.0
    transaction_costs: float = 0.0
    consistency_bonus: float = 0.0
    learning_bonus: float = 0.0  # æ–°å¢å­¸ç¿’é€²æ­¥çå‹µ
    total_reward: float = 0.0


class Stage1BasicReward(nn.Module):
    """
    éšæ®µä¸€ï¼šåŸºç¤äº¤æ˜“åŸç†å­¸ç¿’çå‹µå‡½æ•¸
    ç›®æ¨™ï¼šå­¸ç¿’åŸºæœ¬çš„ç›ˆè™§æ¦‚å¿µå’Œé¢¨éšªæ§åˆ¶
    
    å­¸ç¿’é‡é»ï¼š
    - åŸºæœ¬çš„è²·è³£æ™‚æ©Ÿ
    - é¢¨éšªæ„è­˜åŸ¹é¤Š
    - é¿å…éåº¦äº¤æ˜“
    """
    
    def __init__(self):
        super().__init__()
        self.pnl_weight = 0.70      # åŸºæœ¬ç›ˆè™§æ¬Šé‡
        self.risk_weight = 0.20     # é¢¨éšªæ§åˆ¶æ¬Šé‡
        self.frequency_weight = 0.10 # äº¤æ˜“é »ç‡æ¬Šé‡
        
    def forward(self, metrics: Dict[str, float]) -> RewardComponents:
        """è¨ˆç®—éšæ®µä¸€çå‹µ (å„ªåŒ–åˆæœŸå­¸ç¿’)"""
        components = RewardComponents()
        
        # 1. åŸºæœ¬ç›ˆè™§ (æ¬Šé‡70%) - ä½¿ç”¨æ›´æº«å’Œçš„æ˜ å°„
        pnl = metrics.get('pnl', 0.0)
        components.basic_pnl = np.tanh(pnl * 5) * self.pnl_weight  # é™ä½æ•æ„Ÿåº¦
        
        # 2. ç°¡å–®é¢¨éšªæ§åˆ¶ (æ¬Šé‡20%) - æ›´å¯¬å®¹çš„é¢¨éšªè©•ä¼°
        drawdown = metrics.get('drawdown', 0.0)
        if drawdown > 0.10:  # æ”¾å¯¬åˆ°å›æ’¤>10%
            components.risk_penalty = -2.0 * self.risk_weight  # æ¸›å°‘æ‡²ç½°å¼·åº¦
        elif drawdown > 0.05:  # ä¸­ç­‰å›æ’¤è¼•å¾®æ‡²ç½°
            components.risk_penalty = -0.5 * self.risk_weight
        else:
            components.risk_penalty = 0.5 * self.risk_weight  # çå‹µä½å›æ’¤
            
        # 3. äº¤æ˜“é »ç‡æ§åˆ¶ (æ¬Šé‡10%) - é¼“å‹µåˆç†äº¤æ˜“
        trade_freq = metrics.get('trade_frequency', 0.0)
        if trade_freq > 0.15:  # æ”¾å¯¬éåº¦äº¤æ˜“é–¾å€¼
            components.trade_frequency = -1.0 * self.frequency_weight  # æ¸›å°‘æ‡²ç½°
        elif trade_freq < 0.02:  # æ‡²ç½°éå°‘äº¤æ˜“
            components.trade_frequency = -0.5 * self.frequency_weight
        else:
            components.trade_frequency = 0.3 * self.frequency_weight  # çå‹µé©ä¸­äº¤æ˜“
            
        # 4. æ–°å¢å­¸ç¿’é€²æ­¥çå‹µ - é¼“å‹µä»»ä½•å½¢å¼çš„æ”¹å–„
        learning_bonus = 0.0
        if pnl > -0.02:  # å°è™§ææˆ–ç›ˆåˆ©çµ¦äºˆçå‹µ
            learning_bonus = 0.1
        if drawdown < 0.08:  # è‰¯å¥½çš„é¢¨éšªæ§åˆ¶
            learning_bonus += 0.05
        
        components.total_reward = (components.basic_pnl + 
                                 components.risk_penalty + 
                                 components.trade_frequency +
                                 learning_bonus)
        
        return components


class Stage2IntermediateReward(nn.Module):
    """
    éšæ®µäºŒï¼šé¢¨éšªç®¡ç†å¼·åŒ–çå‹µå‡½æ•¸
    ç›®æ¨™ï¼šå¼•å…¥æ›´è¤‡é›œçš„é¢¨éšªæŒ‡æ¨™å’Œç¸¾æ•ˆè©•ä¼°
    
    å­¸ç¿’é‡é»ï¼š
    - é¢¨éšªèª¿æ•´å¾Œæ”¶ç›Šå„ªåŒ–
    - å‹ç‡èˆ‡ç›ˆè™§æ¯”å¹³è¡¡
    - ç©©å®šæ€§è¿½æ±‚
    """
    
    def __init__(self):
        super().__init__()
        self.base_reward = Stage1BasicReward()
        self.sharpe_weight = 0.25
        self.drawdown_weight = 0.20
        self.winrate_weight = 0.15
        
    def forward(self, metrics: Dict[str, float]) -> RewardComponents:
        """è¨ˆç®—éšæ®µäºŒçå‹µ"""
        # ç¹¼æ‰¿éšæ®µä¸€åŸºç¤çå‹µ (ä½†é™ä½æ¬Šé‡)
        components = self.base_reward(metrics)
        
        # èª¿æ•´åŸºç¤çå‹µæ¬Šé‡
        components.basic_pnl *= 0.7
        components.risk_penalty *= 0.7
        components.trade_frequency *= 0.7
        
        # æ–°å¢å¤æ™®æ¯”ç‡çå‹µ
        sharpe = metrics.get('sharpe_ratio', 0.0)
        components.sharpe_ratio = np.tanh(sharpe) * self.sharpe_weight
        
        # æ–°å¢æœ€å¤§å›æ’¤æ§åˆ¶
        max_drawdown = metrics.get('max_drawdown', 0.0)
        components.max_drawdown = -max_drawdown * 8.0 * self.drawdown_weight
        
        # æ–°å¢å‹ç‡æ¿€å‹µ
        win_rate = metrics.get('win_rate', 0.0)
        if win_rate > 0.6:
            components.win_rate = 2.0 * self.winrate_weight
        elif win_rate < 0.4:
            components.win_rate = -1.0 * self.winrate_weight
        else:
            components.win_rate = 0.0
            
        # é‡æ–°è¨ˆç®—ç¸½çå‹µ
        components.total_reward = (components.basic_pnl + 
                                 components.risk_penalty + 
                                 components.trade_frequency +
                                 components.sharpe_ratio +
                                 components.max_drawdown +
                                 components.win_rate)
        
        return components


class Stage3AdvancedReward(nn.Module):
    """
    éšæ®µä¸‰ï¼šè¤‡é›œç­–ç•¥æŒæ¡çå‹µå‡½æ•¸
    ç›®æ¨™ï¼šä½¿ç”¨å®Œæ•´çš„è¤‡é›œçå‹µå‡½æ•¸ï¼Œç™¼å±•é«˜ç´šç­–ç•¥
    
    å­¸ç¿’é‡é»ï¼š
    - è¤‡é›œå¸‚å ´ç’°å¢ƒé©æ‡‰
    - å¤šç­–ç•¥å‹•æ…‹çµ„åˆ
    - è¶…äººé¡ç­–ç•¥å‰µæ–°
    """
    
    def __init__(self):
        super().__init__()
        self.stage2_reward = Stage2IntermediateReward()
        
        # é«˜ç´šæŒ‡æ¨™æ¬Šé‡
        self.sortino_weight = 0.15
        self.var_weight = 0.12
        self.skewness_weight = 0.08
        self.kurtosis_weight = 0.08
        self.cost_weight = 0.05
        self.consistency_weight = 0.10
        
    def forward(self, metrics: Dict[str, float]) -> RewardComponents:
        """è¨ˆç®—éšæ®µä¸‰çå‹µ"""
        # ç¹¼æ‰¿éšæ®µäºŒçå‹µ (ä½†é€²ä¸€æ­¥èª¿æ•´æ¬Šé‡)
        components = self.stage2_reward(metrics)
        
        # èª¿æ•´å‰æœŸçå‹µæ¬Šé‡
        components.basic_pnl *= 0.8
        components.risk_penalty *= 0.8
        components.trade_frequency *= 0.8
        components.sharpe_ratio *= 0.9
        components.max_drawdown *= 0.9
        components.win_rate *= 0.9
        
        # Sortinoæ¯”ç‡ (åªè€ƒæ…®ä¸‹è¡Œé¢¨éšª)
        sortino = metrics.get('sortino_ratio', 0.0)
        components.sortino_ratio = np.tanh(sortino) * self.sortino_weight
        
        # VaRé¢¨éšª (Value at Risk)
        var_risk = metrics.get('var_risk', 0.0)
        components.var_risk = -abs(var_risk) * self.var_weight
        
        # æ”¶ç›Šåˆ†ä½ˆååº¦ (çå‹µæ­£å)
        skewness = metrics.get('skewness', 0.0)
        components.skewness = skewness * self.skewness_weight
        
        # æ”¶ç›Šåˆ†ä½ˆå³°åº¦ (æ‡²ç½°æ¥µç«¯å³°åº¦)
        kurtosis = metrics.get('kurtosis', 0.0)
        excess_kurtosis = kurtosis - 3.0  # è¶…é¡å³°åº¦
        components.kurtosis = -abs(excess_kurtosis) * 0.1 * self.kurtosis_weight
        
        # äº¤æ˜“æˆæœ¬
        transaction_costs = metrics.get('transaction_costs', 0.0)
        components.transaction_costs = -transaction_costs * self.cost_weight
        
        # ä¸€è‡´æ€§çå‹µ (çå‹µç©©å®šè¡¨ç¾)
        consistency = metrics.get('consistency_score', 0.0)
        components.consistency_bonus = consistency * self.consistency_weight
        
        # é‡æ–°è¨ˆç®—ç¸½çå‹µ
        components.total_reward = (components.basic_pnl + 
                                 components.risk_penalty + 
                                 components.trade_frequency +
                                 components.sharpe_ratio +
                                 components.max_drawdown +
                                 components.win_rate +
                                 components.sortino_ratio +
                                 components.var_risk +
                                 components.skewness +
                                 components.kurtosis +
                                 components.transaction_costs +
                                 components.consistency_bonus)
        
        return components


class ProgressiveLearningSystem:
    """
    æ¼¸é€²å¼å­¸ç¿’ç³»çµ±ä¸»æ§åˆ¶å™¨
    åŸºæ–¼å¯¦éš›äº¤æ˜“è¡¨ç¾ç®¡ç†ä¸‰éšæ®µå­¸ç¿’é€²ç¨‹å’Œçå‹µå‡½æ•¸åˆ‡æ›
    """
    
    def __init__(self, 
                 min_stage_episodes: int = 50,      # æ¯éšæ®µæœ€å°‘å›åˆæ•¸
                 performance_window: int = 20,       # æ€§èƒ½è©•ä¼°çª—å£
                 advancement_patience: int = 10,     # é€²éšè€å¿ƒå€¼
                 device: str = "cpu"):
        """
        åˆå§‹åŒ–æ¼¸é€²å¼å­¸ç¿’ç³»çµ±
        
        Args:
            min_stage_episodes: æ¯éšæ®µæœ€å°‘å›åˆæ•¸
            performance_window: æ€§èƒ½è©•ä¼°çª—å£å¤§å°
            advancement_patience: é”åˆ°é€²éšæ¢ä»¶å¾Œçš„ç­‰å¾…å›åˆæ•¸
            device: è¨ˆç®—è¨­å‚™
        """
        self.min_stage_episodes = min_stage_episodes
        self.performance_window = performance_window
        self.advancement_patience = advancement_patience
        self.device = device
        
        # ç•¶å‰ç‹€æ…‹
        self.current_episode = 0
        self.current_stage = LearningStage.BASIC
        self.stage_episodes = 0
        self.force_advancement = False
        
        # çå‹µå‡½æ•¸
        self.stage1_reward = Stage1BasicReward()
        self.stage2_reward = Stage2IntermediateReward()
        self.stage3_reward = Stage3AdvancedReward()
        
        # æ€§èƒ½è¿½è¹¤
        self.stage_performance_history = {
            LearningStage.BASIC: [],
            LearningStage.INTERMEDIATE: [],
            LearningStage.ADVANCED: []
        }
        
        # é€²éšæ¢ä»¶è¿½è¹¤
        self.advancement_progress = {
            LearningStage.BASIC: 0,
            LearningStage.INTERMEDIATE: 0
        }
          # é€²éšæ¢ä»¶å®šç¾© (åŸºæ–¼å¯¦éš›äº¤æ˜“è¡¨ç¾ï¼Œå„ªåŒ–åˆæœŸå­¸ç¿’)
        self.advancement_criteria = {
            LearningStage.BASIC: {
                'min_episodes': min_stage_episodes,
                'min_avg_reward': -0.10,         # é™ä½åˆæœŸçå‹µè¦æ±‚ (å¾-0.25æå‡åˆ°-0.10)
                'win_rate_threshold': 0.35,      # é™ä½åˆæœŸå‹ç‡è¦æ±‚ (å¾0.45åˆ°0.35)
                'max_drawdown_threshold': 0.15,  # æ”¾å¯¬åˆæœŸå›æ’¤é™åˆ¶ (å¾0.10åˆ°0.15)
                'consistency_episodes': 5,       # æ¸›å°‘é€£çºŒé”æ¨™è¦æ±‚ (å¾10åˆ°5)
                'improvement_threshold': 0.05,   # æ–°å¢ï¼šçå‹µæ”¹å–„è¦æ±‚
                'description': 'åŸºç¤äº¤æ˜“æŠ€èƒ½æŒæ¡'
            },
            LearningStage.INTERMEDIATE: {
                'min_episodes': min_stage_episodes,
                'min_avg_reward': 0.15,          # ä¸­ç´šéšæ®µåˆç†çå‹µè¦æ±‚
                'sharpe_threshold': 0.5,         # é™ä½å¤æ™®æ¯”ç‡è¦æ±‚ (å¾1.0åˆ°0.5)
                'max_drawdown_threshold': 0.10,  # å›æ’¤æ§åˆ¶
                'win_rate_threshold': 0.45,      # ä¸­ç´šå‹ç‡è¦æ±‚ (å¾0.55åˆ°0.45)
                'consistency_episodes': 8,       # ä¸­ç´šé€£çºŒé”æ¨™è¦æ±‚ (å¾15åˆ°8)
                'improvement_threshold': 0.08,   # æ–°å¢ï¼šçå‹µæ”¹å–„è¦æ±‚
                'description': 'é¢¨éšªç®¡ç†èƒ½åŠ›é”æ¨™'
            }
        }
        
        logger.info(f"åˆå§‹åŒ–æ¼¸é€²å¼å­¸ç¿’ç³»çµ±:")
        logger.info(f"  ç•¶å‰éšæ®µ: {self.current_stage.name}")
        logger.info(f"  æ¯éšæ®µæœ€å°‘å›åˆ: {min_stage_episodes}")
        logger.info(f"  æ€§èƒ½è©•ä¼°çª—å£: {performance_window}")
        logger.info(f"  é€²éšè€å¿ƒå€¼: {advancement_patience}")
    
    def get_current_stage(self) -> LearningStage:
        """ç²å–ç•¶å‰å­¸ç¿’éšæ®µ"""
        return self.current_stage
    
    def get_current_reward_function(self):
        """ç²å–ç•¶å‰éšæ®µçš„çå‹µå‡½æ•¸"""
        if self.current_stage == LearningStage.BASIC:
            return self.stage1_reward
        elif self.current_stage == LearningStage.INTERMEDIATE:
            return self.stage2_reward
        else:
            return self.stage3_reward
    
    def calculate_reward(self, metrics: Dict[str, float]) -> Tuple[float, RewardComponents, LearningMetrics]:
        """
        è¨ˆç®—ç•¶å‰éšæ®µçå‹µä¸¦æ›´æ–°å­¸ç¿’é€²åº¦
        
        Args:
            metrics: äº¤æ˜“ç¸¾æ•ˆæŒ‡æ¨™
            
        Returns:
            Tuple[ç¸½çå‹µ, çå‹µçµ„ä»¶, å­¸ç¿’æŒ‡æ¨™]
        """
        # ç²å–ç•¶å‰çå‹µå‡½æ•¸
        reward_fn = self.get_current_reward_function()
        
        # è¨ˆç®—çå‹µçµ„ä»¶
        reward_components = reward_fn(metrics)
        total_reward = reward_components.total_reward
        
        # æ›´æ–°æ€§èƒ½æ­·å²
        self.stage_performance_history[self.current_stage].append(total_reward)
        
        # æª¢æŸ¥æ˜¯å¦æ‡‰è©²é€²éš
        should_advance, advancement_reason = self._should_advance_stage(metrics, total_reward)
        
        # å‰µå»ºå­¸ç¿’æŒ‡æ¨™
        learning_metrics = self._create_learning_metrics(should_advance, advancement_reason)
        
        # æ›´æ–°éšæ®µ
        if should_advance:
            self._advance_to_next_stage(advancement_reason)
        
        # æ›´æ–°è¨ˆæ•¸å™¨
        self.current_episode += 1
        self.stage_episodes += 1
        
        return total_reward, reward_components, learning_metrics
    
    def _should_advance_stage(self, metrics: Dict[str, float], current_reward: float) -> Tuple[bool, str]:
        """åˆ¤æ–·æ˜¯å¦æ‡‰è©²é€²å…¥ä¸‹ä¸€éšæ®µ"""
        if self.force_advancement:
            return True, "å¼·åˆ¶é€²éš"
            
        if self.current_stage == LearningStage.ADVANCED:
            return False, ""  # å·²ç¶“æ˜¯æœ€å¾Œéšæ®µ
        
        # æª¢æŸ¥æœ€å°å›åˆæ•¸
        if self.stage_episodes < self.min_stage_episodes:
            return False, f"å›åˆæ•¸ä¸è¶³ ({self.stage_episodes}/{self.min_stage_episodes})"
        
        # ç²å–ç•¶å‰éšæ®µæ¨™æº–
        criteria = self.advancement_criteria[self.current_stage]
        stage_history = self.stage_performance_history[self.current_stage]
          # è¨ˆç®—æœ€è¿‘æ€§èƒ½å’Œæ”¹å–„è¶¨å‹¢
        recent_window = min(len(stage_history), self.performance_window)
        if recent_window < 5:  # éœ€è¦è‡³å°‘5å€‹æ•¸æ“šé»
            return False, "æ•¸æ“šé»ä¸è¶³"
        
        recent_rewards = stage_history[-recent_window:]
        avg_reward = np.mean(recent_rewards)
        
        # è¨ˆç®—æ”¹å–„è¶¨å‹¢ (æ¯”è¼ƒå‰åŠæ®µå’Œå¾ŒåŠæ®µ)
        if len(stage_history) >= 10:
            early_rewards = stage_history[:len(stage_history)//2]
            late_rewards = stage_history[len(stage_history)//2:]
            improvement = np.mean(late_rewards) - np.mean(early_rewards)
        else:
            improvement = 0.0
        
        # æª¢æŸ¥åŸºæœ¬æ€§èƒ½è¦æ±‚ (çµåˆçµ•å°å€¼å’Œæ”¹å–„è¶¨å‹¢)
        min_reward_met = avg_reward >= criteria['min_avg_reward']
        improvement_met = improvement >= criteria.get('improvement_threshold', 0.0)
        
        if not (min_reward_met or improvement_met):
            return False, f"çå‹µä¸è¶³ä¸”ç„¡æ”¹å–„ (çå‹µ:{avg_reward:.3f}, æ”¹å–„:{improvement:.3f})"
        
        # éšæ®µç‰¹å®šæª¢æŸ¥
        advancement_conditions = []
        
        if self.current_stage == LearningStage.BASIC:
            # åŸºç¤éšæ®µæª¢æŸ¥
            win_rate = metrics.get('win_rate', 0.0)
            max_drawdown = metrics.get('max_drawdown', 1.0)
            
            conditions_met = (
                win_rate >= criteria['win_rate_threshold'] and
                max_drawdown <= criteria['max_drawdown_threshold']
            )
            
            advancement_conditions = [
                f"å‹ç‡: {win_rate:.3f}>={criteria['win_rate_threshold']}",
                f"æœ€å¤§å›æ’¤: {max_drawdown:.3f}<={criteria['max_drawdown_threshold']}"
            ]
            
        elif self.current_stage == LearningStage.INTERMEDIATE:
            # ä¸­ç´šéšæ®µæª¢æŸ¥
            sharpe = metrics.get('sharpe_ratio', 0.0)
            max_drawdown = metrics.get('max_drawdown', 1.0)
            win_rate = metrics.get('win_rate', 0.0)
            
            conditions_met = (
                sharpe >= criteria['sharpe_threshold'] and
                max_drawdown <= criteria['max_drawdown_threshold'] and
                win_rate >= criteria['win_rate_threshold']
            )
            
            advancement_conditions = [
                f"å¤æ™®æ¯”ç‡: {sharpe:.3f}>={criteria['sharpe_threshold']}",
                f"æœ€å¤§å›æ’¤: {max_drawdown:.3f}<={criteria['max_drawdown_threshold']}",
                f"å‹ç‡: {win_rate:.3f}>={criteria['win_rate_threshold']}"
            ]
        
        else:
            conditions_met = False
            advancement_conditions = ["å·²é”æœ€é«˜éšæ®µ"]
          # æ›´æ–°é€²éšé€²åº¦ (ä½¿ç”¨æ›´éˆæ´»çš„æ¨™æº–)
        current_progress = self.advancement_progress[self.current_stage]
        
        if conditions_met or (min_reward_met and improvement_met):
            self.advancement_progress[self.current_stage] += 1
            progress_reason = "æ¢ä»¶é”æ¨™" if conditions_met else "çå‹µæ”¹å–„é¡¯è‘—"
        else:
            # å¦‚æœæœ‰éƒ¨åˆ†æ”¹å–„ï¼Œä¿æŒéƒ¨åˆ†é€²åº¦ï¼Œä¸å®Œå…¨é‡ç½®
            if improvement > 0 or avg_reward > criteria['min_avg_reward'] * 0.8:
                self.advancement_progress[self.current_stage] = max(0, current_progress - 1)
            else:
                self.advancement_progress[self.current_stage] = 0  # é‡ç½®é€²åº¦
        
        # æª¢æŸ¥æ˜¯å¦æ»¿è¶³é€£çºŒæ€§è¦æ±‚ (ä½¿ç”¨å‹•æ…‹æ¨™æº–)
        required_consistency = criteria['consistency_episodes']
        current_progress = self.advancement_progress[self.current_stage]
        
        # å¦‚æœå­¸ç¿’é€²åº¦è‰¯å¥½ï¼Œå¯ä»¥é©ç•¶é™ä½é€£çºŒæ€§è¦æ±‚
        if improvement > criteria.get('improvement_threshold', 0.0) * 2:
            required_consistency = max(3, required_consistency - 2)  # æœ€å°‘3æ¬¡
        
        if current_progress >= required_consistency:
            if conditions_met:
                reason = f"{criteria['description']} - é€£çºŒ{current_progress}å›åˆé”æ¨™"
            else:
                reason = f"{criteria['description']} - çå‹µæ”¹å–„é¡¯è‘— ({improvement:.3f})"
            return True, reason
        
        return False, f"é€²éšé€²åº¦: {current_progress}/{required_consistency} ({progress_reason if 'progress_reason' in locals() else 'æ¨™æº–æœªé”æ¨™'}) - " + ", ".join(advancement_conditions)
    
    def _advance_to_next_stage(self, reason: str):
        """é€²å…¥ä¸‹ä¸€å­¸ç¿’éšæ®µ"""
        old_stage = self.current_stage
        
        if self.current_stage == LearningStage.BASIC:
            self.current_stage = LearningStage.INTERMEDIATE
            logger.info(f"ğŸ¯ å­¸ç¿’éšæ®µå‡ç´š: åŸºç¤å­¸ç¿’ â†’ é¢¨éšªç®¡ç†å¼·åŒ–")
        elif self.current_stage == LearningStage.INTERMEDIATE:
            self.current_stage = LearningStage.ADVANCED
            logger.info(f"ğŸš€ å­¸ç¿’éšæ®µå‡ç´š: é¢¨éšªç®¡ç† â†’ è¤‡é›œç­–ç•¥æŒæ¡")
        
        logger.info(f"   å‡ç´šåŸå› : {reason}")
        logger.info(f"   éšæ®µ{old_stage.name}å®Œæˆ: {self.stage_episodes}å›åˆ")
        
        # é‡ç½®éšæ®µè¨ˆæ•¸å™¨
        self.stage_episodes = 0
        self.force_advancement = False
        
        # é‡ç½®é€²éšé€²åº¦
        if old_stage in self.advancement_progress:
            self.advancement_progress[old_stage] = 0
    
    def _create_learning_metrics(self, should_advance: bool, advancement_reason: str) -> LearningMetrics:
        """å‰µå»ºå­¸ç¿’é€²åº¦æŒ‡æ¨™"""
        # è¨ˆç®—éšæ®µè¡¨ç¾
        stage_history = self.stage_performance_history[self.current_stage]
        stage_performance = np.mean(stage_history[-10:]) if len(stage_history) >= 10 else np.mean(stage_history) if stage_history else 0.0
        
        # è¨ˆç®—éšæ®µé€²åº¦ (åŸºæ–¼æ€§èƒ½æ”¹å–„)
        if len(stage_history) >= 20:
            early_performance = np.mean(stage_history[:10])
            recent_performance = np.mean(stage_history[-10:])
            stage_progress = min(1.0, max(0.0, (recent_performance - early_performance + 1) / 2))
        else:
            stage_progress = len(stage_history) / max(self.min_stage_episodes, 20)
        
        # è¨ˆç®—é€²éšé€²åº¦
        if self.current_stage in self.advancement_progress:
            required_consistency = self.advancement_criteria[self.current_stage]['consistency_episodes']
            current_progress = self.advancement_progress[self.current_stage]
            advancement_progress = current_progress / required_consistency
        else:
            advancement_progress = 1.0  # æœ€é«˜éšæ®µ
        
        return LearningMetrics(
            stage=self.current_stage,
            episode=self.current_episode,
            stage_episodes=self.stage_episodes,
            stage_progress=stage_progress,
            stage_performance=stage_performance,
            advancement_progress=advancement_progress,
            should_advance=should_advance,
            advancement_reason=advancement_reason,
            timestamp=datetime.now()
        )
    
    def force_stage_advancement(self):
        """å¼·åˆ¶é€²å…¥ä¸‹ä¸€éšæ®µ"""
        if self.current_stage != LearningStage.ADVANCED:
            self.force_advancement = True
            logger.info(f"è¨­ç½®å¼·åˆ¶é€²éšæ¨™èªŒ: {self.current_stage.name}")
    
    def get_stage_criteria_status(self) -> Dict[str, Any]:
        """ç²å–ç•¶å‰éšæ®µçš„æ¢ä»¶é”æˆç‹€æ…‹"""
        if self.current_stage == LearningStage.ADVANCED:
            return {"stage": "ADVANCED", "status": "å·²é”æœ€é«˜éšæ®µ"}
        
        criteria = self.advancement_criteria[self.current_stage]
        stage_history = self.stage_performance_history[self.current_stage]
        
        if len(stage_history) < 5:
            return {"stage": self.current_stage.name, "status": "æ•¸æ“šä¸è¶³"}
        
        recent_rewards = stage_history[-self.performance_window:]
        avg_reward = np.mean(recent_rewards)
        
        current_progress = self.advancement_progress[self.current_stage]
        required_consistency = criteria['consistency_episodes']
        
        return {
            "stage": self.current_stage.name,
            "stage_episodes": self.stage_episodes,
            "min_episodes": criteria['min_episodes'],
            "avg_reward": avg_reward,
            "required_avg_reward": criteria['min_avg_reward'],
            "advancement_progress": current_progress,
            "required_consistency": required_consistency,
            "progress_percentage": (current_progress / required_consistency) * 100,
            "criteria": criteria,
            "ready_for_advancement": current_progress >= required_consistency
        }
    
    def get_learning_statistics(self) -> Dict[str, Any]:
        """ç²å–å­¸ç¿’çµ±è¨ˆä¿¡æ¯"""
        stats = {
            'current_stage': self.current_stage.name,
            'current_episode': self.current_episode,
            'stage_episodes': self.stage_episodes,
            'advancement_criteria_status': self.get_stage_criteria_status(),
            'stage_performances': {}
        }
        
        for stage, history in self.stage_performance_history.items():
            if history:
                stats['stage_performances'][stage.name] = {
                    'episodes': len(history),
                    'avg_reward': np.mean(history),
                    'max_reward': np.max(history),
                    'min_reward': np.min(history),
                    'recent_avg': np.mean(history[-10:]) if len(history) >= 10 else np.mean(history),
                    'performance_trend': self._calculate_trend(history)
                }
            else:
                stats['stage_performances'][stage.name] = {
                    'episodes': 0,
                    'avg_reward': 0.0,
                    'max_reward': 0.0,
                    'min_reward': 0.0,
                    'recent_avg': 0.0,
                    'performance_trend': 0.0
                }
        
        return stats
    
    def _calculate_trend(self, history: List[float]) -> float:
        """è¨ˆç®—æ€§èƒ½è¶¨å‹¢"""
        if len(history) < 10:
            return 0.0
        
        # ä½¿ç”¨ç·šæ€§å›æ­¸è¨ˆç®—è¶¨å‹¢
        x = np.arange(len(history))
        y = np.array(history)
        
        # è¨ˆç®—æ–œç‡
        n = len(history)
        slope = (n * np.sum(x * y) - np.sum(x) * np.sum(y)) / (n * np.sum(x**2) - np.sum(x)**2)
        
        return slope
    
    def reset_system(self):
        """é‡ç½®å­¸ç¿’ç³»çµ±"""
        self.current_episode = 0
        self.stage_episodes = 0
        self.current_stage = LearningStage.BASIC
        self.force_advancement = False
        
        for stage in self.stage_performance_history:
            self.stage_performance_history[stage].clear()
        
        for stage in self.advancement_progress:
            self.advancement_progress[stage] = 0
            
        logger.info("æ¼¸é€²å¼å­¸ç¿’ç³»çµ±å·²é‡ç½®")
    
    def save_checkpoint(self, filepath: str):
        """ä¿å­˜å­¸ç¿’é€²åº¦æª¢æŸ¥é»"""
        checkpoint = {
            'current_episode': self.current_episode,
            'stage_episodes': self.stage_episodes,
            'current_stage': self.current_stage.value,
            'stage_performance_history': {
                stage.value: history for stage, history in self.stage_performance_history.items()
            },
            'advancement_progress': {
                stage.value: progress for stage, progress in self.advancement_progress.items()
            },
            'min_stage_episodes': self.min_stage_episodes,
            'performance_window': self.performance_window,
            'advancement_patience': self.advancement_patience
        }
        
        torch.save(checkpoint, filepath)
        logger.info(f"å­¸ç¿’é€²åº¦æª¢æŸ¥é»å·²ä¿å­˜: {filepath}")
    
    def load_checkpoint(self, filepath: str):
        """è¼‰å…¥å­¸ç¿’é€²åº¦æª¢æŸ¥é»"""
        try:
            checkpoint = torch.load(filepath, map_location=self.device)
            
            self.current_episode = checkpoint['current_episode']
            self.stage_episodes = checkpoint['stage_episodes']
            self.current_stage = LearningStage(checkpoint['current_stage'])
            
            # è¼‰å…¥æ€§èƒ½æ­·å²
            for stage_value, history in checkpoint['stage_performance_history'].items():
                stage = LearningStage(stage_value)
                self.stage_performance_history[stage] = history
            
            # è¼‰å…¥é€²éšé€²åº¦
            for stage_value, progress in checkpoint['advancement_progress'].items():
                stage = LearningStage(stage_value)
                self.advancement_progress[stage] = progress
            
            logger.info(f"å­¸ç¿’é€²åº¦æª¢æŸ¥é»å·²è¼‰å…¥: {filepath}")
            logger.info(f"ç•¶å‰éšæ®µ: {self.current_stage.name}, ç¸½å›åˆ: {self.current_episode}, éšæ®µå›åˆ: {self.stage_episodes}")
            
        except Exception as e:
            logger.error(f"è¼‰å…¥æª¢æŸ¥é»å¤±æ•—: {e}")
            raise
    
    def get_reward_function_description(self) -> Dict[str, str]:
        """ç²å–ç•¶å‰çå‹µå‡½æ•¸çš„æè¿°"""
        descriptions = {
            LearningStage.BASIC: {
                "focus": "åŸºç¤äº¤æ˜“åŸç†å­¸ç¿’",
                "components": "åŸºæœ¬ç›ˆè™§(70%) + é¢¨éšªæ§åˆ¶(20%) + äº¤æ˜“é »ç‡(10%)",
                "objectives": "å­¸ç¿’è²·è³£æ™‚æ©Ÿã€é¢¨éšªæ„è­˜ã€é¿å…éåº¦äº¤æ˜“"
            },
            LearningStage.INTERMEDIATE: {
                "focus": "é¢¨éšªç®¡ç†å¼·åŒ–",
                "components": "åŸºç¤çå‹µ + å¤æ™®æ¯”ç‡(25%) + å›æ’¤æ§åˆ¶(20%) + å‹ç‡(15%)",
                "objectives": "é¢¨éšªèª¿æ•´æ”¶ç›Šã€å‹ç‡å¹³è¡¡ã€ç©©å®šæ€§è¿½æ±‚"
            },
            LearningStage.ADVANCED: {
                "focus": "è¤‡é›œç­–ç•¥æŒæ¡",
                "components": "ä¸­ç´šçå‹µ + Sortinoæ¯”ç‡ + VaR + ååº¦ + å³°åº¦ + æˆæœ¬ + ä¸€è‡´æ€§",
                "objectives": "è¤‡é›œç’°å¢ƒé©æ‡‰ã€å¤šç­–ç•¥çµ„åˆã€ç­–ç•¥å‰µæ–°"
            }
        }
        
        return descriptions[self.current_stage]
    
    def __repr__(self):
        stats = self.get_learning_statistics()
        advancement_status = stats['advancement_criteria_status']
        
        return (f"ProgressiveLearningSystem("
                f"stage={stats['current_stage']}, "
                f"episode={stats['current_episode']}, "
                f"stage_progress={advancement_status.get('progress_percentage', 0):.1f}%)")


def test_progressive_learning_system():
    """æ¸¬è©¦æ¼¸é€²å¼å­¸ç¿’ç³»çµ±"""
    logger.info("ğŸ§ª é–‹å§‹æ¸¬è©¦æ¼¸é€²å¼å­¸ç¿’ç³»çµ±...")
    
    # å‰µå»ºå­¸ç¿’ç³»çµ±
    learning_system = ProgressiveLearningSystem(
        min_stage_episodes=20,
        performance_window=10,
        advancement_patience=5
    )
    
    logger.info(f"åˆå§‹åŒ–: {learning_system}")
    
    # æ¨¡æ“¬å­¸ç¿’éç¨‹
    test_episodes = 100
    stage_transitions = []
    
    for episode in range(test_episodes):
        # æ¨¡æ“¬äº¤æ˜“æŒ‡æ¨™ (éš¨è‘—å›åˆå¢åŠ ï¼Œæ€§èƒ½é€æ¼¸æå‡)
        stage_factor = learning_system.current_stage.value
        base_performance = 0.1 + (episode / test_episodes) * 0.6 + stage_factor * 0.1
        noise = np.random.normal(0, 0.1)
        
        # ç”Ÿæˆä¸åŒéšæ®µé©åˆçš„æŒ‡æ¨™
        mock_metrics = {
            'pnl': base_performance + noise,
            'drawdown': max(0, 0.15 - episode * 0.001 + abs(noise) * 0.3),
            'trade_frequency': 0.05 + np.random.uniform(-0.02, 0.02),
            'sharpe_ratio': 0.3 + episode * 0.015 + noise * 0.5,
            'max_drawdown': max(0, 0.20 - episode * 0.002),
            'win_rate': 0.35 + episode * 0.003 + noise * 0.1,
            'sortino_ratio': 0.4 + episode * 0.01 + noise * 0.3,
            'var_risk': -0.05 - abs(noise) * 0.02,
            'skewness': noise * 0.5,
            'kurtosis': 3.0 + abs(noise),
            'transaction_costs': 0.01 + np.random.uniform(0, 0.005),
            'consistency_score': min(1.0, 0.3 + episode * 0.005 + noise * 0.2)
        }
        
        # è¨ˆç®—çå‹µ
        total_reward, components, learning_metrics = learning_system.calculate_reward(mock_metrics)
        
        # è¨˜éŒ„éšæ®µè½‰æ›
        if learning_metrics.should_advance:
            stage_transitions.append({
                'episode': episode,
                'from_stage': learning_metrics.stage.name,
                'reason': learning_metrics.advancement_reason
            })
            logger.info(f"ğŸ”„ å›åˆ {episode}: éšæ®µå‡ç´š!")
            logger.info(f"   åŸå› : {learning_metrics.advancement_reason}")
        
        # æ¯20å€‹å›åˆè¼¸å‡ºä¸€æ¬¡é€²åº¦
        if episode % 20 == 0:
            criteria_status = learning_system.get_stage_criteria_status()
            logger.info(f"å›åˆ {episode}: éšæ®µ={learning_metrics.stage.name}, "
                       f"çå‹µ={total_reward:.3f}, "
                       f"é€²éšé€²åº¦={criteria_status.get('progress_percentage', 0):.1f}%")
    
    # è¼¸å‡ºæœ€çµ‚çµ±è¨ˆ
    final_stats = learning_system.get_learning_statistics()
    logger.info("\n=== ğŸ“Š å­¸ç¿’çµ±è¨ˆ ===")
    
    for stage_name, perf in final_stats['stage_performances'].items():
        if perf['episodes'] > 0:
            logger.info(f"{stage_name}: {perf['episodes']}å›åˆ, "
                       f"å¹³å‡çå‹µ={perf['avg_reward']:.3f}, "
                       f"è¶¨å‹¢={perf['performance_trend']:.4f}")
    
    # è¼¸å‡ºéšæ®µè½‰æ›æ­·å²
    logger.info(f"\n=== ğŸ¯ éšæ®µè½‰æ›æ­·å² ===")
    for transition in stage_transitions:
        logger.info(f"å›åˆ {transition['episode']}: {transition['from_stage']} - {transition['reason']}")
    
    # è¼¸å‡ºç•¶å‰çå‹µå‡½æ•¸æè¿°
    reward_desc = learning_system.get_reward_function_description()
    logger.info(f"\n=== ğŸ® ç•¶å‰çå‹µå‡½æ•¸ ===")
    logger.info(f"å°ˆæ³¨é ˜åŸŸ: {reward_desc['focus']}")
    logger.info(f"çµ„ä»¶: {reward_desc['components']}")
    logger.info(f"ç›®æ¨™: {reward_desc['objectives']}")
    
    logger.info(f"\næœ€çµ‚ç‹€æ…‹: {learning_system}")
    logger.info("âœ… æ¼¸é€²å¼å­¸ç¿’ç³»çµ±æ¸¬è©¦å®Œæˆ!")
    
    return learning_system


if __name__ == "__main__":
    # é‹è¡Œæ¸¬è©¦
    test_system = test_progressive_learning_system()