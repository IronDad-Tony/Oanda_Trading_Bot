# src/agent/enhanced_quantum_strategy_layer.py
"""
å¢å¼·ç‰ˆé‡å­ç­–ç•¥å±¤å¯¦ç¾
æ“´å±•åŸæœ‰3ç¨®ç­–ç•¥è‡³15+ç¨®ç­–ç•¥ï¼Œå¯¦ç¾éšæ®µä¸€æ ¸å¿ƒæ¶æ§‹å¢å¼·

ä¸»è¦å¢å¼·ï¼š
1. 15ç¨®å°ˆæ¥­äº¤æ˜“ç­–ç•¥å¯¦ç¾
2. å‹•æ…‹ç­–ç•¥ç”Ÿæˆå™¨
3. é‡å­ç­–ç•¥çµ„åˆå„ªåŒ–
4. è‡ªé©æ‡‰æ¬Šé‡å­¸ç¿’æ©Ÿåˆ¶
5. ç­–ç•¥å‰µæ–°å¼•æ“ï¼ˆåŸºç¤ç‰ˆï¼‰
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from typing import Dict, List, Tuple, Optional, Any, Union
import logging
from abc import ABC, abstractmethod
import math
from dataclasses import dataclass

try:
    from src.common.logger_setup import logger
    from src.common.config import DEVICE, MAX_SYMBOLS_ALLOWED
    from src.agent.quantum_strategy_layer import BaseStrategy, QuantumEncoder
except ImportError:
    logger = logging.getLogger(__name__)
    DEVICE = "cpu"
    MAX_SYMBOLS_ALLOWED = 5


# è‡ªå®šç¾©æ¿€æ´»å‡½æ•¸å¯¦ç¾
class Swish(nn.Module):
    """Swishæ¿€æ´»å‡½æ•¸å¯¦ç¾"""
    def forward(self, x):
        return x * torch.sigmoid(x)


class Mish(nn.Module):
    """Mishæ¿€æ´»å‡½æ•¸å¯¦ç¾"""
    def forward(self, x):
        return x * torch.tanh(F.softplus(x))


# åŸºç¤ç­–ç•¥é¡ï¼ˆå¦‚æœå°å…¥å¤±æ•—å‰‡å®šç¾©ï¼‰
if 'BaseStrategy' not in globals():
    class BaseStrategy(nn.Module, ABC):
        @abstractmethod
        def forward(self, state: torch.Tensor) -> torch.Tensor:
            pass
        
        @abstractmethod
        def get_strategy_name(self) -> str:
            pass


if 'QuantumEncoder' not in globals():
    class QuantumEncoder(nn.Module):
        def __init__(self, input_dim: int, latent_dim: int, num_qubits: int = 8):
            super().__init__()
            self.encoding_layers = nn.Sequential(
                nn.Linear(input_dim, latent_dim),
                nn.GELU()
            )
        
        def forward(self, x: torch.Tensor) -> torch.Tensor:
            return self.encoding_layers(x)


@dataclass
class StrategyConfig:
    """ç­–ç•¥é…ç½®é¡"""
    name: str
    description: str
    risk_level: float  # 0.0-1.0
    market_regime: str  # "trending", "ranging", "volatile", "all"
    complexity: int    # 1-5
    base_performance: float = 0.5


# ===============================
# 15ç¨®å°ˆæ¥­äº¤æ˜“ç­–ç•¥å¯¦ç¾
# ===============================

class MomentumStrategy(BaseStrategy):
    """å‹•é‡ç­–ç•¥ï¼šåŸºæ–¼åƒ¹æ ¼å‹•é‡é€²è¡Œäº¤æ˜“"""
    
    def __init__(self, state_dim: int, action_dim: int):
        super().__init__()
        self.strategy_net = nn.Sequential(
            nn.Linear(state_dim, 128),
            nn.ReLU(),
            nn.LayerNorm(128),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, action_dim),
            nn.Tanh()
        )
        self.momentum_amplifier = nn.Parameter(torch.tensor(1.5))
        
    def forward(self, state: torch.Tensor) -> torch.Tensor:
        base_output = self.strategy_net(state)
        return base_output * self.momentum_amplifier
    
    def get_strategy_name(self) -> str:
        return "Momentum"


class BreakoutStrategy(BaseStrategy):
    """çªç ´ç­–ç•¥ï¼šè­˜åˆ¥ä¸¦è·Ÿéš¨åƒ¹æ ¼çªç ´"""
    
    def __init__(self, state_dim: int, action_dim: int):
        super().__init__()
        self.strategy_net = nn.Sequential(
            nn.Linear(state_dim, 128),
            nn.GELU(),
            nn.BatchNorm1d(128),
            nn.Linear(128, 64),
            nn.GELU(),
            nn.Linear(64, action_dim),
            nn.Tanh()
        )
        self.breakout_threshold = nn.Parameter(torch.tensor(0.3))
        
    def forward(self, state: torch.Tensor) -> torch.Tensor:
        raw_signal = self.strategy_net(state)
        # æ‡‰ç”¨çªç ´é–¾å€¼
        breakout_mask = torch.abs(raw_signal) > self.breakout_threshold
        return raw_signal * breakout_mask.float() * 1.3
    
    def get_strategy_name(self) -> str:
        return "Breakout"


class StatisticalArbitrageStrategy(BaseStrategy):
    """çµ±è¨ˆå¥—åˆ©ç­–ç•¥ï¼šåŸºæ–¼çµ±è¨ˆæ¨¡å‹çš„å¥—åˆ©æ©Ÿæœƒ"""
    
    def __init__(self, state_dim: int, action_dim: int):
        super().__init__()
        self.strategy_net = nn.Sequential(
            nn.Linear(state_dim, 128),
            Swish(),
            nn.LayerNorm(128),
            nn.Linear(128, 96),
            Swish(),
            nn.Linear(96, 64),
            Swish(),
            nn.Linear(64, action_dim),
            nn.Tanh()
        )
        self.correlation_matrix = nn.Parameter(torch.eye(action_dim) * 0.1)
        
    def forward(self, state: torch.Tensor) -> torch.Tensor:
        base_signal = self.strategy_net(state)
        # æ‡‰ç”¨çµ±è¨ˆç›¸é—œæ€§èª¿æ•´
        adjusted_signal = torch.matmul(base_signal.unsqueeze(1), self.correlation_matrix).squeeze(1)
        return adjusted_signal * 0.8  # ä¿å®ˆçš„å¥—åˆ©ä¿¡è™Ÿ
    
    def get_strategy_name(self) -> str:
        return "StatisticalArbitrage"


class OptionFlowStrategy(BaseStrategy):
    """æœŸæ¬Šæµç­–ç•¥ï¼šåŸºæ–¼æœŸæ¬Šå¸‚å ´ä¿¡è™Ÿ"""
    
    def __init__(self, state_dim: int, action_dim: int):
        super().__init__()
        self.strategy_net = nn.Sequential(
            nn.Linear(state_dim, 128),
            nn.ELU(),
            nn.LayerNorm(128),
            nn.Linear(128, 64),
            nn.ELU(),
            nn.Linear(64, action_dim),
            nn.Tanh()
        )
        self.option_sensitivity = nn.Parameter(torch.tensor(0.7))
        
    def forward(self, state: torch.Tensor) -> torch.Tensor:
        return self.strategy_net(state) * self.option_sensitivity
    
    def get_strategy_name(self) -> str:
        return "OptionFlow"


class MicrostructureStrategy(BaseStrategy):
    """å¸‚å ´å¾®è§€çµæ§‹ç­–ç•¥ï¼šåŸºæ–¼é«˜é »äº¤æ˜“ä¿¡è™Ÿ"""
    
    def __init__(self, state_dim: int, action_dim: int):
        super().__init__()
        self.strategy_net = nn.Sequential(
            nn.Linear(state_dim, 128),
            nn.GELU(),
            nn.GroupNorm(8, 128),
            nn.Linear(128, 64),
            nn.GELU(),
            nn.Linear(64, action_dim),
            nn.Tanh()
        )
        self.microstructure_weight = nn.Parameter(torch.tensor(0.6))
        
    def forward(self, state: torch.Tensor) -> torch.Tensor:
        return self.strategy_net(state) * self.microstructure_weight
    
    def get_strategy_name(self) -> str:
        return "Microstructure"


class VolatilityStrategy(BaseStrategy):
    """æ³¢å‹•ç‡ç­–ç•¥ï¼šåŸºæ–¼æ³¢å‹•ç‡æ¨¡å¼çš„äº¤æ˜“"""
    
    def __init__(self, state_dim: int, action_dim: int):
        super().__init__()
        self.strategy_net = nn.Sequential(
            nn.Linear(state_dim, 128),
            Mish(),
            nn.LayerNorm(128),
            nn.Linear(128, 64),
            Mish(),
            nn.Linear(64, action_dim),
            nn.Tanh()
        )
        self.vol_scaling = nn.Parameter(torch.tensor(1.1))
        
    def forward(self, state: torch.Tensor) -> torch.Tensor:
        return self.strategy_net(state) * self.vol_scaling
    
    def get_strategy_name(self) -> str:
        return "Volatility"


class CarryTradeStrategy(BaseStrategy):
    """åˆ©å·®äº¤æ˜“ç­–ç•¥ï¼šåŸºæ–¼åˆ©ç‡å·®ç•°"""
    
    def __init__(self, state_dim: int, action_dim: int):
        super().__init__()
        self.strategy_net = nn.Sequential(
            nn.Linear(state_dim, 128),
            nn.LeakyReLU(0.1),
            nn.BatchNorm1d(128),
            nn.Linear(128, 64),
            nn.LeakyReLU(0.1),
            nn.Linear(64, action_dim),
            nn.Tanh()
        )
        self.carry_multiplier = nn.Parameter(torch.tensor(0.9))
        
    def forward(self, state: torch.Tensor) -> torch.Tensor:
        return self.strategy_net(state) * self.carry_multiplier
    
    def get_strategy_name(self) -> str:
        return "CarryTrade"


class MacroEconomicStrategy(BaseStrategy):
    """å®è§€ç¶“æ¿Ÿç­–ç•¥ï¼šåŸºæ–¼å®è§€ç¶“æ¿ŸæŒ‡æ¨™"""
    
    def __init__(self, state_dim: int, action_dim: int):
        super().__init__()
        self.strategy_net = nn.Sequential(
            nn.Linear(state_dim, 128),
            nn.SELU(),
            nn.LayerNorm(128),
            nn.Linear(128, 64),
            nn.SELU(),
            nn.Linear(64, action_dim),
            nn.Tanh()
        )
        self.macro_influence = nn.Parameter(torch.tensor(0.8))
        
    def forward(self, state: torch.Tensor) -> torch.Tensor:
        return self.strategy_net(state) * self.macro_influence
    
    def get_strategy_name(self) -> str:
        return "MacroEconomic"


class EventDrivenStrategy(BaseStrategy):
    """äº‹ä»¶é©…å‹•ç­–ç•¥ï¼šåŸºæ–¼å¸‚å ´äº‹ä»¶å’Œæ–°è"""
    
    def __init__(self, state_dim: int, action_dim: int):
        super().__init__()
        self.strategy_net = nn.Sequential(
            nn.Linear(state_dim, 128),
            nn.GELU(),
            nn.LayerNorm(128),
            nn.Linear(128, 64),
            nn.GELU(),
            nn.Linear(64, action_dim),
            nn.Tanh()
        )
        self.event_intensity = nn.Parameter(torch.tensor(1.2))
        
    def forward(self, state: torch.Tensor) -> torch.Tensor:
        return self.strategy_net(state) * self.event_intensity
    
    def get_strategy_name(self) -> str:
        return "EventDriven"


class SentimentStrategy(BaseStrategy):
    """å¸‚å ´æƒ…ç·’ç­–ç•¥ï¼šåŸºæ–¼å¸‚å ´æƒ…ç·’æŒ‡æ¨™"""
    
    def __init__(self, state_dim: int, action_dim: int):
        super().__init__()
        self.strategy_net = nn.Sequential(
            nn.Linear(state_dim, 128),
            Swish(),
            nn.LayerNorm(128),
            nn.Linear(128, 64),
            Swish(),
            nn.Linear(64, action_dim),
            nn.Tanh()
        )
        self.sentiment_factor = nn.Parameter(torch.tensor(0.7))
        
    def forward(self, state: torch.Tensor) -> torch.Tensor:
        return self.strategy_net(state) * self.sentiment_factor
    
    def get_strategy_name(self) -> str:
        return "Sentiment"


class QuantitativeStrategy(BaseStrategy):
    """é‡åŒ–å› å­ç­–ç•¥ï¼šåŸºæ–¼é‡åŒ–å› å­æ¨¡å‹"""
    
    def __init__(self, state_dim: int, action_dim: int):
        super().__init__()
        self.strategy_net = nn.Sequential(
            nn.Linear(state_dim, 128),
            nn.ReLU(),
            nn.LayerNorm(128),
            nn.Linear(128, 96),
            nn.ReLU(),            nn.Linear(96, 64),
            nn.ReLU(),
            nn.Linear(64, action_dim),
            nn.Tanh()
        )
        self.factor_loadings = nn.Parameter(torch.randn(action_dim, 5) * 0.1)
        
    def forward(self, state: torch.Tensor) -> torch.Tensor:
        base_signal = self.strategy_net(state)
        # æ‡‰ç”¨å› å­è¼‰è· - ä¿®å¾©ç¶­åº¦å•é¡Œ
        factor_effect = torch.matmul(base_signal, self.factor_loadings).mean(dim=-1, keepdim=True)
        enhanced_signal = base_signal + factor_effect * 0.1
        return enhanced_signal * 0.9
    
    def get_strategy_name(self) -> str:
        return "Quantitative"


class PairsTradeStrategy(BaseStrategy):
    """é…å°äº¤æ˜“ç­–ç•¥ï¼šåŸºæ–¼é…å°è‚¡ç¥¨çš„ç›¸å°åƒ¹å€¼"""
    
    def __init__(self, state_dim: int, action_dim: int):
        super().__init__()
        self.strategy_net = nn.Sequential(
            nn.Linear(state_dim, 128),
            nn.GELU(),
            nn.LayerNorm(128),
            nn.Linear(128, 64),
            nn.GELU(),
            nn.Linear(64, action_dim),
            nn.Tanh()
        )
        self.pairs_correlation = nn.Parameter(torch.eye(action_dim) * 0.2)
        
    def forward(self, state: torch.Tensor) -> torch.Tensor:
        base_signal = self.strategy_net(state)
        pairs_adjusted = torch.matmul(base_signal.unsqueeze(1), self.pairs_correlation).squeeze(1)
        return pairs_adjusted * 0.85
    
    def get_strategy_name(self) -> str:
        return "PairsTrade"


class MarketMakingStrategy(BaseStrategy):
    """åšå¸‚ç­–ç•¥ï¼šæä¾›æµå‹•æ€§ç²å–å·®åƒ¹"""
    
    def __init__(self, state_dim: int, action_dim: int):
        super().__init__()
        self.strategy_net = nn.Sequential(
            nn.Linear(state_dim, 128),
            nn.ELU(),
            nn.LayerNorm(128),
            nn.Linear(128, 64),
            nn.ELU(),
            nn.Linear(64, action_dim),
            nn.Tanh()
        )
        self.spread_sensitivity = nn.Parameter(torch.tensor(0.5))
        
    def forward(self, state: torch.Tensor) -> torch.Tensor:
        return self.strategy_net(state) * self.spread_sensitivity
    
    def get_strategy_name(self) -> str:
        return "MarketMaking"


class HighFrequencyStrategy(BaseStrategy):
    """é«˜é »äº¤æ˜“ç­–ç•¥ï¼šåŸºæ–¼å¾®ç§’ç´šå¸‚å ´å‹•æ…‹"""
    
    def __init__(self, state_dim: int, action_dim: int):
        super().__init__()
        self.strategy_net = nn.Sequential(
            nn.Linear(state_dim, 128),
            Mish(),
            nn.GroupNorm(8, 128),
            nn.Linear(128, 64),
            Mish(),
            nn.Linear(64, action_dim),
            nn.Tanh()
        )
        self.hft_multiplier = nn.Parameter(torch.tensor(0.4))
        
    def forward(self, state: torch.Tensor) -> torch.Tensor:
        return self.strategy_net(state) * self.hft_multiplier
    
    def get_strategy_name(self) -> str:
        return "HighFrequency"


class AlgorithmicStrategy(BaseStrategy):
    """ç®—æ³•äº¤æ˜“ç­–ç•¥ï¼šåŸºæ–¼é å®šç¾©äº¤æ˜“ç®—æ³•"""
    
    def __init__(self, state_dim: int, action_dim: int):
        super().__init__()
        self.strategy_net = nn.Sequential(
            nn.Linear(state_dim, 128),
            nn.SELU(),
            nn.LayerNorm(128),
            nn.Linear(128, 64),
            nn.SELU(),
            nn.Linear(64, action_dim),
            nn.Tanh()
        )
        self.algo_parameters = nn.Parameter(torch.randn(3) * 0.1)
        
    def forward(self, state: torch.Tensor) -> torch.Tensor:
        base_signal = self.strategy_net(state)
        # æ‡‰ç”¨ç®—æ³•åƒæ•¸èª¿æ•´
        algo_adjustment = torch.sum(self.algo_parameters) * 0.1
        return base_signal * (1.0 + algo_adjustment)
    
    def get_strategy_name(self) -> str:
        return "Algorithmic"


# ===============================
# å‹•æ…‹ç­–ç•¥ç”Ÿæˆå™¨
# ===============================

class DynamicStrategyGenerator(nn.Module):
    """
    å‹•æ…‹ç­–ç•¥ç”Ÿæˆå™¨ï¼šå¯¦æ™‚å‰µå»ºå’Œèª¿æ•´äº¤æ˜“ç­–ç•¥
    åŸºæ–¼éºå‚³ç®—æ³•å’Œç¥ç¶“é€²åŒ–çš„æ··åˆæ–¹æ³•
    æ”¯æŒå®Œå…¨å‹•æ…‹è‡ªé©æ‡‰ç¶­åº¦é…ç½®
    """
    
    def __init__(self, state_dim: int, action_dim: int, 
                 base_strategies: List[BaseStrategy],
                 max_generated_strategies: int = 5):
        super().__init__()
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.base_strategies = base_strategies
        self.max_generated_strategies = max_generated_strategies
        
        # å‹•æ…‹è¨ˆç®—ä¸­é–“å±¤ç¶­åº¦ - è‡ªé©æ‡‰ç¸®æ”¾
        self.gene_hidden_dim = max(16, min(128, state_dim // 4))  # å‹•æ…‹éš±è—å±¤ç¶­åº¦
        self.gene_latent_dim = max(8, min(64, state_dim // 8))   # å‹•æ…‹æ½›åœ¨ç¶­åº¦
        self.decoder_hidden_1 = max(32, min(256, action_dim * 2))  # è§£ç¢¼å™¨ç¬¬ä¸€å±¤
        self.decoder_hidden_2 = max(64, min(512, action_dim * 4))  # è§£ç¢¼å™¨ç¬¬äºŒå±¤
        self.fitness_hidden_1 = max(32, min(128, (state_dim + action_dim) // 2))
        self.fitness_hidden_2 = max(16, min(64, (state_dim + action_dim) // 4))
        
        # ç­–ç•¥åŸºå› ç·¨ç¢¼å™¨ - å‹•æ…‹ç¶­åº¦
        self.gene_encoder = nn.Sequential(
            nn.Linear(state_dim, self.gene_hidden_dim),
            nn.GELU(),
            nn.Linear(self.gene_hidden_dim, self.gene_hidden_dim // 2),
            nn.GELU(),
            nn.Linear(self.gene_hidden_dim // 2, self.gene_latent_dim)
        )
        
        # ç­–ç•¥è§£ç¢¼å™¨ç¶²çµ¡ - å‹•æ…‹ç¶­åº¦
        self.strategy_decoder = nn.ModuleDict({
            f"decoder_{i}": nn.Sequential(
                nn.Linear(self.gene_latent_dim, self.decoder_hidden_1),
                nn.GELU(),
                nn.Linear(self.decoder_hidden_1, self.decoder_hidden_2),
                nn.GELU(),
                nn.Linear(self.decoder_hidden_2, action_dim),
                nn.Tanh()
            ) for i in range(max_generated_strategies)
        })
        
        # ç­–ç•¥é©æ‡‰åº¦è©•ä¼°å™¨ - å‹•æ…‹ç¶­åº¦
        self.fitness_evaluator = nn.Sequential(
            nn.Linear(state_dim + action_dim, self.fitness_hidden_1),
            nn.ReLU(),
            nn.Linear(self.fitness_hidden_1, self.fitness_hidden_2),
            nn.ReLU(),
            nn.Linear(self.fitness_hidden_2, 1),
            nn.Sigmoid()
        )
        
        # åŸºå› çªè®Šæ§åˆ¶å™¨
        self.mutation_controller = nn.Parameter(torch.tensor(0.1))
        
        # ç”Ÿæˆç­–ç•¥çš„æ€§èƒ½æ­·å²
        self.register_buffer('strategy_performance', torch.zeros(max_generated_strategies))
        self.register_buffer('generation_counter', torch.tensor(0))
        
    def encode_market_genes(self, state: torch.Tensor) -> torch.Tensor:
        """å°‡å¸‚å ´ç‹€æ…‹ç·¨ç¢¼ç‚ºç­–ç•¥åŸºå› """
        return self.gene_encoder(state)
    
    def decode_strategy(self, genes: torch.Tensor, strategy_id: int) -> torch.Tensor:
        """å°‡åŸºå› è§£ç¢¼ç‚ºç­–ç•¥è¼¸å‡º"""
        decoder = self.strategy_decoder[f"decoder_{strategy_id}"]
        return decoder(genes)
    
    def mutate_genes(self, genes: torch.Tensor, 
                    mutation_rate: Optional[float] = None) -> torch.Tensor:
        """å°åŸºå› é€²è¡Œçªè®Š"""
        if mutation_rate is None:
            mutation_rate = self.mutation_controller.item()
        
        noise = torch.randn_like(genes) * mutation_rate
        mutated_genes = genes + noise
        return torch.clamp(mutated_genes, -2.0, 2.0)
    
    def evaluate_strategy_fitness(self, state: torch.Tensor, 
                                action: torch.Tensor) -> torch.Tensor:
        """è©•ä¼°ç­–ç•¥çš„é©æ‡‰åº¦"""
        combined_input = torch.cat([state, action], dim=-1)
        return self.fitness_evaluator(combined_input)
    
    def generate_strategies(self, state: torch.Tensor) -> Tuple[List[torch.Tensor], torch.Tensor]:
        """
        ç”Ÿæˆå‹•æ…‹ç­–ç•¥
        
        Returns:
            Tuple[ç­–ç•¥è¼¸å‡ºåˆ—è¡¨, åŸºå› ç·¨ç¢¼]
        """
        batch_size = state.size(0)
        
        # ç·¨ç¢¼å¸‚å ´åŸºå› 
        market_genes = self.encode_market_genes(state)
        
        # ç”Ÿæˆå¤šå€‹ç­–ç•¥è®Šé«”
        generated_strategies = []
        fitness_scores = []
        
        for i in range(self.max_generated_strategies):
            # åŸºå› çªè®Š
            mutated_genes = self.mutate_genes(market_genes)
            
            # è§£ç¢¼ç­–ç•¥
            strategy_output = self.decode_strategy(mutated_genes, i)
            generated_strategies.append(strategy_output)
            
            # è©•ä¼°é©æ‡‰åº¦
            fitness = self.evaluate_strategy_fitness(state, strategy_output)
            fitness_scores.append(fitness)
        
        # é¸æ“‡æœ€ä½³ç­–ç•¥çµ„åˆ
        fitness_tensor = torch.stack(fitness_scores, dim=1)  # [batch, num_strategies]
        
        return generated_strategies, fitness_tensor
    
    def evolve_strategies(self, performance_feedback: torch.Tensor):
        """åŸºæ–¼æ€§èƒ½åé¥‹é€²åŒ–ç­–ç•¥"""
        # æ›´æ–°æ€§èƒ½æ­·å²
        self.strategy_performance = 0.9 * self.strategy_performance + 0.1 * performance_feedback
        
        # èª¿æ•´çªè®Šç‡
        avg_performance = torch.mean(self.strategy_performance)
        if avg_performance < 0.5:
            self.mutation_controller.data *= 1.1  # å¢åŠ æ¢ç´¢
        else:
            self.mutation_controller.data *= 0.95  # æ¸›å°‘æ¢ç´¢
        
        # é™åˆ¶çªè®Šç‡ç¯„åœ
        self.mutation_controller.data = torch.clamp(self.mutation_controller.data, 0.01, 0.3)
        
        self.generation_counter += 1


# ===============================
# å¢å¼·ç‰ˆç­–ç•¥ç–ŠåŠ ç³»çµ±
# ===============================

class EnhancedStrategySuperposition(nn.Module):
    """
    å¢å¼·ç‰ˆç­–ç•¥ç–ŠåŠ ç³»çµ±
    ç®¡ç†15+ç¨®ç­–ç•¥çš„é‡å­ç–ŠåŠ ï¼ŒåŒ…å«å‹•æ…‹ç­–ç•¥ç”Ÿæˆ
    æ”¯æŒå®Œå…¨å‹•æ…‹è‡ªé©æ‡‰ç¶­åº¦é…ç½®
    """
    
    def __init__(self, state_dim: int, action_dim: int, 
                 enable_dynamic_generation: bool = True):
        super().__init__()
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.enable_dynamic_generation = enable_dynamic_generation
        
        # åˆå§‹åŒ–æ‰€æœ‰15ç¨®åŸºç¤ç­–ç•¥
        self.base_strategies = nn.ModuleList([
            # åŸæœ‰ç­–ç•¥
            MomentumStrategy(state_dim, action_dim),
            BreakoutStrategy(state_dim, action_dim),
            StatisticalArbitrageStrategy(state_dim, action_dim),
            # æ–°å¢ç­–ç•¥
            OptionFlowStrategy(state_dim, action_dim),
            MicrostructureStrategy(state_dim, action_dim),
            VolatilityStrategy(state_dim, action_dim),
            CarryTradeStrategy(state_dim, action_dim),
            MacroEconomicStrategy(state_dim, action_dim),
            EventDrivenStrategy(state_dim, action_dim),
            SentimentStrategy(state_dim, action_dim),
            QuantitativeStrategy(state_dim, action_dim),
            PairsTradeStrategy(state_dim, action_dim),
            MarketMakingStrategy(state_dim, action_dim),
            HighFrequencyStrategy(state_dim, action_dim),
            AlgorithmicStrategy(state_dim, action_dim),
        ])
        
        self.num_base_strategies = len(self.base_strategies)
        
        # å‹•æ…‹ç­–ç•¥ç”Ÿæˆå™¨
        if enable_dynamic_generation:
            self.dynamic_generator = DynamicStrategyGenerator(
                state_dim, action_dim, self.base_strategies, max_generated_strategies=5
            )
            self.total_strategies = self.num_base_strategies + 5
        else:
            self.total_strategies = self.num_base_strategies
        
        # å‹•æ…‹è¨ˆç®—æ¬Šé‡ç¶²çµ¡çš„éš±è—å±¤ç¶­åº¦ - è‡ªé©æ‡‰ç¸®æ”¾
        self.vol_hidden_dim = max(16, min(64, self.total_strategies))
        self.regime_hidden_dim = max(32, min(128, state_dim // 4))
        self.corr_hidden_dim = max(32, min(128, (state_dim + 1) // 4))
        
        # é‡å­æŒ¯å¹…åƒæ•¸ï¼ˆå¯å­¸ç¿’ï¼‰
        self.quantum_amplitudes = nn.Parameter(
            torch.ones(self.total_strategies) / math.sqrt(self.total_strategies)
        )
        
        # å¤šå±¤æ¬¡æ¬Šé‡èª¿æ•´ç¶²çµ¡ - å‹•æ…‹ç¶­åº¦é…ç½®
        self.weight_networks = nn.ModuleDict({
            'volatility_net': nn.Sequential(
                nn.Linear(1, self.vol_hidden_dim),
                nn.ReLU(),
                nn.Linear(self.vol_hidden_dim, self.total_strategies),
                nn.Sigmoid()
            ),
            'regime_net': nn.Sequential(
                nn.Linear(state_dim, self.regime_hidden_dim),
                nn.GELU(),
                nn.Linear(self.regime_hidden_dim, self.total_strategies),
                nn.Softmax(dim=-1)
            ),
            'correlation_net': nn.Sequential(
                nn.Linear(state_dim + 1, self.corr_hidden_dim),
                nn.GELU(),
                nn.Linear(self.corr_hidden_dim, self.total_strategies),
                nn.Softmax(dim=-1)
            )
        })
        
        # ç­–ç•¥ç›¸äº’ä½œç”¨çŸ©é™£
        self.interaction_matrix = nn.Parameter(
            torch.eye(self.total_strategies) + torch.randn(self.total_strategies, self.total_strategies) * 0.05
        )
        
        # ç­–ç•¥æ€§èƒ½è¿½è¹¤
        self.register_buffer('strategy_performance_history', 
                           torch.zeros(self.total_strategies, 100))  # è¨˜éŒ„æœ€è¿‘100æ¬¡æ€§èƒ½
        self.register_buffer('performance_index', torch.tensor(0))
        
        # é‡å­ç³¾çºæ•ˆæ‡‰æ¨¡æ“¬
        self.entanglement_strength = nn.Parameter(torch.tensor(0.1))
        
        logger.info(f"ğŸŒŸ åˆå§‹åŒ–å¢å¼·ç‰ˆç­–ç•¥ç–ŠåŠ ç³»çµ±: {self.total_strategies}ç¨®ç­–ç•¥")
        logger.info(f"ğŸ“ å‹•æ…‹ç¶­åº¦é…ç½® - State: {state_dim}, Action: {action_dim}")
        logger.info(f"ğŸ”§ æ¬Šé‡ç¶²çµ¡éš±è—å±¤ - Vol: {self.vol_hidden_dim}, Regime: {self.regime_hidden_dim}, Corr: {self.corr_hidden_dim}")
    
    def get_dynamic_dimensions(self) -> Dict[str, int]:
        """ç²å–ç•¶å‰å‹•æ…‹ç¶­åº¦é…ç½®ä¿¡æ¯"""
        return {
            'state_dim': self.state_dim,
            'action_dim': self.action_dim,
            'total_strategies': self.total_strategies,
            'vol_hidden_dim': self.vol_hidden_dim,
            'regime_hidden_dim': self.regime_hidden_dim,
            'corr_hidden_dim': self.corr_hidden_dim,
            'num_base_strategies': self.num_base_strategies,
            'dynamic_generation_enabled': self.enable_dynamic_generation
        }
    
    def _adaptive_dimension_handler(self, tensor: torch.Tensor, 
                                   expected_dim: int, 
                                   operation_name: str = "unknown") -> torch.Tensor:
        """
        å‹•æ…‹ç¶­åº¦é©é…è™•ç†å™¨
        è‡ªå‹•èª¿æ•´è¼¸å…¥å¼µé‡ä»¥åŒ¹é…æœŸæœ›ç¶­åº¦
        
        Args:
            tensor: è¼¸å…¥å¼µé‡
            expected_dim: æœŸæœ›çš„æœ€å¾Œä¸€å€‹ç¶­åº¦
            operation_name: æ“ä½œåç¨±ï¼Œç”¨æ–¼æ—¥èªŒ
            
        Returns:
            é©é…å¾Œçš„å¼µé‡
        """
        current_shape = tensor.shape
        current_last_dim = current_shape[-1]
        
        if current_last_dim == expected_dim:
            return tensor
        
        batch_dims = current_shape[:-1]
        
        if current_last_dim > expected_dim:
            # ç¶­åº¦éå¤§ï¼šä½¿ç”¨ç·šæ€§æŠ•å½±é™ç¶­
            if not hasattr(self, f'_adaptive_projector_{operation_name}_{current_last_dim}_{expected_dim}'):
                projector = nn.Linear(current_last_dim, expected_dim).to(tensor.device)
                setattr(self, f'_adaptive_projector_{operation_name}_{current_last_dim}_{expected_dim}', projector)
                logger.info(f"ğŸ”§ å‰µå»ºå‹•æ…‹æŠ•å½±å™¨: {operation_name} {current_last_dim}â†’{expected_dim}")
            
            projector = getattr(self, f'_adaptive_projector_{operation_name}_{current_last_dim}_{expected_dim}')
            adapted_tensor = projector(tensor)
            
        elif current_last_dim < expected_dim:
            # ç¶­åº¦éå°ï¼šä½¿ç”¨é›¶å¡«å……æ“´å±•
            pad_size = expected_dim - current_last_dim
            padding = torch.zeros(*batch_dims, pad_size, device=tensor.device, dtype=tensor.dtype)
            adapted_tensor = torch.cat([tensor, padding], dim=-1)
            logger.info(f"ğŸ”§ å‹•æ…‹é›¶å¡«å……: {operation_name} {current_last_dim}â†’{expected_dim}")
        
        return adapted_tensor
    
    def _validate_and_adapt_inputs(self, state: torch.Tensor, 
                                  volatility: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        é©—è­‰ä¸¦é©é…è¼¸å…¥ç¶­åº¦
        
        Args:
            state: å¸‚å ´ç‹€æ…‹å¼µé‡
            volatility: æ³¢å‹•ç‡å¼µé‡
            
        Returns:
            é©é…å¾Œçš„ç‹€æ…‹å’Œæ³¢å‹•ç‡å¼µé‡
        """
        # é©é…ç‹€æ…‹å¼µé‡
        adapted_state = self._adaptive_dimension_handler(
            state, self.state_dim, "state_input"
        )
        
        # ç¢ºä¿æ³¢å‹•ç‡æ˜¯ä¸€ç¶­çš„
        if volatility.dim() > 1 and volatility.shape[-1] != 1:
            volatility = volatility.mean(dim=-1, keepdim=True)
        
        if volatility.dim() == 1:
            volatility = volatility.unsqueeze(-1)
        
        return adapted_state, volatility.squeeze(-1)
    
    def forward(self, state: torch.Tensor, volatility: torch.Tensor, 
                market_regime: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:
        """
        å‰å‘å‚³æ’­ï¼šè¨ˆç®—å¢å¼·ç­–ç•¥ç–ŠåŠ è¼¸å‡º
        æ”¯æŒå‹•æ…‹ç¶­åº¦é©é…
        
        Args:
            state: é‡å­ç·¨ç¢¼çš„å¸‚å ´ç‹€æ…‹
            volatility: å¸‚å ´æ³¢å‹•ç‡
            market_regime: å¸‚å ´ç’°å¢ƒæŒ‡æ¨™
            
        Returns:
            Tuple[ç–ŠåŠ ç­–ç•¥è¼¸å‡º, è©³ç´°ä¿¡æ¯å­—å…¸]
        """
        # å‹•æ…‹ç¶­åº¦é©é…
        state, volatility = self._validate_and_adapt_inputs(state, volatility)
        batch_size = state.size(0)
        
        # åŸ·è¡Œæ‰€æœ‰åŸºç¤ç­–ç•¥
        base_strategy_outputs = []
        for strategy in self.base_strategies:
            try:
                output = strategy(state)
                base_strategy_outputs.append(output)
            except RuntimeError as e:
                if "mat1 and mat2 shapes cannot be multiplied" in str(e):
                    logger.warning(f"âš ï¸ ç­–ç•¥ {strategy.get_strategy_name()} ç¶­åº¦ä¸åŒ¹é…ï¼Œä½¿ç”¨è‡ªé©æ‡‰è™•ç†")
                    # å˜—è©¦è‡ªé©æ‡‰è™•ç†
                    adapted_state = self._adaptive_dimension_handler(state, strategy.strategy_net[0].in_features, f"strategy_{strategy.get_strategy_name()}")
                    output = strategy(adapted_state)
                    base_strategy_outputs.append(output)
                else:
                    raise e
        
        strategy_outputs = base_strategy_outputs.copy()
        
        # å‹•æ…‹ç­–ç•¥ç”Ÿæˆ
        dynamic_fitness = None
        if self.enable_dynamic_generation:
            try:
                dynamic_strategies, dynamic_fitness = self.dynamic_generator.generate_strategies(state)
                strategy_outputs.extend(dynamic_strategies)
            except RuntimeError as e:
                if "mat1 and mat2 shapes cannot be multiplied" in str(e):
                    logger.warning("âš ï¸ å‹•æ…‹ç­–ç•¥ç”Ÿæˆç¶­åº¦ä¸åŒ¹é…ï¼Œä½¿ç”¨è‡ªé©æ‡‰è™•ç†")
                    adapted_state = self._adaptive_dimension_handler(state, self.dynamic_generator.state_dim, "dynamic_generator")
                    dynamic_strategies, dynamic_fitness = self.dynamic_generator.generate_strategies(adapted_state)
                    strategy_outputs.extend(dynamic_strategies)
                else:
                    raise e
        
        # å †ç–Šæ‰€æœ‰ç­–ç•¥è¼¸å‡º
        strategy_tensor = torch.stack(strategy_outputs, dim=1)  # [batch, num_strategies, action_dim]
        
        # è¨ˆç®—å¤šå±¤æ¬¡æ¬Šé‡ - ä½¿ç”¨å‹•æ…‹é©é…
        vol_weights = self.weight_networks['volatility_net'](volatility.unsqueeze(-1))
        regime_weights = self.weight_networks['regime_net'](state)
        corr_input = torch.cat([state, volatility.unsqueeze(-1)], dim=-1)
        corr_weights = self.weight_networks['correlation_net'](corr_input)
        
        # é‡å­æŒ¯å¹…æ­£è¦åŒ–
        quantum_amps = F.softmax(self.quantum_amplitudes, dim=0)
        
        # çµ„åˆæ¬Šé‡ï¼ˆè€ƒæ…®é‡å­æ•ˆæ‡‰ï¼‰
        combined_weights = quantum_amps.unsqueeze(0) * vol_weights * regime_weights * corr_weights
        combined_weights = F.softmax(combined_weights, dim=-1)
        
        # ç­–ç•¥ç›¸äº’ä½œç”¨æ•ˆæ‡‰
        interaction_effect = torch.matmul(combined_weights.unsqueeze(1), self.interaction_matrix).squeeze(1)
        final_weights = combined_weights + self.entanglement_strength * interaction_effect
        final_weights = F.softmax(final_weights, dim=-1)
        
        # åŠ æ¬Šç–ŠåŠ ç­–ç•¥è¼¸å‡º
        superposition_output = torch.sum(
            strategy_tensor * final_weights.unsqueeze(-1), dim=1
        )
        
        # æ§‹å»ºè©³ç´°ä¿¡æ¯
        info_dict = {
            'strategy_weights': final_weights,
            'quantum_amplitudes': quantum_amps,
            'vol_weights': vol_weights,
            'regime_weights': regime_weights,
            'corr_weights': corr_weights,
            'num_active_strategies': torch.sum(final_weights > 0.01, dim=-1).float(),
            'dimensions_info': self.get_dynamic_dimensions()
        }
        
        if dynamic_fitness is not None:
            info_dict['dynamic_fitness'] = dynamic_fitness
        
        return superposition_output, info_dict
    
    def update_strategy_performance(self, performance_scores: torch.Tensor):
        """æ›´æ–°ç­–ç•¥æ€§èƒ½æ­·å²"""
        current_idx = self.performance_index.item() % 100
        self.strategy_performance_history[:, current_idx] = performance_scores.mean(dim=0)
        self.performance_index += 1
        
        # æ›´æ–°å‹•æ…‹ç­–ç•¥ç”Ÿæˆå™¨
        if self.enable_dynamic_generation:
            dynamic_performance = performance_scores[:, self.num_base_strategies:]
            if dynamic_performance.numel() > 0:
                self.dynamic_generator.evolve_strategies(dynamic_performance.mean(dim=0))
    
    def get_strategy_analysis(self) -> Dict[str, Any]:
        """ç²å–ç­–ç•¥åˆ†æä¿¡æ¯"""
        recent_performance = self.strategy_performance_history[:, :min(100, self.performance_index.item())]
        
        return {
            'num_strategies': len(self.base_strategies) + (5 if self.enable_dynamic_generation else 0),
            'avg_performance': recent_performance.mean(dim=-1),
            'performance_std': recent_performance.std(dim=-1),
            'best_strategy_idx': recent_performance.mean(dim=-1).argmax().item(),
            'worst_strategy_idx': recent_performance.mean(dim=-1).argmin().item(),
            'strategy_names': [s.get_strategy_name() for s in self.base_strategies] + 
                            ([f"Dynamic_{i}" for i in range(5)] if self.enable_dynamic_generation else []),
            'quantum_amplitude_distribution': F.softmax(self.quantum_amplitudes, dim=0),
            'entanglement_strength': self.entanglement_strength.item(),
        }


if __name__ == "__main__":
    # æ¸¬è©¦å¢å¼·ç‰ˆé‡å­ç­–ç•¥å±¤
    logger.info("é–‹å§‹æ¸¬è©¦å¢å¼·ç‰ˆé‡å­ç­–ç•¥å±¤...")
    
    # æ¸¬è©¦åƒæ•¸
    batch_size = 8
    state_dim = 64
    action_dim = 10
    
    # å‰µå»ºæ¸¬è©¦æ•¸æ“š
    test_state = torch.randn(batch_size, state_dim)
    test_volatility = torch.rand(batch_size) * 0.5
    
    # åˆå§‹åŒ–å¢å¼·ç‰ˆç­–ç•¥ç–ŠåŠ ç³»çµ±
    enhanced_strategy_layer = EnhancedStrategySuperposition(
        state_dim=state_dim,
        action_dim=action_dim,
        enable_dynamic_generation=True
    )
    
    try:
        # å‰å‘å‚³æ’­æ¸¬è©¦
        with torch.no_grad():
            output, info = enhanced_strategy_layer(test_state, test_volatility)
            
        logger.info(f"æ¸¬è©¦æˆåŠŸï¼")
        logger.info(f"è¼¸å…¥ç‹€æ…‹å½¢ç‹€: {test_state.shape}")
        logger.info(f"è¼¸å‡ºå‹•ä½œå½¢ç‹€: {output.shape}")
        logger.info(f"ç­–ç•¥æ¬Šé‡å½¢ç‹€: {info['strategy_weights'].shape}")
        logger.info(f"æ´»èºç­–ç•¥æ•¸é‡: {info['num_active_strategies'].mean():.2f}")
        
        # ç²å–ç­–ç•¥åˆ†æ
        analysis = enhanced_strategy_layer.get_strategy_analysis()
        logger.info(f"ç­–ç•¥ç¸½æ•¸: {analysis['num_strategies']}")
        logger.info(f"ç­–ç•¥åç¨±: {analysis['strategy_names']}")
        
        # æ¸¬è©¦æ¢¯åº¦è¨ˆç®—
        enhanced_strategy_layer.train()
        output, info = enhanced_strategy_layer(test_state, test_volatility)
        loss = output.abs().mean()
        loss.backward()
        
        logger.info("æ¢¯åº¦è¨ˆç®—æ¸¬è©¦é€šé")
        logger.info(f"ç¸½åƒæ•¸é‡: {sum(p.numel() for p in enhanced_strategy_layer.parameters()):,}")
        
    except Exception as e:
        logger.error(f"æ¸¬è©¦å¤±æ•—: {e}")
        raise e
