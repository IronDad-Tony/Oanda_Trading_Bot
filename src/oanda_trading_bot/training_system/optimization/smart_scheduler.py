# src/optimization/smart_scheduler.py
"""
æ™ºèƒ½è¨“ç·´èª¿åº¦å™¨
åŸºæ–¼ç³»çµ±æ€§èƒ½å’Œè¨“ç·´é€²åº¦å‹•æ…‹èª¿æ•´å­¸ç¿’åƒæ•¸

ä¸»è¦åŠŸèƒ½ï¼š
1. è‡ªé©æ‡‰å­¸ç¿’ç‡èª¿åº¦
2. å‹•æ…‹æ‰¹æ¬¡å¤§å°èª¿æ•´
3. æ—©åœæ©Ÿåˆ¶å„ªåŒ–
4. è¨“ç·´é€²åº¦ç›£æ§
5. è³‡æºä½¿ç”¨å„ªåŒ–
"""

import torch
import torch.nn as nn
import numpy as np
from typing import Dict, Any, List, Optional, Callable
import logging
from datetime import datetime, timedelta
import json
from pathlib import Path

try:
    from oanda_trading_bot.training_system.common.logger_setup import logger
    from oanda_trading_bot.training_system.optimization.performance_optimizer import PerformanceOptimizer
except ImportError:
    logger = logging.getLogger(__name__)


class SmartTrainingScheduler:
    """æ™ºèƒ½è¨“ç·´èª¿åº¦å™¨"""
    
    def __init__(self, 
                 model: nn.Module,
                 initial_lr: float = 0.0001,
                 patience_factor: float = 1.5,
                 performance_threshold: float = 0.02):
        self.model = model
        self.initial_lr = initial_lr
        self.current_lr = initial_lr
        self.patience_factor = patience_factor
        self.performance_threshold = performance_threshold
        
        # è¨“ç·´æ­·å²
        self.training_history = {
            "losses": [],
            "rewards": [],
            "learning_rates": [],
            "batch_sizes": [],
            "timestamps": [],
            "gpu_utilization": [],
            "training_speed": []
        }
        
        # æ—©åœåƒæ•¸
        self.best_performance = float('-inf')
        self.patience_counter = 0
        self.base_patience = 50
        
        # è‡ªé©æ‡‰åƒæ•¸
        self.lr_reduction_factor = 0.7
        self.lr_increase_factor = 1.1
        self.min_lr = 1e-6
        self.max_lr = 0.01
        
        logger.info("æ™ºèƒ½è¨“ç·´èª¿åº¦å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def update(self, 
               current_loss: float,
               current_reward: float,
               training_time: float,
               gpu_utilization: float = None) -> Dict[str, Any]:
        """æ›´æ–°èª¿åº¦å™¨ç‹€æ…‹ä¸¦è¿”å›èª¿æ•´å»ºè­°"""
        
        timestamp = datetime.now()
        
        # è¨˜éŒ„è¨“ç·´æ­·å²
        self.training_history["losses"].append(current_loss)
        self.training_history["rewards"].append(current_reward)
        self.training_history["learning_rates"].append(self.current_lr)
        self.training_history["timestamps"].append(timestamp)
        self.training_history["training_speed"].append(training_time)
        
        if gpu_utilization is not None:
            self.training_history["gpu_utilization"].append(gpu_utilization)
        
        # è¨ˆç®—æ€§èƒ½æŒ‡æ¨™
        performance_metrics = self._calculate_performance_metrics()
        
        # å­¸ç¿’ç‡èª¿æ•´
        lr_adjustment = self._adjust_learning_rate(performance_metrics)
        
        # æ—©åœæª¢æŸ¥
        early_stop_decision = self._check_early_stopping(performance_metrics)
        
        # ç”Ÿæˆèª¿æ•´å»ºè­°
        recommendations = self._generate_recommendations(performance_metrics)
        
        return {
            "performance_metrics": performance_metrics,
            "lr_adjustment": lr_adjustment,
            "early_stop": early_stop_decision,
            "recommendations": recommendations,
            "should_save_checkpoint": self._should_save_checkpoint(performance_metrics)
        }
    
    def _calculate_performance_metrics(self) -> Dict[str, float]:
        """è¨ˆç®—æ€§èƒ½æŒ‡æ¨™"""
        if len(self.training_history["losses"]) < 2:
            return {"improvement_rate": 0.0, "stability": 0.0, "trend": 0.0}
        
        recent_window = min(10, len(self.training_history["losses"]))
        recent_losses = self.training_history["losses"][-recent_window:]
        recent_rewards = self.training_history["rewards"][-recent_window:]
        
        # æ”¹å–„ç‡è¨ˆç®—
        if len(recent_losses) >= 5:
            early_avg = np.mean(recent_losses[:len(recent_losses)//2])
            late_avg = np.mean(recent_losses[len(recent_losses)//2:])
            improvement_rate = (early_avg - late_avg) / max(early_avg, 1e-8)
        else:
            improvement_rate = 0.0
        
        # ç©©å®šæ€§è¨ˆç®—ï¼ˆæå¤±æ–¹å·®ï¼‰
        stability = 1.0 / (1.0 + np.var(recent_losses))
        
        # è¶¨å‹¢è¨ˆç®—
        if len(recent_rewards) >= 3:
            trend = np.polyfit(range(len(recent_rewards)), recent_rewards, 1)[0]
        else:
            trend = 0.0
        
        # è¨“ç·´é€Ÿåº¦
        if len(self.training_history["training_speed"]) >= 5:
            avg_speed = np.mean(self.training_history["training_speed"][-5:])
            speed_stability = 1.0 / (1.0 + np.var(self.training_history["training_speed"][-5:]))
        else:
            avg_speed = 0.0
            speed_stability = 1.0
        
        return {
            "improvement_rate": improvement_rate,
            "stability": stability,
            "trend": trend,
            "avg_training_speed": avg_speed,
            "speed_stability": speed_stability,
            "current_performance": recent_rewards[-1] if recent_rewards else 0.0
        }
    
    def _adjust_learning_rate(self, metrics: Dict[str, float]) -> Dict[str, Any]:
        """èª¿æ•´å­¸ç¿’ç‡"""
        improvement_rate = metrics["improvement_rate"]
        stability = metrics["stability"]
        trend = metrics["trend"]
        
        old_lr = self.current_lr
        adjustment_factor = 1.0
        reason = "ä¿æŒä¸è®Š"
        
        # å­¸ç¿’ç‡èª¿æ•´é‚è¼¯
        if improvement_rate < -0.05:  # æ€§èƒ½é€€åŒ–
            adjustment_factor = self.lr_reduction_factor
            reason = "æ€§èƒ½é€€åŒ–ï¼Œé™ä½å­¸ç¿’ç‡"
        elif improvement_rate > 0.02 and stability > 0.8:  # ç©©å®šæ”¹å–„
            adjustment_factor = self.lr_increase_factor
            reason = "ç©©å®šæ”¹å–„ï¼Œé©åº¦æé«˜å­¸ç¿’ç‡"
        elif improvement_rate < 0.005 and stability > 0.9:  # æ”¶æ–‚ç·©æ…¢
            adjustment_factor = self.lr_reduction_factor
            reason = "æ”¶æ–‚ç·©æ…¢ï¼Œé™ä½å­¸ç¿’ç‡ç²¾ç´°èª¿æ•´"
        elif trend < -0.1:  # çå‹µä¸‹é™è¶¨å‹¢
            adjustment_factor = self.lr_reduction_factor * 0.8
            reason = "çå‹µä¸‹é™è¶¨å‹¢ï¼Œå¤§å¹…é™ä½å­¸ç¿’ç‡"
        
        # æ‡‰ç”¨èª¿æ•´
        self.current_lr = np.clip(
            self.current_lr * adjustment_factor,
            self.min_lr,
            self.max_lr
        )
        
        return {
            "old_lr": old_lr,
            "new_lr": self.current_lr,
            "adjustment_factor": adjustment_factor,
            "reason": reason
        }
    
    def _check_early_stopping(self, metrics: Dict[str, float]) -> Dict[str, Any]:
        """æª¢æŸ¥æ˜¯å¦æ‡‰è©²æ—©åœ"""
        current_performance = metrics["current_performance"]
        
        if current_performance > self.best_performance + self.performance_threshold:
            self.best_performance = current_performance
            self.patience_counter = 0
            should_stop = False
            reason = "æ€§èƒ½æå‡ï¼Œé‡ç½®è¨ˆæ•¸å™¨"
        else:
            self.patience_counter += 1
            
            # å‹•æ…‹èª¿æ•´è€å¿ƒå€¼
            dynamic_patience = int(self.base_patience * self.patience_factor)
            
            if self.patience_counter >= dynamic_patience:
                should_stop = True
                reason = f"è¶…éè€å¿ƒå€¼ {dynamic_patience}ï¼Œå»ºè­°æ—©åœ"
            else:
                should_stop = False
                reason = f"ç­‰å¾…ä¸­ ({self.patience_counter}/{dynamic_patience})"
        
        return {
            "should_stop": should_stop,
            "patience_counter": self.patience_counter,
            "best_performance": self.best_performance,
            "reason": reason
        }
    
    def _generate_recommendations(self, metrics: Dict[str, float]) -> List[str]:
        """ç”Ÿæˆè¨“ç·´å»ºè­°"""
        recommendations = []
        
        # æ€§èƒ½ç›¸é—œå»ºè­°
        if metrics["improvement_rate"] < -0.1:
            recommendations.append("âš ï¸ æ€§èƒ½åš´é‡é€€åŒ–ï¼Œè€ƒæ…®å›é€€åˆ°ä¸Šä¸€å€‹æª¢æŸ¥é»")
        elif metrics["improvement_rate"] < 0.001:
            recommendations.append("ğŸ“ˆ æ”¶æ–‚ç·©æ…¢ï¼Œå¯ä»¥è€ƒæ…®èª¿æ•´æ¨¡å‹æ¶æ§‹æˆ–å¢åŠ æ­£å‰‡åŒ–")
        
        # ç©©å®šæ€§ç›¸é—œå»ºè­°
        if metrics["stability"] < 0.5:
            recommendations.append("ğŸŒŠ è¨“ç·´ä¸ç©©å®šï¼Œå»ºè­°é™ä½å­¸ç¿’ç‡æˆ–å¢åŠ æ‰¹æ¬¡å¤§å°")
        elif metrics["stability"] > 0.95:
            recommendations.append("âœ… è¨“ç·´éå¸¸ç©©å®šï¼Œå¯ä»¥è€ƒæ…®é©åº¦æé«˜å­¸ç¿’ç‡")
        
        # è¶¨å‹¢ç›¸é—œå»ºè­°
        if metrics["trend"] > 0.1:
            recommendations.append("ğŸš€ çå‹µä¸Šå‡è¶¨å‹¢è‰¯å¥½ï¼Œä¿æŒç•¶å‰ç­–ç•¥")
        elif metrics["trend"] < -0.1:
            recommendations.append("ğŸ“‰ çå‹µä¸‹é™è¶¨å‹¢ï¼Œéœ€è¦èª¿æ•´ç­–ç•¥")
        
        # è¨“ç·´é€Ÿåº¦ç›¸é—œå»ºè­°
        if metrics.get("speed_stability", 1.0) < 0.7:
            recommendations.append("â±ï¸ è¨“ç·´é€Ÿåº¦ä¸ç©©å®šï¼Œæª¢æŸ¥GPUä½¿ç”¨ç‡å’Œè¨˜æ†¶é«”")
        
        return recommendations
    
    def _should_save_checkpoint(self, metrics: Dict[str, float]) -> bool:
        """åˆ¤æ–·æ˜¯å¦æ‡‰è©²ä¿å­˜æª¢æŸ¥é»"""
        return (
            metrics["current_performance"] > self.best_performance or
            metrics["improvement_rate"] > 0.05 or
            len(self.training_history["losses"]) % 100 == 0
        )
    
    def get_training_summary(self) -> Dict[str, Any]:
        """ç²å–è¨“ç·´æ‘˜è¦"""
        if not self.training_history["losses"]:
            return {"status": "no_data"}
        
        total_steps = len(self.training_history["losses"])
        training_duration = (
            self.training_history["timestamps"][-1] - 
            self.training_history["timestamps"][0]
        ).total_seconds() / 3600  # å°æ™‚
        
        return {
            "total_steps": total_steps,
            "training_duration_hours": training_duration,
            "best_performance": self.best_performance,
            "current_lr": self.current_lr,
            "avg_loss": np.mean(self.training_history["losses"][-100:]),
            "avg_reward": np.mean(self.training_history["rewards"][-100:]),
            "improvement_trend": self._calculate_overall_trend(),
            "efficiency_score": self._calculate_efficiency_score()
        }
    
    def _calculate_overall_trend(self) -> str:
        """è¨ˆç®—æ•´é«”è¶¨å‹¢"""
        if len(self.training_history["rewards"]) < 10:
            return "insufficient_data"
        
        recent_rewards = self.training_history["rewards"][-50:]
        early_avg = np.mean(recent_rewards[:len(recent_rewards)//2])
        late_avg = np.mean(recent_rewards[len(recent_rewards)//2:])
        
        improvement = (late_avg - early_avg) / max(abs(early_avg), 1e-8)
        
        if improvement > 0.1:
            return "strong_improvement"
        elif improvement > 0.02:
            return "moderate_improvement"
        elif improvement > -0.02:
            return "stable"
        elif improvement > -0.1:
            return "slight_decline"
        else:
            return "significant_decline"
    
    def _calculate_efficiency_score(self) -> float:
        """è¨ˆç®—è¨“ç·´æ•ˆç‡è©•åˆ†"""
        if len(self.training_history["losses"]) < 10:
            return 0.0
        
        # è€ƒæ…®æ”¶æ–‚é€Ÿåº¦ã€ç©©å®šæ€§å’Œè³‡æºåˆ©ç”¨ç‡
        improvement_rate = self._calculate_performance_metrics()["improvement_rate"]
        stability = self._calculate_performance_metrics()["stability"]
        
        # GPUåˆ©ç”¨ç‡
        if self.training_history["gpu_utilization"]:
            avg_gpu_util = np.mean(self.training_history["gpu_utilization"][-20:])
        else:
            avg_gpu_util = 0.5  # å‡è¨­ä¸­ç­‰åˆ©ç”¨ç‡
        
        # æ•ˆç‡è©•åˆ†ï¼ˆ0-1ï¼‰
        efficiency = (
            0.4 * min(1.0, max(0.0, improvement_rate * 10)) +  # æ”¹å–„ç‡
            0.3 * stability +  # ç©©å®šæ€§
            0.3 * avg_gpu_util  # GPUåˆ©ç”¨ç‡
        )
        
        return efficiency
    
    def save_training_history(self, filepath: str):
        """ä¿å­˜è¨“ç·´æ­·å²"""
        # è½‰æ›datetimeç‚ºå­—ç¬¦ä¸²
        history_to_save = self.training_history.copy()
        history_to_save["timestamps"] = [
            ts.isoformat() for ts in history_to_save["timestamps"]
        ]
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(history_to_save, f, indent=2, ensure_ascii=False)
        
        logger.info(f"è¨“ç·´æ­·å²å·²ä¿å­˜è‡³: {filepath}")


def create_smart_scheduler(model: nn.Module, 
                         config: Dict[str, Any] = None) -> SmartTrainingScheduler:
    """å‰µå»ºæ™ºèƒ½èª¿åº¦å™¨çš„ä¾¿æ·å‡½æ•¸"""
    default_config = {
        "initial_lr": 0.0001,
        "patience_factor": 1.5,
        "performance_threshold": 0.02
    }
    
    if config:
        default_config.update(config)
    
    return SmartTrainingScheduler(model, **default_config)


if __name__ == "__main__":
    # æ¸¬è©¦æ™ºèƒ½èª¿åº¦å™¨
    class DummyModel(nn.Module):
        def forward(self, x):
            return x
    
    model = DummyModel()
    scheduler = create_smart_scheduler(model)
    
    # æ¨¡æ“¬è¨“ç·´éç¨‹
    print("ğŸ§  æ™ºèƒ½è¨“ç·´èª¿åº¦å™¨æ¸¬è©¦")
    print("="*50)
    
    for step in range(20):
        # æ¨¡æ“¬è¨“ç·´æ•¸æ“š
        loss = 1.0 - step * 0.03 + np.random.normal(0, 0.05)
        reward = step * 0.1 + np.random.normal(0, 0.02)
        training_time = 0.5 + np.random.normal(0, 0.1)
        gpu_util = 0.8 + np.random.normal(0, 0.1)
        
        result = scheduler.update(loss, reward, training_time, gpu_util)
        
        if step % 5 == 0:
            print(f"\næ­¥é©Ÿ {step+1}:")
            print(f"  ç•¶å‰å­¸ç¿’ç‡: {result['lr_adjustment']['new_lr']:.6f}")
            print(f"  æ€§èƒ½æ”¹å–„ç‡: {result['performance_metrics']['improvement_rate']:.4f}")
            print(f"  èª¿æ•´åŸå› : {result['lr_adjustment']['reason']}")
            if result["recommendations"]:
                print(f"  å»ºè­°: {result['recommendations'][0]}")
    
    summary = scheduler.get_training_summary()
    print(f"\nğŸ“Š è¨“ç·´æ‘˜è¦:")
    print(f"  ç¸½æ­¥æ•¸: {summary['total_steps']}")
    print(f"  æœ€ä½³æ€§èƒ½: {summary['best_performance']:.4f}")
    print(f"  ç•¶å‰å­¸ç¿’ç‡: {summary['current_lr']:.6f}")
    print(f"  æ•´é«”è¶¨å‹¢: {summary['improvement_trend']}")
    print(f"  æ•ˆç‡è©•åˆ†: {summary['efficiency_score']:.3f}")
