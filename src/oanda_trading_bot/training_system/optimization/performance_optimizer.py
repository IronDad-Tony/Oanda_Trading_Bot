# src/optimization/performance_optimizer.py
"""
ç³»çµ±æ€§èƒ½å„ªåŒ–å™¨
åŸºæ–¼RTX 4060 Ti 16GB GPUå„ªåŒ–è¨“ç·´æ€§èƒ½

ä¸»è¦åŠŸèƒ½ï¼š
1. GPUè¨˜æ†¶é«”å„ªåŒ–
2. æ‰¹æ¬¡å¤§å°å‹•æ…‹èª¿æ•´
3. æ··åˆç²¾åº¦è¨“ç·´å„ªåŒ–
4. æ¢¯åº¦ç´¯ç©ç­–ç•¥
5. æ¨¡å‹ä¸¦è¡ŒåŒ–
"""

import torch
import torch.nn as nn
import numpy as np
from typing import Dict, Any, Optional, Tuple
import logging
from pathlib import Path
import psutil
import gc

try:
    from oanda_trading_bot.training_system.common.logger_setup import logger
    from oanda_trading_bot.training_system.common.config import DEVICE, USE_AMP
except ImportError:
    logger = logging.getLogger(__name__)
    DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
    USE_AMP = True


class PerformanceOptimizer:
    """æ€§èƒ½å„ªåŒ–å™¨"""
    
    def __init__(self, model: nn.Module, target_gpu_utilization: float = 0.85):
        self.model = model
        self.target_gpu_utilization = target_gpu_utilization
        self.device = DEVICE
        self.gpu_info = self._get_gpu_info()
        self.optimal_batch_size = None
        self.optimal_gradient_accumulation = 1
        
    def _get_gpu_info(self) -> Dict[str, Any]:
        """ç²å–GPUä¿¡æ¯"""
        if not torch.cuda.is_available():
            return {"available": False}
            
        gpu_info = {
            "available": True,
            "name": torch.cuda.get_device_name(0),
            "total_memory": torch.cuda.get_device_properties(0).total_memory,
            "total_memory_gb": torch.cuda.get_device_properties(0).total_memory / (1024**3),
            "compute_capability": torch.cuda.get_device_properties(0).major
        }
        
        logger.info(f"GPUä¿¡æ¯: {gpu_info['name']} - {gpu_info['total_memory_gb']:.1f}GB")
        return gpu_info
    
    def optimize_batch_size(self, input_shape: Tuple[int, ...], 
                          min_batch_size: int = 1, 
                          max_batch_size: int = 256) -> int:
        """å‹•æ…‹å„ªåŒ–æ‰¹æ¬¡å¤§å°"""
        logger.info("é–‹å§‹æ‰¹æ¬¡å¤§å°å„ªåŒ–...")
        
        if not self.gpu_info["available"]:
            logger.warning("GPUä¸å¯ç”¨ï¼Œä½¿ç”¨é»˜èªæ‰¹æ¬¡å¤§å°")
            return 16
        
        # å¾æœ€å¤§å€¼é–‹å§‹äºŒåˆ†æœç´¢
        left, right = min_batch_size, max_batch_size
        optimal_size = min_batch_size
        
        while left <= right:
            mid = (left + right) // 2
            
            try:
                # æ¸¬è©¦æ‰¹æ¬¡å¤§å°
                if self._test_batch_size(input_shape, mid):
                    optimal_size = mid
                    left = mid + 1
                else:
                    right = mid - 1
                    
            except Exception as e:
                logger.warning(f"æ¸¬è©¦æ‰¹æ¬¡å¤§å° {mid} å¤±æ•—: {e}")
                right = mid - 1
        
        # ç•™å‡ºå®‰å…¨é¤˜é‡
        safe_batch_size = max(1, int(optimal_size * 0.8))
        
        logger.info(f"å„ªåŒ–å®Œæˆ - å»ºè­°æ‰¹æ¬¡å¤§å°: {safe_batch_size}")
        self.optimal_batch_size = safe_batch_size
        return safe_batch_size
    
    def _test_batch_size(self, input_shape: Tuple[int, ...], batch_size: int) -> bool:
        """æ¸¬è©¦ç‰¹å®šæ‰¹æ¬¡å¤§å°æ˜¯å¦å¯è¡Œ"""
        try:
            # æ¸…ç†GPUè¨˜æ†¶é«”
            torch.cuda.empty_cache()
            gc.collect()
            
            # å‰µå»ºæ¸¬è©¦æ•¸æ“š
            test_input = torch.randn(batch_size, *input_shape[1:]).to(self.device)
            
            # å‰å‘å‚³æ’­æ¸¬è©¦
            self.model.eval()
            with torch.no_grad():
                _ = self.model(test_input)
            
            # æª¢æŸ¥è¨˜æ†¶é«”ä½¿ç”¨ç‡
            memory_used = torch.cuda.memory_allocated() / self.gpu_info["total_memory"]
            
            # æ¸…ç†æ¸¬è©¦æ•¸æ“š
            del test_input
            torch.cuda.empty_cache()
            
            return memory_used < self.target_gpu_utilization
            
        except Exception:
            return False
    
    def setup_mixed_precision(self) -> Tuple[torch.cuda.amp.GradScaler, bool]:
        """è¨­ç½®æ··åˆç²¾åº¦è¨“ç·´"""
        if not self.gpu_info["available"] or self.gpu_info["compute_capability"] < 7:
            logger.warning("GPUä¸æ”¯æŒæ··åˆç²¾åº¦è¨“ç·´")
            return None, False
        
        scaler = torch.cuda.amp.GradScaler()
        logger.info("âœ“ æ··åˆç²¾åº¦è¨“ç·´å·²å•Ÿç”¨")
        return scaler, True
    
    def optimize_gradient_accumulation(self, target_effective_batch_size: int) -> int:
        """å„ªåŒ–æ¢¯åº¦ç´¯ç©æ­¥æ•¸"""
        if self.optimal_batch_size is None:
            logger.warning("è«‹å…ˆé‹è¡Œæ‰¹æ¬¡å¤§å°å„ªåŒ–")
            return 1
        
        gradient_accumulation = max(1, target_effective_batch_size // self.optimal_batch_size)
        self.optimal_gradient_accumulation = gradient_accumulation
        
        effective_batch_size = self.optimal_batch_size * gradient_accumulation
        logger.info(f"æ¢¯åº¦ç´¯ç©å„ªåŒ–: {gradient_accumulation}æ­¥, æœ‰æ•ˆæ‰¹æ¬¡å¤§å°: {effective_batch_size}")
        
        return gradient_accumulation
    
    def enable_torch_optimizations(self):
        """å•Ÿç”¨PyTorchå„ªåŒ–"""
        # å•Ÿç”¨cudnn benchmark
        torch.backends.cudnn.benchmark = True
        
        # è¨­ç½®è¨˜æ†¶é«”ç¢ç‰‡æ•´ç†
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        # è¨­ç½®ç·šç¨‹æ•¸
        num_threads = min(8, psutil.cpu_count())
        torch.set_num_threads(num_threads)
        
        logger.info(f"âœ“ PyTorchå„ªåŒ–å·²å•Ÿç”¨ - ç·šç¨‹æ•¸: {num_threads}")
    
    def get_optimization_summary(self) -> Dict[str, Any]:
        """ç²å–å„ªåŒ–æ‘˜è¦"""
        return {
            "gpu_info": self.gpu_info,
            "optimal_batch_size": self.optimal_batch_size,
            "gradient_accumulation": self.optimal_gradient_accumulation,
            "mixed_precision_available": self.gpu_info.get("compute_capability", 0) >= 7,
            "recommendations": self._get_recommendations()
        }
    
    def _get_recommendations(self) -> Dict[str, str]:
        """ç²å–å„ªåŒ–å»ºè­°"""
        recommendations = {}
        
        if self.gpu_info["available"]:
            if self.gpu_info["total_memory_gb"] >= 12:
                recommendations["model_size"] = "å¯ä»¥ä½¿ç”¨å¤§å‹æ¨¡å‹é…ç½®"
            else:
                recommendations["model_size"] = "å»ºè­°ä½¿ç”¨ä¸­å‹æ¨¡å‹é…ç½®"
                
            if self.gpu_info["compute_capability"] >= 7:
                recommendations["precision"] = "å»ºè­°ä½¿ç”¨æ··åˆç²¾åº¦è¨“ç·´"
            else:
                recommendations["precision"] = "ä½¿ç”¨FP32ç²¾åº¦"
        else:
            recommendations["fallback"] = "GPUä¸å¯ç”¨ï¼Œå°‡ä½¿ç”¨CPUè¨“ç·´"
        
        return recommendations


def optimize_training_config(model: nn.Module, 
                           input_shape: Tuple[int, ...],
                           target_batch_size: int = 128) -> Dict[str, Any]:
    """å„ªåŒ–è¨“ç·´é…ç½®çš„ä¾¿æ·å‡½æ•¸"""
    optimizer = PerformanceOptimizer(model)
    
    # å•Ÿç”¨åŸºç¤å„ªåŒ–
    optimizer.enable_torch_optimizations()
    
    # å„ªåŒ–æ‰¹æ¬¡å¤§å°
    optimal_batch_size = optimizer.optimize_batch_size(input_shape)
    
    # å„ªåŒ–æ¢¯åº¦ç´¯ç©
    gradient_accumulation = optimizer.optimize_gradient_accumulation(target_batch_size)
    
    # è¨­ç½®æ··åˆç²¾åº¦
    scaler, use_amp = optimizer.setup_mixed_precision()
    
    config = {
        "batch_size": optimal_batch_size,
        "gradient_accumulation_steps": gradient_accumulation,
        "effective_batch_size": optimal_batch_size * gradient_accumulation,
        "use_amp": use_amp,
        "scaler": scaler,
        "summary": optimizer.get_optimization_summary()
    }
    
    return config


if __name__ == "__main__":
    # æ¸¬è©¦æ€§èƒ½å„ªåŒ–å™¨
    class DummyModel(nn.Module):
        def __init__(self):
            super().__init__()
            self.layers = nn.Sequential(
                nn.Linear(768, 1024),
                nn.ReLU(),
                nn.Linear(1024, 512),
                nn.ReLU(),
                nn.Linear(512, 256)
            )
        
        def forward(self, x):
            return self.layers(x)
    
    model = DummyModel().to(DEVICE)
    input_shape = (32, 768)  # (batch_size, features)
    
    config = optimize_training_config(model, input_shape)
    
    print("\n" + "="*50)
    print("ğŸš€ æ€§èƒ½å„ªåŒ–çµæœ")
    print("="*50)
    print(f"å»ºè­°æ‰¹æ¬¡å¤§å°: {config['batch_size']}")
    print(f"æ¢¯åº¦ç´¯ç©æ­¥æ•¸: {config['gradient_accumulation_steps']}")
    print(f"æœ‰æ•ˆæ‰¹æ¬¡å¤§å°: {config['effective_batch_size']}")
    print(f"æ··åˆç²¾åº¦è¨“ç·´: {config['use_amp']}")
    print("="*50)
