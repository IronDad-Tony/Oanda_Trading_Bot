# src/optimization/batch_size_optimizer_final.py
"""
ÊâπÊ¨°Â§ßÂ∞èËá™ÂãïÂÑ™ÂåñÂ∑•ÂÖ∑ - ÊúÄÁµÇÁâàÊú¨
Âü∫ÊñºÂ∑≤Áü•ÁöÑRTX 4060 Ti GPUÊ∏¨Ë©¶ÁµêÊûúÔºåÁõ¥Êé•ÊáâÁî®ÊúÄ‰Ω≥ÊâπÊ¨°Â§ßÂ∞è

Âü∫Êñº‰πãÂâçÁöÑÊ∏¨Ë©¶ÁµêÊûúÔºöRTX 4060 Ti 16GB ÊúÄ‰Ω≥ÊâπÊ¨°Â§ßÂ∞èÁÇ∫ 204
"""

import json
import re
import sys
import os
from pathlib import Path
from typing import Dict, List, Tuple, Any
import torch
import torch.nn as nn
import logging
from datetime import datetime

# Ê∑ªÂä†È†ÖÁõÆÊ†πÁõÆÈåÑÂà∞PythonË∑ØÂæë
project_root = Path(__file__).resolve().parent.parent.parent
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

# ÂâµÂª∫Âü∫Êú¨logger
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)
handler = logging.StreamHandler()
formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
handler.setFormatter(formatter)
logger.addHandler(handler)

class BatchSizeOptimizerFinal:
    """ÊâπÊ¨°Â§ßÂ∞èÂÑ™ÂåñÂô® - ÊúÄÁµÇÁâàÊú¨"""
    
    def __init__(self, project_root: Path = None):
        self.project_root = project_root or Path(__file__).resolve().parent.parent.parent
        self.config_files = self._discover_config_files()
        self.backup_dir = self.project_root / "backups" / "config_backups"
        self.backup_dir.mkdir(parents=True, exist_ok=True)
        
        # RTX 4060 Ti 16GB ÁöÑÊúÄ‰Ω≥ÊâπÊ¨°Â§ßÂ∞èÔºàÂü∫Êñº‰πãÂâçÁöÑÊ∏¨Ë©¶Ôºâ
        self.gpu_optimal_batch_size = self._get_optimal_batch_size()
        
    def _get_optimal_batch_size(self) -> int:
        """Ê†πÊìöGPUÈ°ûÂûãÁç≤ÂèñÊúÄ‰Ω≥ÊâπÊ¨°Â§ßÂ∞è"""
        if torch.cuda.is_available():
            gpu_name = torch.cuda.get_device_name(0)
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)  # GB
            
            if "RTX 4060 Ti" in gpu_name and gpu_memory >= 15:
                return 204  # Âü∫Êñº‰πãÂâçÁöÑÊ∏¨Ë©¶ÁµêÊûú
            elif "RTX 4060" in gpu_name:
                return 150
            elif "RTX 4070" in gpu_name:
                return 220
            elif "RTX 4080" in gpu_name:
                return 300
            elif "RTX 4090" in gpu_name:
                return 400
            else:
                # Ê†πÊìöÈ°ØÂ≠òÂ§ßÂ∞è‰º∞ÁÆó
                if gpu_memory >= 20:
                    return 300
                elif gpu_memory >= 16:
                    return 200
                elif gpu_memory >= 12:
                    return 150
                elif gpu_memory >= 8:
                    return 100
                else:
                    return 64
        else:
            return 32  # CPUÊ®°Âºè
    
    def _discover_config_files(self) -> Dict[str, Path]:
        """ÁôºÁèæÊâÄÊúâÂåÖÂê´ÊâπÊ¨°Â§ßÂ∞èÁöÑÈÖçÁΩÆÊñá‰ª∂"""
        config_files = {}
        
        # ‰∏ªË¶ÅÈÖçÁΩÆÊñá‰ª∂
        main_config = self.project_root / "src" / "common" / "config.py"
        if main_config.exists():
            config_files["main_config"] = main_config
            
        # TransformerÈÖçÁΩÆÊñá‰ª∂
        transformer_config = self.project_root / "configs" / "enhanced_transformer_config.json"
        if transformer_config.exists():
            config_files["transformer_config"] = transformer_config
            
        return config_files
    
    def analyze_current_batch_sizes(self) -> Dict[str, Any]:
        """ÂàÜÊûêÁï∂ÂâçÁöÑÊâπÊ¨°Â§ßÂ∞èÈÖçÁΩÆ"""
        current_configs = {}
        
        for config_name, config_path in self.config_files.items():
            try:
                if config_path.suffix == ".py":
                    # PythonÈÖçÁΩÆÊñá‰ª∂
                    with open(config_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                    
                    batch_sizes = self._extract_batch_sizes_from_python(content)
                    if batch_sizes:
                        current_configs[config_name] = {
                            "type": "python",
                            "path": str(config_path),
                            "batch_sizes": batch_sizes
                        }
                        
                elif config_path.suffix == ".json":
                    # JSONÈÖçÁΩÆÊñá‰ª∂
                    with open(config_path, 'r', encoding='utf-8') as f:
                        content = json.load(f)
                    
                    batch_sizes = self._extract_batch_sizes_from_json(content)
                    if batch_sizes:
                        current_configs[config_name] = {
                            "type": "json",
                            "path": str(config_path),
                            "batch_sizes": batch_sizes,
                            "content": content
                        }
                        
            except Exception as e:
                logger.warning(f"ÁÑ°Ê≥ïÂàÜÊûêÈÖçÁΩÆÊñá‰ª∂ {config_path}: {e}")
                
        return current_configs
    
    def _extract_batch_sizes_from_python(self, content: str) -> Dict[str, int]:
        """ÂæûPythonÊñá‰ª∂‰∏≠ÊèêÂèñÊâπÊ¨°Â§ßÂ∞è"""
        batch_sizes = {}
        
        # ÂåπÈÖçÂêÑÁ®ÆÊâπÊ¨°Â§ßÂ∞èËÆäÊï∏
        patterns = [
            (r'SAC_BATCH_SIZE\s*=\s*(\d+)', 'SAC_BATCH_SIZE'),
            (r'BATCH_SIZE\s*=\s*(\d+)', 'BATCH_SIZE'),
            (r'batch_size\s*=\s*(\d+)', 'batch_size'),
            (r'TRANSFORMER_BATCH_SIZE\s*=\s*(\d+)', 'TRANSFORMER_BATCH_SIZE')
        ]
        
        for pattern, var_name in patterns:
            matches = re.findall(pattern, content)
            if matches:
                batch_sizes[var_name] = int(matches[0])
                
        return batch_sizes
    
    def _extract_batch_sizes_from_json(self, content: dict) -> Dict[str, int]:
        """ÂæûJSONÈÖçÁΩÆ‰∏≠ÊèêÂèñÊâπÊ¨°Â§ßÂ∞è"""
        batch_sizes = {}
        
        def search_recursive(obj, path=""):
            if isinstance(obj, dict):
                for key, value in obj.items():
                    new_path = f"{path}.{key}" if path else key
                    if key == "batch_size" and isinstance(value, int):
                        batch_sizes[new_path] = value
                    elif isinstance(value, (dict, list)):
                        search_recursive(value, new_path)
            elif isinstance(obj, list):
                for i, item in enumerate(obj):
                    search_recursive(item, f"{path}[{i}]")
        
        search_recursive(content)
        return batch_sizes
    
    def calculate_optimal_batch_sizes(self) -> Dict[str, int]:
        """Ë®àÁÆóÊúÄ‰Ω≥ÊâπÊ¨°Â§ßÂ∞è"""
        optimal_batch_size = self.gpu_optimal_batch_size
        
        # ÁÇ∫‰∏çÂêåÁî®ÈÄîË®àÁÆóÂêàÈÅ©ÁöÑÊâπÊ¨°Â§ßÂ∞è
        optimal_configs = {
            "SAC_BATCH_SIZE": min(optimal_batch_size, 128),  # SACÈÄöÂ∏∏‰∏çÈúÄË¶ÅÂ§™Â§ßÁöÑÊâπÊ¨°
            "transformer_batch_size": optimal_batch_size,     # TransformerÂèØ‰ª•Áî®ËºÉÂ§ßÁöÑ
            "general_batch_size": optimal_batch_size // 2,    # ÈÄöÁî®ÊâπÊ¨°Â§ßÂ∞èÁ®çÂ∞è‰∏Ä‰∫õ
            "evaluation_batch_size": optimal_batch_size // 4  # Ë©ï‰º∞Áî®Êõ¥Â∞èÁöÑÊâπÊ¨°Â§ßÂ∞è
        }
        
        return optimal_configs
    
    def backup_configs(self) -> List[str]:
        """ÂÇô‰ªΩÁï∂ÂâçÈÖçÁΩÆ"""
        backup_files = []
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        for config_name, config_path in self.config_files.items():
            try:
                if config_path.is_file():
                    backup_path = self.backup_dir / f"{config_name}_backup_{timestamp}{config_path.suffix}"
                    backup_path.write_text(config_path.read_text(encoding='utf-8'), encoding='utf-8')
                    backup_files.append(str(backup_path))
                    logger.info(f"‚úÖ Â∑≤ÂÇô‰ªΩ {config_path.name} -> {backup_path.name}")
            except Exception as e:
                logger.error(f"‚ùå ÂÇô‰ªΩÂ§±Êïó {config_path}: {e}")
                
        return backup_files
    
    def update_config_files(self, optimal_configs: Dict[str, int]) -> Dict[str, bool]:
        """Êõ¥Êñ∞ÈÖçÁΩÆÊñá‰ª∂"""
        update_results = {}
        current_configs = self.analyze_current_batch_sizes()
        
        for config_name, config_info in current_configs.items():
            try:
                if config_info["type"] == "python":
                    success = self._update_python_config(config_info, optimal_configs)
                elif config_info["type"] == "json":
                    success = self._update_json_config(config_info, optimal_configs)
                else:
                    success = False
                    
                update_results[config_name] = success
                
            except Exception as e:
                logger.error(f"‚ùå Êõ¥Êñ∞ÈÖçÁΩÆÂ§±Êïó {config_name}: {e}")
                update_results[config_name] = False
                
        return update_results
    
    def _update_python_config(self, config_info: Dict, optimal_configs: Dict[str, int]) -> bool:
        """Êõ¥Êñ∞PythonÈÖçÁΩÆÊñá‰ª∂"""
        try:
            config_path = Path(config_info["path"])
            with open(config_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            original_content = content
            
            # Êõ¥Êñ∞SAC_BATCH_SIZE
            if "SAC_BATCH_SIZE" in optimal_configs:
                pattern = r'SAC_BATCH_SIZE\s*=\s*\d+'
                replacement = f'SAC_BATCH_SIZE = {optimal_configs["SAC_BATCH_SIZE"]}'
                content = re.sub(pattern, replacement, content)
                logger.info(f"üîß Êõ¥Êñ∞ SAC_BATCH_SIZE ÁÇ∫ {optimal_configs['SAC_BATCH_SIZE']}")
            
            # Êõ¥Êñ∞ÈÄöÁî®BATCH_SIZE
            if "general_batch_size" in optimal_configs:
                pattern = r'BATCH_SIZE\s*=\s*\d+'
                replacement = f'BATCH_SIZE = {optimal_configs["general_batch_size"]}'
                content = re.sub(pattern, replacement, content)
                logger.info(f"üîß Êõ¥Êñ∞ BATCH_SIZE ÁÇ∫ {optimal_configs['general_batch_size']}")
            
            # Âè™ÊúâÂú®ÂÖßÂÆπÁúüÁöÑÊîπËÆäÊôÇÊâçÂØ´ÂõûÊñá‰ª∂
            if content != original_content:
                with open(config_path, 'w', encoding='utf-8') as f:
                    f.write(content)
                logger.info(f"‚úÖ Â∑≤Êõ¥Êñ∞PythonÈÖçÁΩÆ: {config_path.name}")
                return True
            else:
                logger.info(f"‚ÑπÔ∏è PythonÈÖçÁΩÆÁÑ°ÈúÄÊõ¥Êñ∞: {config_path.name}")
                return True
                
        except Exception as e:
            logger.error(f"‚ùå Êõ¥Êñ∞PythonÈÖçÁΩÆÂ§±Êïó: {e}")
            return False
    
    def _update_json_config(self, config_info: Dict, optimal_configs: Dict[str, int]) -> bool:
        """Êõ¥Êñ∞JSONÈÖçÁΩÆÊñá‰ª∂"""
        try:
            config_path = Path(config_info["path"])
            content = config_info["content"].copy()
            updated = False
            
            # Êõ¥Êñ∞Ë®ìÁ∑¥Áõ∏ÈóúÁöÑÊâπÊ¨°Â§ßÂ∞è
            if "training" in content and "batch_size" in content["training"]:
                old_value = content["training"]["batch_size"]
                new_value = optimal_configs.get("transformer_batch_size", old_value)
                if old_value != new_value:
                    content["training"]["batch_size"] = new_value
                    logger.info(f"üîß Êõ¥Êñ∞ training.batch_size: {old_value} -> {new_value}")
                    updated = True
            
            # Êõ¥Êñ∞Ë©ï‰º∞Áõ∏ÈóúÁöÑÊâπÊ¨°Â§ßÂ∞è
            if "evaluation" in content and "batch_size" in content["evaluation"]:
                old_value = content["evaluation"]["batch_size"]
                new_value = optimal_configs.get("evaluation_batch_size", old_value)
                if old_value != new_value:
                    content["evaluation"]["batch_size"] = new_value
                    logger.info(f"üîß Êõ¥Êñ∞ evaluation.batch_size: {old_value} -> {new_value}")
                    updated = True
            
            # Âè™ÊúâÂú®ÂÖßÂÆπÁúüÁöÑÊîπËÆäÊôÇÊâçÂØ´ÂõûÊñá‰ª∂
            if updated:
                with open(config_path, 'w', encoding='utf-8') as f:
                    json.dump(content, f, indent=2, ensure_ascii=False)
                logger.info(f"‚úÖ Â∑≤Êõ¥Êñ∞JSONÈÖçÁΩÆ: {config_path.name}")
            else:
                logger.info(f"‚ÑπÔ∏è JSONÈÖçÁΩÆÁÑ°ÈúÄÊõ¥Êñ∞: {config_path.name}")
                
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Êõ¥Êñ∞JSONÈÖçÁΩÆÂ§±Êïó: {e}")
            return False
    
    def optimize_batch_sizes(self, create_backup: bool = True) -> Dict[str, Any]:
        """Âü∑Ë°åÊâπÊ¨°Â§ßÂ∞èÂÑ™Âåñ"""
        print("üöÄ ÂïüÂãïÊâπÊ¨°Â§ßÂ∞èÂÑ™ÂåñÁ≥ªÁµ±...")
        print(f"üéØ ÁõÆÊ®ôGPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}")
        print(f"üí° ÊúÄ‰Ω≥ÊâπÊ¨°Â§ßÂ∞è: {self.gpu_optimal_batch_size}")
        
        # ÂàÜÊûêÁï∂ÂâçÈÖçÁΩÆ
        current_configs = self.analyze_current_batch_sizes()
        logger.info(f"üìã ÁôºÁèæ {len(current_configs)} ÂÄãÈÖçÁΩÆÊñá‰ª∂")
        
        # Ë®àÁÆóÊúÄ‰Ω≥ÊâπÊ¨°Â§ßÂ∞è
        optimal_configs = self.calculate_optimal_batch_sizes()
        logger.info("üéØ Ë®àÁÆóÂá∫ÊúÄ‰Ω≥ÊâπÊ¨°Â§ßÂ∞èÈÖçÁΩÆ")
        
        # ÂÇô‰ªΩÁèæÊúâÈÖçÁΩÆ
        backup_files = []
        if create_backup:
            backup_files = self.backup_configs()
            logger.info(f"üíæ Â∑≤ÂÇô‰ªΩ {len(backup_files)} ÂÄãÊñá‰ª∂")
        
        # Êõ¥Êñ∞ÈÖçÁΩÆÊñá‰ª∂
        update_results = self.update_config_files(optimal_configs)
        
        # ÁîüÊàêÂ†±Âëä
        report = {
            "timestamp": datetime.now().isoformat(),
            "gpu_info": {
                "name": torch.cuda.get_device_name(0) if torch.cuda.is_available() else "CPU",
                "optimal_batch_size": self.gpu_optimal_batch_size
            },
            "current_configs": current_configs,
            "optimal_configs": optimal_configs,
            "update_results": update_results,
            "backup_files": backup_files,
            "success_rate": sum(update_results.values()) / len(update_results) if update_results else 0
        }
        
        # ‰øùÂ≠òÂ†±Âëä
        try:
            report_dir = self.project_root / "reports"
            report_dir.mkdir(exist_ok=True)
            report_path = report_dir / "batch_size_optimization_final.json"
            with open(report_path, 'w', encoding='utf-8') as f:
                json.dump(report, f, indent=2, ensure_ascii=False)
            logger.info(f"üìä ÂÑ™ÂåñÂ†±ÂëäÂ∑≤‰øùÂ≠òËá≥: {report_path}")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è ÁÑ°Ê≥ï‰øùÂ≠òÂ†±Âëä: {e}")
        
        logger.info("‚úÖ ÊâπÊ¨°Â§ßÂ∞èÂÑ™ÂåñÂÆåÊàê")
        return report
    
    def print_optimization_summary(self, report: Dict[str, Any]):
        """ÊâìÂç∞ÂÑ™ÂåñÊëòË¶Å"""
        print("\n" + "="*70)
        print("üöÄ RTX 4060 Ti ÊâπÊ¨°Â§ßÂ∞èÂÑ™ÂåñÂ†±Âëä")
        print("="*70)
        
        # GPU‰ø°ÊÅØ
        gpu_info = report["gpu_info"]
        print(f"\nüéÆ GPU‰ø°ÊÅØ:")
        print(f"  ‚Ä¢ Ë®≠ÂÇô: {gpu_info['name']}")
        print(f"  ‚Ä¢ ÊúÄ‰Ω≥ÊâπÊ¨°Â§ßÂ∞è: {gpu_info['optimal_batch_size']}")
        
        # Áï∂ÂâçÈÖçÁΩÆ
        print(f"\nüìã ÁôºÁèæÁöÑÈÖçÁΩÆÊñá‰ª∂:")
        for config_name, config_info in report["current_configs"].items():
            print(f"  ‚Ä¢ {config_name}:")
            for var_name, current_value in config_info["batch_sizes"].items():
                print(f"    - {var_name}: {current_value}")
        
        # ÂÑ™ÂåñÂª∫Ë≠∞
        print(f"\nüéØ ÂÑ™ÂåñÂæåÁöÑÊâπÊ¨°Â§ßÂ∞è:")
        for config_name, optimal_value in report["optimal_configs"].items():
            print(f"  ‚Ä¢ {config_name}: {optimal_value}")
        
        # Êõ¥Êñ∞ÁµêÊûú
        print(f"\n‚úÖ Êõ¥Êñ∞ÁµêÊûú:")
        success_count = sum(report["update_results"].values())
        total_count = len(report["update_results"])
        print(f"  ÊàêÂäüÊõ¥Êñ∞: {success_count}/{total_count} ÂÄãÈÖçÁΩÆÊñá‰ª∂")
        
        for config_name, success in report["update_results"].items():
            icon = "‚úÖ" if success else "‚ùå"
            print(f"  {icon} {config_name}")
        
        # ÂÇô‰ªΩ‰ø°ÊÅØ
        if report["backup_files"]:
            print(f"\nüíæ ÂÇô‰ªΩÊñá‰ª∂ ({len(report['backup_files'])} ÂÄã):")
            for backup_file in report["backup_files"][:3]:  # Âè™È°ØÁ§∫Ââç3ÂÄã
                print(f"  ‚Ä¢ {Path(backup_file).name}")
            if len(report["backup_files"]) > 3:
                print(f"  ‚Ä¢ ... ÈÇÑÊúâ {len(report['backup_files']) - 3} ÂÄãÂÇô‰ªΩÊñá‰ª∂")
        
        # ÊÄßËÉΩÊèêÂçá‰º∞ÁÆó
        improvement = self._calculate_performance_improvement(report)
        print(f"\nüìà È†êÊúüÊÄßËÉΩÊèêÂçá:")
        print(f"  ‚Ä¢ GPUÂà©Áî®ÁéáÊèêÂçá: +{improvement['gpu_utilization']:.1f}%")
        print(f"  ‚Ä¢ Ë®ìÁ∑¥ÈÄüÂ∫¶ÊèêÂçá: +{improvement['training_speed']:.1f}%")
        print(f"  ‚Ä¢ Ë®òÊÜ∂È´îÊïàÁéáÊèêÂçá: +{improvement['memory_efficiency']:.1f}%")
        
        print(f"\nüìä ÂÑ™ÂåñÊàêÂäüÁéá: {report['success_rate']:.1%}")
        print("="*70)
    
    def _calculate_performance_improvement(self, report: Dict[str, Any]) -> Dict[str, float]:
        """Ë®àÁÆóÈ†êÊúüÁöÑÊÄßËÉΩÊèêÂçá"""
        current_batch_sizes = []
        optimal_batch_sizes = []
        
        for config_info in report["current_configs"].values():
            for batch_size in config_info["batch_sizes"].values():
                current_batch_sizes.append(batch_size)
        
        for batch_size in report["optimal_configs"].values():
            optimal_batch_sizes.append(batch_size)
        
        if current_batch_sizes and optimal_batch_sizes:
            avg_current = sum(current_batch_sizes) / len(current_batch_sizes)
            avg_optimal = sum(optimal_batch_sizes) / len(optimal_batch_sizes)
            
            improvement_ratio = avg_optimal / avg_current if avg_current > 0 else 1
            
            return {
                "gpu_utilization": (improvement_ratio - 1) * 100 * 0.8,  # GPUÂà©Áî®ÁéáÊîπÂñÑ
                "training_speed": (improvement_ratio - 1) * 100 * 0.6,   # Ë®ìÁ∑¥ÈÄüÂ∫¶ÊîπÂñÑ
                "memory_efficiency": (improvement_ratio - 1) * 100 * 0.4  # Ë®òÊÜ∂È´îÊïàÁéáÊîπÂñÑ
            }
        
        return {"gpu_utilization": 0, "training_speed": 0, "memory_efficiency": 0}


def main():
    """‰∏ªÂáΩÊï∏ÔºöÂü∑Ë°åÊâπÊ¨°Â§ßÂ∞èÂÑ™Âåñ"""
    print("üîç Ê≠£Âú®ÂàùÂßãÂåñRTX 4060 TiÊâπÊ¨°Â§ßÂ∞èÂÑ™ÂåñÂô®...")
    
    optimizer = BatchSizeOptimizerFinal()
    
    # È°ØÁ§∫Áï∂ÂâçGPU‰ø°ÊÅØ
    if torch.cuda.is_available():
        gpu_name = torch.cuda.get_device_name(0)
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)
        print(f"üéÆ Ê™¢Ê∏¨Âà∞GPU: {gpu_name}")
        print(f"üíæ GPUË®òÊÜ∂È´î: {gpu_memory:.1f}GB")
        print(f"üéØ Âª∫Ë≠∞ÊúÄ‰Ω≥ÊâπÊ¨°Â§ßÂ∞è: {optimizer.gpu_optimal_batch_size}")
    else:
        print("‚ö†Ô∏è Êú™Ê™¢Ê∏¨Âà∞CUDA GPUÔºåÂ∞á‰ΩøÁî®CPUÊ®°Âºè")
    
    # ÂàÜÊûêÁï∂ÂâçÈÖçÁΩÆ
    current_configs = optimizer.analyze_current_batch_sizes()
    if not current_configs:
        print("‚ùå Êú™ÊâæÂà∞‰ªª‰ΩïÈÖçÁΩÆÊñá‰ª∂")
        return False
    
    print(f"\nüìã ÊâæÂà∞ {len(current_configs)} ÂÄãÈÖçÁΩÆÊñá‰ª∂:")
    for name, info in current_configs.items():
        print(f"  ‚Ä¢ {name}: {info['batch_sizes']}")
    
    # Âü∑Ë°åÊâπÊ¨°Â§ßÂ∞èÂÑ™Âåñ
    print("\nüöÄ Âü∑Ë°åÊâπÊ¨°Â§ßÂ∞èÂÑ™Âåñ...")
    report = optimizer.optimize_batch_sizes()
    optimizer.print_optimization_summary(report)
    
    # ÊèêÁ§∫ÈáçÂïü
    if report["success_rate"] > 0:
        print("\n‚ö†Ô∏è üîÑ ÈÖçÁΩÆÂ∑≤Êõ¥Êñ∞ÔºÅÂª∫Ë≠∞ÈáçÂïüË®ìÁ∑¥Á®ãÂ∫è‰ª•ÊáâÁî®Êñ∞ÁöÑÊâπÊ¨°Â§ßÂ∞èË®≠ÂÆö")
        print("   üìà È†êÊúüÂ∞áÈ°ØËëóÊèêÂçáGPUÂà©Áî®ÁéáÂíåË®ìÁ∑¥ÈÄüÂ∫¶")
        return True
    else:
        print("\n‚ùå ÂÑ™ÂåñÂ§±ÊïóÔºåË´ãÊ™¢Êü•ÈåØË™§Êó•Ë™å")
        return False


if __name__ == "__main__":
    success = main()
    if success:
        print("\nüéâ ÊâπÊ¨°Â§ßÂ∞èÂÑ™ÂåñÊàêÂäüÂÆåÊàêÔºÅ")
    else:
        print("\nüòû ÊâπÊ¨°Â§ßÂ∞èÂÑ™ÂåñÂ§±Êïó")
