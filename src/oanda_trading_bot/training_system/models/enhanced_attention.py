# src/models/enhanced_attention.py
"""
å¢å¼·æ³¨æ„åŠ›æ©Ÿåˆ¶å¯¦ç¾
åŸºæ–¼æœ€æ–°ç ”ç©¶å„ªåŒ–Transformeræ³¨æ„åŠ›å±¤

ä¸»è¦å‰µæ–°ï¼š
1. å¤šå°ºåº¦æ³¨æ„åŠ›èåˆ
2. å‹•æ…‹æ³¨æ„åŠ›æ¬Šé‡
3. è¨˜æ†¶é«”æ•ˆç‡å„ªåŒ–
4. ä½ç½®ç·¨ç¢¼å¢å¼·
5. è·¨æ™‚é–“å°ºåº¦æ³¨æ„åŠ›
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import math
import numpy as np
from typing import Optional, Tuple, Dict, Any
import logging

try:
    from oanda_trading_bot.training_system.common.logger_setup import logger
except ImportError:
    logger = logging.getLogger(__name__)


class MultiScaleAttention(nn.Module):
    """å¤šå°ºåº¦æ³¨æ„åŠ›æ©Ÿåˆ¶"""
    
    def __init__(self, 
                 d_model: int, 
                 num_heads: int,
                 scales: list = [1, 2, 4, 8],
                 dropout: float = 0.1):
        super().__init__()
        assert d_model % num_heads == 0
        
        self.d_model = d_model
        self.num_heads = num_heads
        self.d_k = d_model // num_heads
        self.scales = scales
        self.dropout = dropout
        
        # æ¯å€‹å°ºåº¦çš„æŸ¥è©¢ã€éµã€å€¼æŠ•å½±
        self.scale_projections = nn.ModuleDict()
        for scale in scales:
            self.scale_projections[f'scale_{scale}'] = nn.ModuleDict({
                'query': nn.Linear(d_model, d_model // len(scales)),
                'key': nn.Linear(d_model, d_model // len(scales)),
                'value': nn.Linear(d_model, d_model // len(scales))
            })
        
        # å°ºåº¦èåˆå±¤
        self.scale_fusion = nn.Sequential(
            nn.Linear(d_model, d_model),
            nn.GELU(),
            nn.Dropout(dropout)
        )
        
        # è¼¸å‡ºæŠ•å½±
        self.out_proj = nn.Linear(d_model, d_model)
        self.dropout_layer = nn.Dropout(dropout)
        
    def forward(self, 
                query: torch.Tensor, 
                key: torch.Tensor, 
                value: torch.Tensor,
                mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        Args:
            query, key, value: [batch_size, seq_len, d_model]
            mask: [batch_size, seq_len, seq_len]
        """
        batch_size, seq_len, _ = query.shape
        
        scale_outputs = []
        
        for scale in self.scales:
            # ç²å–è©²å°ºåº¦çš„æŠ•å½±
            q_proj = self.scale_projections[f'scale_{scale}']['query']
            k_proj = self.scale_projections[f'scale_{scale}']['key']
            v_proj = self.scale_projections[f'scale_{scale}']['value']
            
            # æ‡‰ç”¨å°ºåº¦è®Šæ›
            if scale > 1:
                # ä¸‹æ¡æ¨£
                pooled_len = seq_len // scale
                pooled_query = F.adaptive_avg_pool1d(
                    query.transpose(1, 2), pooled_len
                ).transpose(1, 2)
                pooled_key = F.adaptive_avg_pool1d(
                    key.transpose(1, 2), pooled_len
                ).transpose(1, 2)
                pooled_value = F.adaptive_avg_pool1d(
                    value.transpose(1, 2), pooled_len
                ).transpose(1, 2)
            else:
                pooled_query = query
                pooled_key = key
                pooled_value = value
            
            # è¨ˆç®—æ³¨æ„åŠ›
            scale_attention = self._scaled_dot_product_attention(
                q_proj(pooled_query),
                k_proj(pooled_key), 
                v_proj(pooled_value),
                mask
            )
            
            # ä¸Šæ¡æ¨£å›åŸå°ºåº¦
            if scale > 1:
                scale_attention = F.interpolate(
                    scale_attention.transpose(1, 2),
                    size=seq_len,
                    mode='linear',
                    align_corners=False
                ).transpose(1, 2)
            
            scale_outputs.append(scale_attention)
        
        # èåˆæ‰€æœ‰å°ºåº¦
        fused_output = torch.cat(scale_outputs, dim=-1)
        fused_output = self.scale_fusion(fused_output)
        
        # è¼¸å‡ºæŠ•å½±
        output = self.out_proj(fused_output)
        output = self.dropout_layer(output)
        
        return output
    
    def _scaled_dot_product_attention(self,
                                    query: torch.Tensor,
                                    key: torch.Tensor, 
                                    value: torch.Tensor,
                                    mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        """ç¸®æ”¾é»ç©æ³¨æ„åŠ›"""
        d_k = query.size(-1)
        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)
        
        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)
        
        attention_weights = F.softmax(scores, dim=-1)
        attention_weights = self.dropout_layer(attention_weights)
        
        return torch.matmul(attention_weights, value)


class DynamicPositionalEncoding(nn.Module):
    """å‹•æ…‹ä½ç½®ç·¨ç¢¼"""
    
    def __init__(self, d_model: int, max_len: int = 5000):
        super().__init__()
        self.d_model = d_model
        self.max_len = max_len
        
        # å¯å­¸ç¿’çš„ä½ç½®ç·¨ç¢¼
        self.pos_embedding = nn.Parameter(torch.randn(max_len, d_model) * 0.1)
        
        # ç›¸å°ä½ç½®ç·¨ç¢¼
        self.relative_pos_k = nn.Parameter(torch.randn(2 * max_len - 1, d_model // 2))
        self.relative_pos_v = nn.Parameter(torch.randn(2 * max_len - 1, d_model // 2))
        
        # æ™‚é–“å°ºåº¦ç·¨ç¢¼
        self.time_scale_embedding = nn.Parameter(torch.randn(10, d_model) * 0.1)
        
    def forward(self, x: torch.Tensor, time_scale: int = 1) -> torch.Tensor:
        """
        Args:
            x: [batch_size, seq_len, d_model]
            time_scale: æ™‚é–“å°ºåº¦ (1, 2, 4, 8, ...)
        """
        batch_size, seq_len, d_model = x.shape
        
        # çµ•å°ä½ç½®ç·¨ç¢¼
        pos_enc = self.pos_embedding[:seq_len].unsqueeze(0).expand(batch_size, -1, -1)
        
        # æ™‚é–“å°ºåº¦ç·¨ç¢¼
        scale_idx = min(int(math.log2(time_scale)), 9)
        time_enc = self.time_scale_embedding[scale_idx].unsqueeze(0).unsqueeze(0)
        time_enc = time_enc.expand(batch_size, seq_len, -1)
        
        return x + pos_enc + time_enc
    
    def get_relative_position_encoding(self, seq_len: int) -> Tuple[torch.Tensor, torch.Tensor]:
        """ç²å–ç›¸å°ä½ç½®ç·¨ç¢¼"""
        positions = torch.arange(seq_len).unsqueeze(1) - torch.arange(seq_len).unsqueeze(0)
        positions = positions + self.max_len - 1
        positions = torch.clamp(positions, 0, 2 * self.max_len - 2)
        
        rel_pos_k = self.relative_pos_k[positions]
        rel_pos_v = self.relative_pos_v[positions]
        
        return rel_pos_k, rel_pos_v


class MemoryEfficientAttention(nn.Module):
    """è¨˜æ†¶é«”é«˜æ•ˆæ³¨æ„åŠ›æ©Ÿåˆ¶"""
    
    def __init__(self, 
                 d_model: int,
                 num_heads: int,
                 chunk_size: int = 256,
                 dropout: float = 0.1):
        super().__init__()
        assert d_model % num_heads == 0
        
        self.d_model = d_model
        self.num_heads = num_heads
        self.d_k = d_model // num_heads
        self.chunk_size = chunk_size
        
        self.q_proj = nn.Linear(d_model, d_model)
        self.k_proj = nn.Linear(d_model, d_model)
        self.v_proj = nn.Linear(d_model, d_model)
        self.out_proj = nn.Linear(d_model, d_model)
        
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, 
                query: torch.Tensor,
                key: torch.Tensor,
                value: torch.Tensor,
                mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        """åˆ†å¡Šè¨ˆç®—æ³¨æ„åŠ›ä»¥ç¯€çœè¨˜æ†¶é«”"""
        batch_size, seq_len, _ = query.shape
        
        # æŠ•å½±
        Q = self.q_proj(query).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)
        K = self.k_proj(key).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)
        V = self.v_proj(value).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)
        
        if seq_len <= self.chunk_size:
            # åºåˆ—è¼ƒçŸ­ï¼Œç›´æ¥è¨ˆç®—
            attention_output = self._compute_attention(Q, K, V, mask)
        else:
            # åºåˆ—è¼ƒé•·ï¼Œåˆ†å¡Šè¨ˆç®—
            attention_output = self._compute_chunked_attention(Q, K, V, mask)
        
        # é‡å¡‘ä¸¦è¼¸å‡ºæŠ•å½±
        attention_output = attention_output.transpose(1, 2).contiguous().view(
            batch_size, seq_len, self.d_model
        )
        
        return self.out_proj(attention_output)
    
    def _compute_attention(self, Q, K, V, mask=None):
        """æ¨™æº–æ³¨æ„åŠ›è¨ˆç®—"""
        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)
        
        if mask is not None:
            scores = scores.masked_fill(mask.unsqueeze(1).unsqueeze(1) == 0, -1e9)
        
        attention_weights = F.softmax(scores, dim=-1)
        attention_weights = self.dropout(attention_weights)
        
        return torch.matmul(attention_weights, V)
    
    def _compute_chunked_attention(self, Q, K, V, mask=None):
        """åˆ†å¡Šæ³¨æ„åŠ›è¨ˆç®—"""
        batch_size, num_heads, seq_len, d_k = Q.shape
        chunk_size = self.chunk_size
        
        output = torch.zeros_like(Q)
        
        for start_q in range(0, seq_len, chunk_size):
            end_q = min(start_q + chunk_size, seq_len)
            
            chunk_scores = []
            chunk_values = []
            
            for start_k in range(0, seq_len, chunk_size):
                end_k = min(start_k + chunk_size, seq_len)
                
                # è¨ˆç®—ç•¶å‰å¡Šçš„åˆ†æ•¸
                scores = torch.matmul(
                    Q[:, :, start_q:end_q], 
                    K[:, :, start_k:end_k].transpose(-2, -1)
                ) / math.sqrt(d_k)
                
                if mask is not None:
                    chunk_mask = mask[start_q:end_q, start_k:end_k]
                    scores = scores.masked_fill(
                        chunk_mask.unsqueeze(0).unsqueeze(0) == 0, -1e9
                    )
                
                chunk_scores.append(scores)
                chunk_values.append(V[:, :, start_k:end_k])
            
            # åœ¨æ‰€æœ‰å¡Šä¸Šè¨ˆç®—softmax
            all_scores = torch.cat(chunk_scores, dim=-1)
            attention_weights = F.softmax(all_scores, dim=-1)
            attention_weights = self.dropout(attention_weights)
            
            # åˆ†å¡Šæ‡‰ç”¨æ³¨æ„åŠ›æ¬Šé‡
            chunk_output = torch.zeros(
                batch_size, num_heads, end_q - start_q, d_k,
                device=Q.device, dtype=Q.dtype
            )
            
            start_idx = 0
            for i, values in enumerate(chunk_values):
                end_idx = start_idx + values.size(-2)
                weights = attention_weights[:, :, :, start_idx:end_idx]
                chunk_output += torch.matmul(weights, values)
                start_idx = end_idx
            
            output[:, :, start_q:end_q] = chunk_output
        
        return output


class CrossTimeScaleAttention(nn.Module):
    """è·¨æ™‚é–“å°ºåº¦æ³¨æ„åŠ›"""
    
    def __init__(self, 
                 d_model: int,
                 num_heads: int, 
                 time_scales: list = [1, 4, 16, 64],
                 dropout: float = 0.1):
        super().__init__()
        self.d_model = d_model
        self.num_heads = num_heads
        self.time_scales = time_scales
        
        # æ¯å€‹æ™‚é–“å°ºåº¦çš„æ³¨æ„åŠ›å±¤
        self.scale_attentions = nn.ModuleList([
            MultiScaleAttention(d_model, num_heads, [scale], dropout)
            for scale in time_scales
        ])
        
        # æ™‚é–“å°ºåº¦èåˆ
        self.temporal_fusion = nn.Sequential(
            nn.Linear(d_model * len(time_scales), d_model),
            nn.GELU(),
            nn.Dropout(dropout)
        )
        
        # é–€æ§æ©Ÿåˆ¶
        self.gate = nn.Sequential(
            nn.Linear(d_model, len(time_scales)),
            nn.Sigmoid()
        )
        
    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        Args:
            x: [batch_size, seq_len, d_model]
            mask: [batch_size, seq_len, seq_len]
        """
        scale_outputs = []
        
        # è¨ˆç®—æ¯å€‹æ™‚é–“å°ºåº¦çš„æ³¨æ„åŠ›
        for attention_layer in self.scale_attentions:
            scale_output = attention_layer(x, x, x, mask)
            scale_outputs.append(scale_output)
        
        # è¨ˆç®—é–€æ§æ¬Šé‡
        gate_weights = self.gate(x.mean(dim=1, keepdim=True))  # [batch_size, 1, num_scales]
        
        # åŠ æ¬Šèåˆ
        weighted_outputs = []
        for i, output in enumerate(scale_outputs):
            weight = gate_weights[:, :, i].unsqueeze(-1)  # [batch_size, 1, 1]
            weighted_outputs.append(output * weight)
        
        # æ‹¼æ¥ä¸¦èåˆ
        concatenated = torch.cat(weighted_outputs, dim=-1)
        fused_output = self.temporal_fusion(concatenated)
        
        return fused_output + x  # æ®˜å·®é€£æ¥


def create_enhanced_attention_layer(d_model: int,
                                  num_heads: int,
                                  attention_type: str = "multi_scale",
                                  **kwargs) -> nn.Module:
    """å‰µå»ºå¢å¼·æ³¨æ„åŠ›å±¤çš„ä¾¿æ·å‡½æ•¸"""
    
    if attention_type == "multi_scale":
        return MultiScaleAttention(d_model, num_heads, **kwargs)
    elif attention_type == "memory_efficient":
        return MemoryEfficientAttention(d_model, num_heads, **kwargs)
    elif attention_type == "cross_time_scale":
        return CrossTimeScaleAttention(d_model, num_heads, **kwargs)
    else:
        raise ValueError(f"æœªçŸ¥çš„æ³¨æ„åŠ›é¡å‹: {attention_type}")


if __name__ == "__main__":
    # æ¸¬è©¦å¢å¼·æ³¨æ„åŠ›æ©Ÿåˆ¶
    print("ğŸ” æ¸¬è©¦å¢å¼·æ³¨æ„åŠ›æ©Ÿåˆ¶")
    print("="*50)
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    # æ¸¬è©¦åƒæ•¸
    batch_size, seq_len, d_model = 4, 128, 768
    num_heads = 12
    
    # å‰µå»ºæ¸¬è©¦æ•¸æ“š
    x = torch.randn(batch_size, seq_len, d_model).to(device)
    
    # æ¸¬è©¦å¤šå°ºåº¦æ³¨æ„åŠ›
    print("ğŸ¯ æ¸¬è©¦å¤šå°ºåº¦æ³¨æ„åŠ›...")
    multi_scale_attn = MultiScaleAttention(d_model, num_heads).to(device)
    output1 = multi_scale_attn(x, x, x)
    print(f"  è¼¸å…¥å½¢ç‹€: {x.shape}")
    print(f"  è¼¸å‡ºå½¢ç‹€: {output1.shape}")
    print(f"  åƒæ•¸æ•¸é‡: {sum(p.numel() for p in multi_scale_attn.parameters()):,}")
    
    # æ¸¬è©¦è¨˜æ†¶é«”é«˜æ•ˆæ³¨æ„åŠ›
    print("\nğŸ’¾ æ¸¬è©¦è¨˜æ†¶é«”é«˜æ•ˆæ³¨æ„åŠ›...")
    memory_eff_attn = MemoryEfficientAttention(d_model, num_heads, chunk_size=64).to(device)
    output2 = memory_eff_attn(x, x, x)
    print(f"  è¼¸å…¥å½¢ç‹€: {x.shape}")
    print(f"  è¼¸å‡ºå½¢ç‹€: {output2.shape}")
    print(f"  åƒæ•¸æ•¸é‡: {sum(p.numel() for p in memory_eff_attn.parameters()):,}")
    
    # æ¸¬è©¦è·¨æ™‚é–“å°ºåº¦æ³¨æ„åŠ›
    print("\nâ±ï¸ æ¸¬è©¦è·¨æ™‚é–“å°ºåº¦æ³¨æ„åŠ›...")
    cross_time_attn = CrossTimeScaleAttention(d_model, num_heads).to(device)
    output3 = cross_time_attn(x)
    print(f"  è¼¸å…¥å½¢ç‹€: {x.shape}")
    print(f"  è¼¸å‡ºå½¢ç‹€: {output3.shape}")
    print(f"  åƒæ•¸æ•¸é‡: {sum(p.numel() for p in cross_time_attn.parameters()):,}")
    
    print("\nâœ… æ‰€æœ‰æ³¨æ„åŠ›æ©Ÿåˆ¶æ¸¬è©¦å®Œæˆï¼")
