#!/usr/bin/env python3
"""
æ¢¯åº¦å’Œåƒæ•¸ç›£æ§å·¥å…·
ç”¨æ–¼å¯¦æ™‚ç›£æ§æ¨¡å‹è¨“ç·´éç¨‹ä¸­çš„æ¢¯åº¦æµå’Œåƒæ•¸æ›´æ–°
"""

import torch
import numpy as np
from typing import Dict, Any, Optional, List
import logging
from collections import defaultdict

class GradientMonitor:
    """æ¢¯åº¦å’Œåƒæ•¸ç›£æ§å·¥å…·"""
    
    def __init__(self, model, logger: Optional[logging.Logger] = None):
        self.model = model
        self.logger = logger or logging.getLogger(__name__)
        self.parameter_history = defaultdict(list)
        self.gradient_history = defaultdict(list)
        self.iteration_count = 0
        
        # æª¢æŸ¥æ¨¡å‹æ˜¯å¦æœ‰æ•ˆ
        if self.model is None:
            self.logger.warning("âš ï¸ å˜—è©¦ç›£æ§ None æ¨¡å‹")
            self.is_valid = False
        else:
            try:
                # å˜—è©¦è¨ªå• named_parameters ä¾†é©—è­‰æ¨¡å‹
                param_count = sum(1 for _ in self.model.named_parameters())
                self.is_valid = param_count > 0
                if not self.is_valid:
                    self.logger.warning(f"âš ï¸ æ¨¡å‹æ²’æœ‰åƒæ•¸: {type(self.model)}")
                else:
                    self.logger.info(f"âœ“ æ¢¯åº¦ç›£æ§å™¨å·²è¨­ç½®ï¼Œæ¨¡å‹åƒæ•¸æ•¸é‡: {param_count}")
            except Exception as e:
                self.logger.warning(f"âš ï¸ ç„¡æ³•è¨ªå•æ¨¡å‹åƒæ•¸: {e}")
                self.is_valid = False
    
    def record_parameters(self, step: int = None):
        """è¨˜éŒ„ç•¶å‰åƒæ•¸ç‹€æ…‹"""
        if not self.is_valid:
            return {}
            
        if step is None:
            step = self.iteration_count
            self.iteration_count += 1
            
        param_norms = {}
        for name, param in self.model.named_parameters():
            if param.data is not None:
                norm = param.data.norm().item()
                param_norms[name] = norm
                self.parameter_history[name].append((step, norm))
                
        return param_norms
    
    def record_gradients(self, step: int = None):
        """è¨˜éŒ„ç•¶å‰æ¢¯åº¦ç‹€æ…‹"""
        if not self.is_valid:
            return {}
            
        if step is None:
            step = self.iteration_count
            
        grad_norms = {}
        for name, param in self.model.named_parameters():
            if param.grad is not None:
                norm = param.grad.norm().item()
                grad_norms[name] = norm
                self.gradient_history[name].append((step, norm))
            else:
                grad_norms[name] = 0.0
                self.gradient_history[name].append((step, 0.0))
                
        return grad_norms
    
    def check_gradient_flow(self) -> Dict[str, Any]:
        """æª¢æŸ¥æ¢¯åº¦æµç‹€æ³"""
        if not self.is_valid:
            return {
                'has_gradients': False,
                'gradient_norms': {},
                'parameter_norms': {},
                'zero_gradients': [],
                'large_gradients': [],
                'gradient_flow_ok': False,
                'total_gradient_norm': 0.0,
                'zero_gradient_ratio': 1.0
            }
        
        results = {
            'has_gradients': False,
            'gradient_norms': {},
            'parameter_norms': {},
            'zero_gradients': [],
            'large_gradients': [],
            'gradient_flow_ok': False
        }
        
        total_grad_norm = 0.0
        param_count = 0
        zero_grad_count = 0
        
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                param_count += 1
                
                # åƒæ•¸ç¯„æ•¸
                param_norm = param.data.norm().item()
                results['parameter_norms'][name] = param_norm
                
                # æ¢¯åº¦ç¯„æ•¸
                if param.grad is not None:
                    grad_norm = param.grad.norm().item()
                    results['gradient_norms'][name] = grad_norm
                    total_grad_norm += grad_norm
                    
                    if grad_norm == 0.0:
                        zero_grad_count += 1
                        results['zero_gradients'].append(name)
                    elif grad_norm > 10.0:  # å¤§æ¢¯åº¦é–¾å€¼
                        results['large_gradients'].append(name)
                else:
                    results['gradient_norms'][name] = 0.0
                    zero_grad_count += 1
                    results['zero_gradients'].append(name)
        
        results['has_gradients'] = total_grad_norm > 0
        results['total_gradient_norm'] = total_grad_norm
        results['zero_gradient_ratio'] = zero_grad_count / max(param_count, 1)
        results['gradient_flow_ok'] = (
            results['has_gradients'] and 
            results['zero_gradient_ratio'] < 0.8  # å…è¨±æœ€å¤š80%çš„åƒæ•¸ç„¡æ¢¯åº¦
        )
        
        return results
    
    def get_parameter_update_ratio(self) -> float:
        """è¨ˆç®—åƒæ•¸æ›´æ–°æ¯”ä¾‹"""
        if not self.is_valid or len(self.parameter_history) < 2:
            return 0.0
            
        total_change = 0.0
        total_params = 0.0
        
        for param_name, history in self.parameter_history.items():
            if len(history) >= 2:
                # æ¯”è¼ƒæœ€è¿‘å…©æ¬¡è¨˜éŒ„
                _, old_norm = history[-2]
                _, new_norm = history[-1]
                
                change = abs(new_norm - old_norm)
                total_change += change
                total_params += max(old_norm, 1e-8)  # é¿å…é™¤é›¶
        
        if total_params > 0:
            return total_change / total_params
        return 0.0
    
    def get_gradient_statistics(self) -> Dict[str, float]:
        """ç²å–æ¢¯åº¦çµ±è¨ˆä¿¡æ¯"""
        if not self.is_valid:
            return {
                'mean_gradient': 0.0,
                'max_gradient': 0.0,
                'min_gradient': 0.0,
                'std_gradient': 0.0
            }
            
        all_gradients = []
        
        for name, param in self.model.named_parameters():
            if param.grad is not None:
                grad_norm = param.grad.norm().item()
                all_gradients.append(grad_norm)
        
        if not all_gradients:
            return {
                'mean_gradient': 0.0,
                'max_gradient': 0.0,
                'min_gradient': 0.0,
                'std_gradient': 0.0
            }
        
        all_gradients = np.array(all_gradients)
        
        return {
            'mean_gradient': float(np.mean(all_gradients)),
            'max_gradient': float(np.max(all_gradients)),
            'min_gradient': float(np.min(all_gradients)),
            'std_gradient': float(np.std(all_gradients))
        }
    
    def print_gradient_summary(self):
        """æ‰“å°æ¢¯åº¦æ‘˜è¦"""
        if not self.is_valid:
            self.logger.warning("âš ï¸ ç„¡æ³•æ‰“å°æ¢¯åº¦æ‘˜è¦ï¼šæ¨¡å‹ç„¡æ•ˆ")
            return
            
        gradient_stats = self.get_gradient_statistics()
        flow_status = self.check_gradient_flow()
        update_ratio = self.get_parameter_update_ratio()
        
        self.logger.info("=" * 50)
        self.logger.info("ğŸ” æ¢¯åº¦æµåˆ†ææ‘˜è¦")
        self.logger.info("=" * 50)
        self.logger.info(f"æ¢¯åº¦æµç‹€æ…‹: {'âœ… æ­£å¸¸' if flow_status['gradient_flow_ok'] else 'âŒ ç•°å¸¸'}")
        self.logger.info(f"ç¸½æ¢¯åº¦ç¯„æ•¸: {flow_status['total_gradient_norm']:.6f}")
        self.logger.info(f"é›¶æ¢¯åº¦æ¯”ä¾‹: {flow_status['zero_gradient_ratio']:.2%}")
        self.logger.info(f"åƒæ•¸æ›´æ–°æ¯”ä¾‹: {update_ratio:.8f}")
        self.logger.info(f"å¹³å‡æ¢¯åº¦: {gradient_stats['mean_gradient']:.6f}")
        self.logger.info(f"æœ€å¤§æ¢¯åº¦: {gradient_stats['max_gradient']:.6f}")
        self.logger.info(f"æœ€å°æ¢¯åº¦: {gradient_stats['min_gradient']:.6f}")
        self.logger.info("=" * 50)
