"""
ç›´æ¥é‹è¡Œæ¸¬è©¦ï¼Œé©—è­‰æ‰€æœ‰æ¨¡çµ„çš„æµç¨‹å’Œæ¢¯åº¦æµ
"""

import sys
import os
import torch
import torch.nn as nn
import numpy as np
import logging

# Add project root to path
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), "..", "..")))

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def test_complete_model_flow():
    """å®Œæ•´æ¨¡å‹æµç¨‹æ¸¬è©¦"""
    logger.info("é–‹å§‹å®Œæ•´æ¨¡å‹æµç¨‹æ¸¬è©¦...")
    
    try:
        # Import required modules
        from src.models.enhanced_transformer import EnhancedTransformer
        from src.agent.enhanced_quantum_strategy_layer import EnhancedStrategySuperposition
        from src.agent.meta_learning_system import MetaLearningSystem
        from src.environment.progressive_reward_system import ProgressiveLearningSystem
        
        logger.info("âœ“ æ‰€æœ‰æ¨¡çµ„å°å…¥æˆåŠŸ")
        
        # Setup device and parameters
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        batch_size = 4
        seq_len = 100
        num_features = 24
        num_symbols = 3
        
        logger.info(f"ä½¿ç”¨è¨­å‚™: {device}")
        
        # 1. æ¸¬è©¦ Enhanced Transformer
        logger.info("1. æ¸¬è©¦ Enhanced Transformer...")
        transformer_config = {
            'input_size': num_features,
            'hidden_size': 256,
            'num_layers': 4,
            'num_heads': 8,
            'num_symbols': num_symbols,
            'dropout': 0.1,
            'use_msfe': True,
            'use_wavelet': True,
            'use_fourier': True,
            'use_market_state_detection': True
        }
        
        transformer = EnhancedTransformer(**transformer_config).to(device)
        
        # å‰µå»ºæ¸¬è©¦æ•¸æ“š
        market_data = torch.randn(batch_size, seq_len, num_symbols, num_features, device=device)
        input_tensor = market_data.view(batch_size, seq_len, num_symbols * num_features)
        src_key_padding_mask = torch.zeros(batch_size, num_symbols, dtype=torch.bool, device=device)
        
        # Transformerå‰å‘å‚³æ’­
        transformer_output = transformer(input_tensor, src_key_padding_mask=src_key_padding_mask)
        logger.info(f"âœ“ Transformerè¼¸å‡ºå½¢ç‹€: {transformer_output.shape}")
        assert transformer_output.requires_grad, "Transformerè¼¸å‡ºæ‡‰è©²éœ€è¦æ¢¯åº¦"
        
        # 2. æ¸¬è©¦é‡å­ç­–ç•¥å±¤
        logger.info("2. æ¸¬è©¦é‡å­ç­–ç•¥å±¤...")
        quantum_layer = EnhancedStrategySuperposition(
            input_dim=256,
            num_strategies=5,
            dropout_rate=0.1,
            strategy_input_dim=256
        ).to(device)
        
        # é‡å­ç­–ç•¥å±¤å‰å‘å‚³æ’­
        last_hidden = transformer_output[:, -1, :]  # ä½¿ç”¨æœ€å¾Œä¸€å€‹æ™‚é–“æ­¥
        strategy_output = quantum_layer(last_hidden)
        logger.info(f"âœ“ é‡å­ç­–ç•¥å±¤è¼¸å‡ºå½¢ç‹€: {strategy_output.shape}")
        assert strategy_output.requires_grad, "é‡å­ç­–ç•¥å±¤è¼¸å‡ºæ‡‰è©²éœ€è¦æ¢¯åº¦"
        
        # 3. æ¸¬è©¦å…ƒå­¸ç¿’ç³»çµ±
        logger.info("3. æ¸¬è©¦å…ƒå­¸ç¿’ç³»çµ±...")
        meta_learning = MetaLearningSystem(
            strategy_pool_size=10,
            adaptation_lr=1e-3,
            memory_size=1000,
            device=device
        )
        
        # æ¨¡æ“¬ç­–ç•¥è¡¨ç¾æ•¸æ“š
        strategy_performance = {
            'returns': np.random.randn(5),
            'sharpe_ratios': np.random.uniform(0.5, 2.0, 5),
            'max_drawdowns': np.random.uniform(0.05, 0.15, 5),
            'win_rates': np.random.uniform(0.4, 0.7, 5)
        }
        
        market_state = 1
        evaluation_results = meta_learning.evaluate_strategy_performance(strategy_performance, market_state)
        logger.info(f"âœ“ å…ƒå­¸ç¿’è©•ä¼°çµæœ: {evaluation_results}")
        
        # 4. æ¸¬è©¦æ¼¸é€²å¼çå‹µç³»çµ±
        logger.info("4. æ¸¬è©¦æ¼¸é€²å¼çå‹µç³»çµ±...")
        reward_system = ProgressiveLearningSystem(
            initial_stage='simple',
            stage_criteria={'simple': 100, 'intermediate': 500}
        )
        
        # è¨ˆç®—çå‹µ
        trading_results = {
            'profit_loss': 0.05,
            'sharpe_ratio': 1.2,
            'max_drawdown': 0.08,
            'volatility': 0.15,
            'trade_count': 50
        }
        
        current_reward_func = reward_system.get_current_reward_function()
        reward = current_reward_func.calculate_reward(trading_results, market_state)
        logger.info(f"âœ“ è¨ˆç®—çå‹µ: {reward}")
        
        # 5. æ¸¬è©¦æ¢¯åº¦æµ
        logger.info("5. æ¸¬è©¦æ¢¯åº¦æµå’Œæ¬Šé‡æ›´æ–°...")
        
        # è¨˜éŒ„åˆå§‹æ¬Šé‡
        initial_transformer_weights = {}
        initial_quantum_weights = {}
        
        for name, param in transformer.named_parameters():
            if param.requires_grad:
                initial_transformer_weights[name] = param.clone().detach()
        
        for name, param in quantum_layer.named_parameters():
            if param.requires_grad:
                initial_quantum_weights[name] = param.clone().detach()
        
        # è¨ˆç®—æå¤±ä¸¦åå‘å‚³æ’­
        target = torch.randn_like(strategy_output)
        loss = nn.MSELoss()(strategy_output, target)
        
        logger.info(f"è¨ˆç®—æå¤±: {loss.item():.6f}")
        
        # æ¸…é™¤èˆŠæ¢¯åº¦
        transformer.zero_grad()
        quantum_layer.zero_grad()
        
        # åå‘å‚³æ’­
        loss.backward()
        
        # æª¢æŸ¥æ¢¯åº¦
        transformer_gradients_exist = False
        quantum_gradients_exist = False
        
        for name, param in transformer.named_parameters():
            if param.requires_grad and param.grad is not None:
                grad_norm = param.grad.norm().item()
                if grad_norm > 1e-8:
                    transformer_gradients_exist = True
                    logger.info(f"Transformeråƒæ•¸ {name} æ¢¯åº¦ç¯„æ•¸: {grad_norm:.8f}")
                    break
        
        for name, param in quantum_layer.named_parameters():
            if param.requires_grad and param.grad is not None:
                grad_norm = param.grad.norm().item()
                if grad_norm > 1e-8:
                    quantum_gradients_exist = True
                    logger.info(f"é‡å­ç­–ç•¥å±¤åƒæ•¸ {name} æ¢¯åº¦ç¯„æ•¸: {grad_norm:.8f}")
                    break
        
        logger.info(f"âœ“ Transformeræ¢¯åº¦å­˜åœ¨: {transformer_gradients_exist}")
        logger.info(f"âœ“ é‡å­ç­–ç•¥å±¤æ¢¯åº¦å­˜åœ¨: {quantum_gradients_exist}")
        
        # å„ªåŒ–å™¨æ­¥é©Ÿ
        optimizer = torch.optim.Adam(
            list(transformer.parameters()) + list(quantum_layer.parameters()),
            lr=1e-4
        )
        optimizer.step()
        
        # æª¢æŸ¥æ¬Šé‡æ›´æ–°
        transformer_weights_updated = False
        quantum_weights_updated = False
        
        for name, param in transformer.named_parameters():
            if param.requires_grad and name in initial_transformer_weights:
                initial = initial_transformer_weights[name]
                current = param.detach()
                diff = torch.norm(current - initial).item()
                if diff > 1e-8:
                    transformer_weights_updated = True
                    logger.info(f"Transformeråƒæ•¸ {name} æ¬Šé‡è®ŠåŒ–: {diff:.8f}")
                    break
        
        for name, param in quantum_layer.named_parameters():
            if param.requires_grad and name in initial_quantum_weights:
                initial = initial_quantum_weights[name]
                current = param.detach()
                diff = torch.norm(current - initial).item()
                if diff > 1e-8:
                    quantum_weights_updated = True
                    logger.info(f"é‡å­ç­–ç•¥å±¤åƒæ•¸ {name} æ¬Šé‡è®ŠåŒ–: {diff:.8f}")
                    break
        
        logger.info(f"âœ“ Transformeræ¬Šé‡æ›´æ–°: {transformer_weights_updated}")
        logger.info(f"âœ“ é‡å­ç­–ç•¥å±¤æ¬Šé‡æ›´æ–°: {quantum_weights_updated}")
        
        # ç¸½çµæ¸¬è©¦çµæœ
        logger.info("\n" + "="*80)
        logger.info("å®Œæ•´æ¨¡å‹æµç¨‹æ¸¬è©¦çµæœç¸½çµ")
        logger.info("="*80)
        logger.info(f"âœ“ Enhanced Transformer: æ­£å¸¸é‹è¡Œ")
        logger.info(f"âœ“ é‡å­ç­–ç•¥å±¤: æ­£å¸¸é‹è¡Œ")
        logger.info(f"âœ“ å…ƒå­¸ç¿’ç³»çµ±: æ­£å¸¸é‹è¡Œ")
        logger.info(f"âœ“ æ¼¸é€²å¼çå‹µç³»çµ±: æ­£å¸¸é‹è¡Œ")
        logger.info(f"âœ“ æ¢¯åº¦è¨ˆç®—: Transformer={transformer_gradients_exist}, é‡å­å±¤={quantum_gradients_exist}")
        logger.info(f"âœ“ æ¬Šé‡æ›´æ–°: Transformer={transformer_weights_updated}, é‡å­å±¤={quantum_weights_updated}")
        
        # é©—è­‰æ‰€æœ‰é—œéµè¦æ±‚
        all_tests_passed = all([
            transformer_gradients_exist,
            quantum_gradients_exist,
            transformer_weights_updated,
            quantum_weights_updated
        ])
        
        if all_tests_passed:
            logger.info("ğŸ‰ æ‰€æœ‰æ¸¬è©¦å…¨éƒ¨é€šéï¼æ•´å€‹æµç¨‹é‹è¡Œæ­£å¸¸ï¼Œæ¢¯åº¦æµæš¢é€šï¼Œæ¬Šé‡æ­£ç¢ºæ›´æ–°ï¼")
        else:
            logger.error("âŒ éƒ¨åˆ†æ¸¬è©¦æœªé€šéï¼Œè«‹æª¢æŸ¥ç›¸é—œæ¨¡çµ„")
        
        logger.info("="*80)
        
        return all_tests_passed
        
    except Exception as e:
        logger.error(f"æ¸¬è©¦éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = test_complete_model_flow()
    if success:
        print("\nğŸ‰ å®Œæ•´æ¨¡å‹æµç¨‹æ¸¬è©¦æˆåŠŸå®Œæˆï¼")
    else:
        print("\nâŒ å®Œæ•´æ¨¡å‹æµç¨‹æ¸¬è©¦å¤±æ•—ï¼")
        sys.exit(1)
